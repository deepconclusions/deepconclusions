{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04719b4-4318-4159-a1cc-2ca8931bc7a3",
   "metadata": {},
   "source": [
    "## SciKit-Learn Classification Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46056ff-5e88-4216-ae02-9583401b9ded",
   "metadata": {},
   "source": [
    "In scikit-learn, classification metrics are essential tools to evaluate the performance of a classification model. They provide insights into how well the model is performing and where it may need improvements. Here are some commonly used classification metrics along with examples of how to implement them using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d957ac-82e0-46e5-b7a0-eae0dbefb680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True labels\n",
    "y_true = [0, 1, 1, 0, 1]\n",
    "# Predicted labels\n",
    "y_pred = [1, 1, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74613e-4f86-4aa0-a235-8bb8d5fe3c2e",
   "metadata": {},
   "source": [
    "### Accuracy:\n",
    "   - Accuracy measures the ratio of correctly predicted instances to the total instances.\n",
    "   - Formula:\n",
    "\n",
    "`Accuracy = Number of Correct Predictions / Total Number of Predictions`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5d2bd3",
   "metadata": {},
   "source": [
    "![photo by evidentlyai](../assets/accuracy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5646c169-aeed-4edc-9c0c-b3f433031b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac10f0-e261-47cc-8d12-1b6f78388c86",
   "metadata": {},
   "source": [
    "### Precision:\n",
    "   - Precision measures the ability of the classifier not to label as positive a sample that is negative.\n",
    "   - Precision is simply the models ability to not make a mistake\n",
    "   - Formula:\n",
    "\n",
    "`Precision = True Positives / (True Positives + False Positives)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7875eb",
   "metadata": {},
   "source": [
    "![photo by evidentlyai](../assets/precision.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6369a5c-a0c6-4062-8181-b6f660e50af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 66.66666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_true, y_pred)\n",
    "print(\"Precision:\", precision * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a646b6d-738f-45d8-bd8e-a5e41df1c331",
   "metadata": {},
   "source": [
    "### Recall (also known as Sensitivity or True Positive Rate):\n",
    "   - Recall measures the ability of the classifier to find all the positive samples.\n",
    "   - Formula:\n",
    "\n",
    "`Recall = True Positives / (True Positives + False Negatives)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b719a218",
   "metadata": {},
   "source": [
    "![photo by evidentlyai](../assets/recall.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80685ff5-57cf-43fd-b571-f9a1ce7069a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec10becd-bbd7-49e2-a2aa-81be8849b54e",
   "metadata": {},
   "source": [
    "### F1 Score:\n",
    "   - F1 Score is the harmonic mean of precision and recall. \n",
    "   - It provides a balance between precision and recall.\n",
    "   - Formula:\n",
    "\n",
    "`F1 Score = (2 x Precision x Recall) / (Precision + Recall)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed16a2d7-7c54-461e-875c-d777bd34fa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculate F1 Score\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad60411-154f-4dfc-9888-d9fccf80c4b5",
   "metadata": {},
   "source": [
    "In classification tasks, metrics like precision, recall, and F1-score are commonly used to evaluate the performance of a model. When dealing with multi-class classification, you often need a way to aggregate these metrics across all classes. Three common methods for doing this are micro-average, macro-average, and weighted average.\n",
    "\n",
    "### Micro-average:\n",
    "   - Calculate metrics globally by counting the total true positives, false negatives, and false positives.\n",
    "   - This method gives equal weight to each individual prediction, regardless of class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da7317b",
   "metadata": {},
   "source": [
    "![photo by evidentlyai](../assets/micro-average.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624db41-1da1-4df7-ac55-3bced730ac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro Precision: 0.6\n",
      "Micro Recall: 0.6\n",
      "Micro F1: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Calculate micro-average\n",
    "micro_precision = precision_score(y_true, y_pred, average='micro')\n",
    "micro_recall = recall_score(y_true, y_pred, average='micro')\n",
    "micro_f1 = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "print('Micro Precision:', micro_precision)\n",
    "print('Micro Recall:', micro_recall)\n",
    "print('Micro F1:', micro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a29b3-fd64-4134-833e-e14e01d894ee",
   "metadata": {},
   "source": [
    "### Macro-average:\n",
    "   - Calculate metrics for each class individually and then average them.\n",
    "   - This method treats all classes equally, giving each class the same weight.\n",
    "   - To obtain macro-averaged precision, recall, and F1-score:\n",
    "     - Calculate precision, recall, and F1-score for each class.\n",
    "     - Average the precision, recall, and F1-score across all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e3cb8",
   "metadata": {},
   "source": [
    "![photo by evidentlyai](../assets/macro.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a8a3e-960b-4186-97c4-6ba9132d36d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro Precision: 0.5833333333333333\n",
      "Macro Recall: 0.5833333333333333\n",
      "Macro F1: 0.5833333333333333\n"
     ]
    }
   ],
   "source": [
    "# Calculate macro-average\n",
    "macro_precision = precision_score(y_true, y_pred, average='macro')\n",
    "macro_recall = recall_score(y_true, y_pred, average='macro')\n",
    "macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print('Macro Precision:', macro_precision)\n",
    "print('Macro Recall:', macro_recall)\n",
    "print('Macro F1:', macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887d99fc-a858-43d1-9298-cc4351e83eb7",
   "metadata": {},
   "source": [
    "### Weighted average:\n",
    "   - Calculate metrics for each class individually and then average them, weighted by the number of true instances for each class.\n",
    "   - This method considers class imbalance by giving more weight to classes with more instances.\n",
    "   - To obtain weighted-averaged precision, recall, and F1-score:\n",
    "     - Calculate precision, recall, and F1-score for each class.\n",
    "     - Weighted average is calculated as the `sum of (metric * class_weight) / total_number_of_samples`, where class_weight is the ratio of the number of true instances in the given class to the total number of true instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048da32-fdf0-461d-9269-3f7806c24322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Precision: 0.6\n",
      "Weighted Recall: 0.6\n",
      "Weighted F1: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Calculate weighted-average\n",
    "weighted_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "weighted_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "weighted_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print('Weighted Precision:', weighted_precision)\n",
    "print('Weighted Recall:', weighted_recall)\n",
    "print('Weighted F1:', weighted_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09470dd6",
   "metadata": {},
   "source": [
    "Weighted_precision:\n",
    "\n",
    "`(Precision_A * N_A + Precision_B * N_B, ... , Precision_n * N_n) / (N_A + N_B + ... + N_n)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cf752b-79f3-415e-bc4a-d5a89d92739b",
   "metadata": {},
   "source": [
    "- Micro-average is useful when overall performance across all classes is important\n",
    "- Macro-average is helpful when you want to evaluate the model's performance on smaller classes equally.\n",
    "- Weighted average is suitable when you want to account for class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df433580-6cde-4ae8-b74e-2d268874cb79",
   "metadata": {},
   "source": [
    "### The Classification Report\n",
    "\n",
    "The classification report in scikit-learn provides a comprehensive summary of different classification metrics for each class in the dataset. It includes precision, recall, F1-score, and support (the number of true instances for each label). Here's how you can generate a classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac53a1a2-17e3-4d2c-98e7-f7fbe55a1d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score  support\n",
       "0              0.500000  0.500000  0.500000      2.0\n",
       "1              0.666667  0.666667  0.666667      3.0\n",
       "accuracy       0.600000  0.600000  0.600000      0.6\n",
       "macro avg      0.583333  0.583333  0.583333      5.0\n",
       "weighted avg   0.600000  0.600000  0.600000      5.0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate classification report\n",
    "class_report_dict = classification_report(y_true, y_pred, output_dict=True)\n",
    "class_report_df = pd.DataFrame(class_report_dict).transpose()\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "class_report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b56ba0-f2ec-41b7-921f-591bb405bf55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5833335"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.666667 + 0.5)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82375996-3de1-4c31-86c6-e7db6490de8a",
   "metadata": {},
   "source": [
    "### Confusion Matrix:\n",
    "   - A confusion matrix is a table that is often used to describe the performance of a classification model.\n",
    "   - It presents a summary of the model's predictions on the classification problem, showing correct predictions as well as types of errors made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f5e61-4164-459a-b46a-1f08287f2a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  1  1\n",
       "1  1  2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "conf_matrix = pd.DataFrame(conf_matrix, index=[0, 1], columns=[0, 1])\n",
    "# print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d524f0f-68eb-4f22-bd23-d71500b8aa3c",
   "metadata": {},
   "source": [
    "These are just a few of the many classification metrics available in scikit-learn. Depending on your specific problem and requirements, you may want to explore other metrics as well.\n",
    "\n",
    "Understanding these metrics and how they are computed can provide valuable insights into the performance of a classification model and help in making informed decisions about its improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
