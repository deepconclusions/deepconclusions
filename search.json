[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Deep Conclusions!",
    "section": "",
    "text": "Hello there 🤗 and welcome to our new resourses site. We have re-develped our website to use jupyter notebooks, which is the tool we mostly use for teaching. This is so that you can access updated learning materials faster.",
    "crumbs": [
      "Home",
      "Welcome to Deep Conclusions!"
    ]
  },
  {
    "objectID": "index.html#programming-for-data-science",
    "href": "index.html#programming-for-data-science",
    "title": "Welcome to Deep Conclusions!",
    "section": "Programming For Data Science",
    "text": "Programming For Data Science\nProgramming for Data Science is a subject we’ve designed to explore the various programming components of data science. To take on this course means that you’ll learn programming techniques to do Data Science stuff like data manipulation, analysis, visualization, machine learning.\nThis subject has been split into specific courses as below. The duration is the average of what our older students have taken, but can be longer or shorter for different types of students:\n\nFundamentals of Programming (Python or JavaScript or both)(1 month)\nData Analysis with Python and Introduction to Machine Learning (2-3 months)\nDeep Learning Specialization (3-5 months)\nDatabase Management with SQL and Python (optional) (1 month)\nWeb Development with Django (optional) (2-6 months)\n\nThis subject can be taken as a whole or you can take specific parts of strong interest or application to you and your field. But if you’re a complete beginner, we advise you take the whole subject.\nHere is how our subject is offered:\n\nDuration: (Depends on the course units you have chosen)\nSessions: Four times a week (one on one)\nTiming: Evenings or/and weekends\nLocation: Online or/and at UMF House, Sir Apollo Kagwa Road\n\nIf you’re serious about learning Programming and getting prepared for Data Science and Web Development roles, I highly encourage you to enroll in our programming courses.\nDon’t waste your time following disconnected, outdated tutorials. We can teach you all that you need to kickstart your career.\nContact us at:\n\ninfo@deepconclusions.com\n+256701520768 / +256771754118\nwww.deepconclusions.com",
    "crumbs": [
      "Home",
      "Welcome to Deep Conclusions!"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week5/52_inferencial_statistics.html",
    "href": "Python Data Analysis/Week5/52_inferencial_statistics.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Statistical inference is a process in which conclusions about populations or processes are drawn from a sample of data using statistical methods. It involves making inferences, predictions, or decisions about a population based on information obtained from a sample.\n## Uncomment and run this cell to install the packages\n# !pip install pandas numpy statsmodels scipy\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport polars as pl\nimport pandas as pd\n# load the dataset (modify the path to point to your copy of the dataset)\ndata = pd.read_csv('../assets/nobel_prize_year.csv')\ndata.sample(n=5)\n\n\n\nshape: (5, 5)\n\n\n\nYear\nGender\nCategory\nbirth_year\nage\n\n\ni64\nstr\nstr\ni64\ni64\n\n\n\n\n1926\n\"male\"\n\"Peace\"\n1878\n48\n\n\n2019\n\"male\"\n\"Physics\"\n1935\n84\n\n\n1996\n\"male\"\n\"Peace\"\n1948\n48\n\n\n1931\n\"male\"\n\"Medicine\"\n1883\n48\n\n\n1906\n\"male\"\n\"Chemistry\"\n1852\n54\n# get the age column data (optional: convert to numpy array)\nages = np.array(data['age'])",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week5",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week5/52_inferencial_statistics.html#normal-distribution",
    "href": "Python Data Analysis/Week5/52_inferencial_statistics.html#normal-distribution",
    "title": "Statistical Inference",
    "section": "Normal Distribution",
    "text": "Normal Distribution\nThe normal distribution is an important probability distribution used in statistics.\nMany real world examples of data are normally distributed.\nNormal Distribution The normal distribution is described by the mean and the standard deviation\nThe normal distribution is often referred to as a ‘bell curve’ because of it’s shape:\n\nMost of the values are around the center\nThe median and mean are equal\nIt has only one mode\nIt is symmetric, meaning it decreases the same amount on the left and the right of the center\n\nThe area under the curve of the normal distribution represents probabilities for the data.\nThe area under the whole curve is equal to 1, or 100%\nHere is a graph of a normal distribution with probabilities between standard deviations over the nobel prize laureates\n\n# Calculate mean and standard deviation of the standardized ages\nmu = np.mean(ages)\nsigma = np.std(ages)\n\n# Plot histogram\nplt.hist(ages, bins=10, density=True, color='skyblue', edgecolor='black', alpha=0.7)\n\n# Create an array of values within three standard deviations from the mean\nx = np.linspace(min(ages), max(ages), 100)\n\n# Calculate the probability density function (PDF) for the normal distribution\npdf = stats.norm.pdf(x, mu, sigma)\n\n# Plot the normal distribution\nplt.plot(x, pdf, label='Normal Distribution', color='blue')\n\n# Fill the area between one standard deviation from the mean\nplt.fill_between(x, pdf, where=(x &gt;= mu - 3*sigma) & (x &lt;= mu + 3*sigma), color='r', alpha=0.5, label='Between ±3σ')\n\n# Fill the area between two standard deviations from the mean\nplt.fill_between(x, pdf, where=(x &gt;= mu - 2*sigma) & (x &lt;= mu + 2*sigma), color='g', alpha=0.5, label='Between ±2σ')\n\n# Fill the area between one standard deviation from the mean\nplt.fill_between(x, pdf, where=(x &gt;= mu - sigma) & (x &lt;= mu + sigma), color='b', alpha=0.5, label='Between ±σ')\n\n# Add labels and legend\nplt.xlabel('Age')\nplt.ylabel('Probability Density')\nplt.title('Nobel Prize Winners\\' Ages with Probabilities Between Standard Deviations')\n\nplt.legend()\n\n# Display the plot\nplt.grid(True)\nplt.show()",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week5",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week5/52_inferencial_statistics.html#standard-normal-distribution",
    "href": "Python Data Analysis/Week5/52_inferencial_statistics.html#standard-normal-distribution",
    "title": "Statistical Inference",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\nNormally distributed data can be transformed into a standard normal distribution.\nThe standard normal distribution is used for:\n\nCalculating confidence intervals\nHypothesis tests\n\n\nZ-Values\nZ-values express how many standard deviations from the mean a value is.\nThe formula for calculating a Z-value is:\n\\(( z = \\frac{x - \\mu}{\\sigma} )\\)\nWhere:\n\n\\(( \\mu )\\) is the mean.\n\\(( \\sigma )\\) is the standard deviation\n\nHere is a graph of the standard normal distribution with probability values between the standard deviations:\n\n# Standardize ages\nstandardized_ages = (ages - np.mean(ages)) / np.std(ages)\n\n# Calculate mean and standard deviation of the standardized ages\nmu = np.mean(standardized_ages)\nsigma = np.std(standardized_ages)\n\n# Plot histogram\nplt.hist(standardized_ages, bins=10, density=True, color='skyblue', edgecolor='black', alpha=0.7)\n\n# Create an array of values within three standard deviations from the mean\nx = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n\n# Calculate the probability density function (PDF) for the normal distribution\npdf = stats.norm.pdf(x, mu, sigma)\n\n# Plot the normal distribution\nplt.plot(x, pdf, label='Normal Distribution', color='blue')\n\n# Fill the area between three standard deviation from the mean\nplt.fill_between(x, pdf, where=(x &gt;= mu - 3*sigma) & (x &lt;= mu + 3*sigma), color='r', alpha=0.5, label='Between ±3σ')\n\n# Fill the area between two standard deviations from the mean\nplt.fill_between(x, pdf, where=(x &gt;= mu - 2*sigma) & (x &lt;= mu + 2*sigma), color='g', alpha=0.5, label='Between ±2σ')\n\n# Fill the area between one standard deviation from the mean\nplt.fill_between(x, pdf, where=(x &gt;= mu - sigma) & (x &lt;= mu + sigma), color='b', alpha=0.5, label='Between ±σ')\n\n# Add labels and legend\nplt.title('Normal Distribution of Ages of Nobel Prize Winners')\nplt.xlabel('Standardized Ages')\nplt.ylabel('Probability Density')\nplt.legend()\n\n# Display the plot\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe Probability Density Function (PDF)\nThe Probability Density Function (PDF) of a normal distribution represents the relative likelihood of observing different values of a continuous random variable. For a normal distribution with mean \\(( \\mu )\\) and standard deviation \\(( \\sigma )\\), the PDF is denoted as \\(( f(x) )\\).\nThe formula for the standard normal PDF \\(( \\phi(z) )\\) is:\n\\(\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{1}{2}z^2}\\)\nWhere: - \\(( z )\\) is the standardized value of the random variable \\(( X )\\), calculated as \\(( z = \\frac{x - \\mu}{\\sigma} )\\). - \\(( e )\\) is the base of the natural logarithm. - \\(( \\pi )\\) is the mathematical constant pi (approximately 3.14159).\nIf your data is not standardized (i.e., does not have a mean of 0 and standard deviation of 1), you would first standardize the values using the formula \\(( z = \\frac{x - \\mu}{\\sigma} )\\) before evaluating the PDF.\n\n\nThe Cummulative Distribution Function\nThe CDF represents the probability that a random variable \\(( X )\\) is less than or equal to a given value \\(( x )\\). For a normal distribution with mean \\(( \\mu )\\) and standard deviation \\(( \\sigma )\\), the CDF is denoted as \\(( \\Phi(x) )\\).\nThe formula for the standard normal cumulative distribution function \\(( \\Phi(z) )\\) is:\n$(z) = _{-}^{z} e{-t2} dt $\nWhere: - \\(( z )\\) is the standardized value of the random variable \\(( X )\\), calculated as \\(( z = \\frac{x - \\mu}{\\sigma} )\\). - \\(( e )\\) is the base of the natural logarithm. - \\(( \\pi )\\) is the mathematical constant pi (approximately 3.14159).\nTo calculate the probability of a specific range of values, you can subtract the CDF at the lower bound of the range from the CDF at the upper bound of the range:\n$P(a X b) = () - () $\nWhere \\(( a )\\) and \\(( b )\\) are the lower and upper bounds of the range, respectively.\nFor a normal distribution with a mean \\(( \\mu )\\) and standard deviation \\(( \\sigma )\\), you can use the above formula to calculate probabilities associated with specific values or ranges of values. If your data is not standardized (i.e., does not have a mean of 0 and standard deviation of 1), you would first standardize the values using the formula \\(( z = \\frac{x - \\mu}{\\sigma} )\\) before using the standard normal CDF.\n\n\nFinding the P-value of a Z-Value\n\nmean = ages.mean()\nstandard_deviation = ages.std()\n\npdf2 = stats.norm(loc=mean, scale=standard_deviation)\np_value = pdf2.cdf(40)\n\nprint('p-value:', p_value)\n\np-value: 0.05756104393909259\n\n\n\n\nFinding the Z-value of a P-Value\n\nmean = ages.mean()\nstandard_deviation = ages.std()\n\npdf2 = stats.norm(loc=mean, scale=standard_deviation)\nz_val = pdf2.ppf(0.05756104393909259)\n\nprint('z-value:', z_val)\n\nz-value: 40.0",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week5",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week5/52_inferencial_statistics.html#hypothesis-testing-and-p-values",
    "href": "Python Data Analysis/Week5/52_inferencial_statistics.html#hypothesis-testing-and-p-values",
    "title": "Statistical Inference",
    "section": "Hypothesis Testing and p-values",
    "text": "Hypothesis Testing and p-values\nHypothesis testing is a statistical method used to make inferences about a population based on sample data. It involves testing a hypothesis or claim about a population parameter, such as a mean or proportion.\n\nNull and Alternative Hypotheses:\n\nNull Hypothesis (H0): It is a statement of no effect or no difference, typically denoted as the status quo.\nAlternative Hypothesis (H1 or Ha): It is the hypothesis that we are trying to support, indicating there is an effect or difference.\n\n\nExample: Null hypothesis - mean of age is equal to 40\nAlternative hypothesis - mean of age is not equal to 40\nH0: μ = 40, Ha: μ ≠ 40\n\nTypes of Errors: Type I and Type II:\n\nType I Error (False Positive): It occurs when we reject the null hypothesis when it is actually true.\nType II Error (False Negative): It occurs when we fail to reject the null hypothesis when it is actually false.\n\n\nExample: Type I error - Rejecting the null hypothesis when the population mean is actually equal to 40\nType II error - Failing to reject the null hypothesis when the population mean is actually not equal to 40\n\nSignificance Level (α):\n\nSignificance level is denoted by α and represents the probability of making a Type I error.\nCommonly used significance levels include 0.05, 0.01, etc.\n\np-values and their interpretation:\n\np-value is the probability of observing the data or more extreme data under the null hypothesis.\nIf p-value is less than α, we reject the null hypothesis; otherwise, we fail to reject it.\n\n\n\nt_statistic, p_value = stats.ttest_1samp(ages, 40)\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\nt-statistic: 48.69033819659244\np-value: 5.42663134957101e-261\n\n\n\nOne-sample and Two-sample t-tests:\n\nOne-sample t-test is used to compare the mean of a single sample to a known value or population mean.\nTwo-sample t-test is used to compare the means of two independent samples.\n\n\n\ngroup1, group2 = np.array_split(ages, 2)\n\nt_statistic, p_value = stats.ttest_ind(group1, group2)\n\nprint(\"t-statistic:\", t_statistic)\nprint(\"p-value:\", p_value)\n\nt-statistic: 1.4459199913013356\np-value: 0.14852807347682806\n\n\n\nZ-tests and Confidence Intervals:\n\nZ-tests are similar to t-tests but are used when sample size is large and population standard deviation is known.\nConfidence Intervals provide a range of values that is likely to contain the population parameter with a certain level of confidence.\n\n\n\nA z-test a statistical test used to determine whether two means are different when the population standard deviation is known\n\n\ndef ztest(sample_data=None, population_data=None, sample_mean:float=None, population_mean:float=None):\n    if sample_data.all():\n        sample_mean = np.mean(sample_ages)\n        sample_size = len(sample_ages)\n    if population_data.all():\n        population_mean = np.mean(ages)\n        population_std = np.std(ages)\n        \n    z_score = (sample_mean - population_mean)/(population_std/(np.sqrt(sample_size)))\n    p_value = stats.norm.cdf(z_score)\n    return (z_score, p_value)\n\n\nsample_ages = np.array(data.sample(n=56)['age'])\n\n\nz_score, p_value = ztest(sample_ages, ages)\n\n\np_value = stats.norm.cdf(z_score)\n\n\np_value\n\n0.6737098436973401\n\n\n\nsample_mean = np.mean(ages)\nsample_std = np.std(ages)\n\nn = len(ages)\nalpha = 0.05\nz_critical = stats.norm.ppf(1 - alpha/2)\n\nmargin_of_error = z_critical * (sample_std / np.sqrt(n))\nconfidence_interval = (sample_mean - margin_of_error, sample_mean + margin_of_error)\n\nprint('z critical:', z_critical)\nprint(\"Confidence Interval:\", confidence_interval)\n\nz critical: 1.959963984540054\nConfidence Interval: (59.314817066181526, 60.934136908713874)\n\n\n\n(1.959963984540054 * sample_std) + sample_mean\n\n85.15855423882927\n\n\n\nChi-square Tests for Independence:\n\nChi-square test is used to determine whether there is a significant association between two categorical variables.\n\n\n\n# Pick out on the Gender and Category from the dataset\n# We drop all the missing values just for demonstration purposes\ngender_category_data = data[['Gender', 'Category']].dropna()\n\n\n# Obtain the cross tabulation of Gender and Category\n# The cross tabulation is also known as the contingency table\ngender_category_tab = pd.crosstab(gender_category_data.Gender, gender_category_data.Category)\n\n\n# Example: Performing a chi-square test for independence\nchi2_stat, p_value, dof, expected = stats.chi2_contingency(gender_category_tab)\n\nprint(\"Chi-square Statistic:\", chi2_stat)\nprint(\"p-value:\", p_value)\n\nChi-square Statistic: 55.9451562721685\np-value: 2.1020428793080495e-08\n\n\n\nANOVA (Analysis of Variance) and its applications:\n\nANOVA is used to compare means of three or more groups to determine if there is a statistically significant difference between them.\n\n\n\n# Example: Performing ANOVA\ngroup1, group2, group3 = np.array_split(ages, 3) \nf_statistic, p_value = stats.f_oneway(group1, group2, group3)\n\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\n\nF-statistic: 0.26064539756513505\np-value: 0.7706090235692291\n\n\nThe p-value of 0.770… is way higher than the significance level (0.05), and therefore we fail to reject the null hypothesis (ie. The means are statistically the same)\nF-statistic tells us whether there are significant differences between the means of the groups\nf_statistic = between_group_variance/within_group_variance\nThese code snippets demonstrate various hypothesis testing techniques and their implementation in Python using libraries like SciPy.",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week5",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week5/52_inferencial_statistics.html#key-terms-associated-with-statistical-inference",
    "href": "Python Data Analysis/Week5/52_inferencial_statistics.html#key-terms-associated-with-statistical-inference",
    "title": "Statistical Inference",
    "section": "Key terms associated with statistical inference:",
    "text": "Key terms associated with statistical inference:\n\nPopulation: The entire group of individuals or elements that the researcher is interested in studying. It’s often impractical or impossible to collect data from the entire population, so we work with samples instead.\nSample: A subset of the population from which data is collected. The sample should ideally be representative of the population to make valid inferences.\nParameter: A numerical characteristic of a population. Examples include the population mean, population proportion, or population standard deviation.\nStatistic: A numerical characteristic of a sample. Examples include the sample mean, sample proportion, or sample standard deviation.\nEstimation: The process of using sample data to estimate the value of a population parameter. Point estimation involves providing a single value as an estimate, while interval estimation provides a range of values (confidence interval) within which the parameter is believed to lie.\nHypothesis Testing: A method used to make decisions or draw conclusions about a population parameter based on sample data. It involves stating a null hypothesis (H0) and an alternative hypothesis (H1), collecting data, and then using statistical tests to determine whether there is enough evidence to reject the null hypothesis.\nConfidence Intervals: A range of values calculated from sample data that is likely to contain the true population parameter with a certain level of confidence (e.g., 95% confidence interval).\nSignificance Level (α): The probability of rejecting the null hypothesis when it is actually true. It is typically set at 0.05 or 0.01, indicating a 5% or 1% chance of a Type I error, respectively.\nType I Error: Rejecting the null hypothesis when it is actually true (false positive).\nType II Error: Failing to reject the null hypothesis when it is actually false (false negative).",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week5",
      "Statistical Inference"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week5/53_implementing_statistical_models.html",
    "href": "Python Data Analysis/Week5/53_implementing_statistical_models.html",
    "title": "deepconclusions",
    "section": "",
    "text": "## Uncomment and run this cell to install the packages\n# !pip install pandas numpy statsmodels",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week5",
      "Implementing Statistical Models in Python"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week5/53_implementing_statistical_models.html#implementing-statistical-models-in-python",
    "href": "Python Data Analysis/Week5/53_implementing_statistical_models.html#implementing-statistical-models-in-python",
    "title": "deepconclusions",
    "section": "Implementing Statistical Models in Python",
    "text": "Implementing Statistical Models in Python\n\nStatistical models\nStatistical models are mathematical representations of relationships between variables in a dataset. These models are used to make predictions, infer causal relationships, and understand patterns in data. Statistical modeling involves formulating hypotheses about the data generating process, estimating model parameters from observed data, and evaluating the fit of the model to the data.\nThe statsmodels.api library in Python provides a wide range of tools for statistical modeling and inference. It allows users to build, estimate, and analyze various statistical models using a simple and intuitive interface.\n\nimport statsmodels.api as sm\nimport numpy as np\n\n\nLinear Regression:\n\nLinear regression is used to model the relationship between one or more independent variables and a continuous dependent variable.\n\n\n\n# Generate example data\nnp.random.seed(0)\nX = np.random.rand(100, 2)  # Two independent variables\n\n\n# Dependent variable with noise\n\ny = 2 * X[:, 0] + 3 * X[:, 1] + np.random.normal(0, 1, 100)\n\n\n# Add constant term for intercept\nX = sm.add_constant(X)\n\n\n# Fit linear regression model\nmodel = sm.OLS(y, X).fit()\n\n\n# Print model summary\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.440\nModel:                            OLS   Adj. R-squared:                  0.428\nMethod:                 Least Squares   F-statistic:                     38.06\nDate:                Wed, 17 Apr 2024   Prob (F-statistic):           6.31e-13\nTime:                        12:22:26   Log-Likelihood:                -140.25\nNo. Observations:                 100   AIC:                             286.5\nDf Residuals:                      97   BIC:                             294.3\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst          0.2554      0.277      0.922      0.359      -0.294       0.805\nx1             1.4260      0.356      4.011      0.000       0.720       2.132\nx2             2.8054      0.351      8.004      0.000       2.110       3.501\n==============================================================================\nOmnibus:                        1.210   Durbin-Watson:                   2.349\nProb(Omnibus):                  0.546   Jarque-Bera (JB):                0.703\nSkew:                           0.122   Prob(JB):                        0.704\nKurtosis:                       3.330   Cond. No.                         5.58\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n0.2554/0.277\n\n0.9220216606498195\n\n\n\nR-squared measures how well the independant variables explain the variability of the dependant variable\nF-Statistic measures the significance of the regression model\nt-statistic for each coefficient measures the significance level of each independent variable\n\n\nLogistic Regression:\n\nLogistic regression is used when the dependent variable is binary (e.g., 0 or 1, True or False).\n\n\n\n# Generate example data for logistic regression\nnp.random.seed(0)\nX = np.random.rand(100, 2)  # Two independent variables\n# Generate binary outcome variable based on a threshold\nthreshold = 0.6\ny = (2 * X[:, 0] + 3 * X[:, 1] &gt; threshold).astype(int)\n\n# Add constant term for intercept\nX = sm.add_constant(X)\n\n# Fit logistic regression model\nlogit_model = sm.Logit(y, X).fit()\n\n# Print model summary\nprint(logit_model.summary())\n\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.000000\n         Iterations: 35\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:                      y   No. Observations:                  100\nModel:                          Logit   Df Residuals:                       97\nMethod:                           MLE   Df Model:                            2\nDate:                Wed, 17 Apr 2024   Pseudo R-squ.:                   1.000\nTime:                        13:05:57   Log-Likelihood:            -1.1958e-06\nconverged:                      False   LL-Null:                       -9.8039\nCovariance Type:            nonrobust   LLR p-value:                 5.524e-05\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst        -70.8342   4.95e+04     -0.001      0.999   -9.71e+04     9.7e+04\nx1           196.7613   1.92e+05      0.001      0.999   -3.76e+05    3.76e+05\nx2           488.7691   7.23e+05      0.001      0.999   -1.42e+06    1.42e+06\n==============================================================================\n\nComplete Separation: The results show that there iscomplete separation or perfect prediction.\nIn this case the Maximum Likelihood Estimator does not exist and the parameters\nare not identified.\n\n\n/home/jumashafara/venvs/dataanalysis/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\nThese examples demonstrate how to implement linear regression and logistic regression using statsmodels.api. The summary output provides detailed information about the model parameters, goodness-of-fit measures, and statistical significance of predictors. This can be useful for interpreting the results and assessing the performance of the models.",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week5",
      "Implementing Statistical Models in Python"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/pandas.html",
    "href": "Python Data Analysis/Week1/pandas.html",
    "title": "Pandas",
    "section": "",
    "text": "## Uncomment and run this cell to install pandas\n#!pip install pandas\n#!pip install openpyxl\nimport pandas as pd\nfrom dataidea.datasets import loadDataset\n# to check python version\npd.__version__\n\n'2.1.4'",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Pandas"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/pandas.html#creating-dataframes",
    "href": "Python Data Analysis/Week1/pandas.html#creating-dataframes",
    "title": "Pandas",
    "section": "Creating Dataframes",
    "text": "Creating Dataframes\n\ncreating dataframes from existing files\n\n# loading an excel file into a dataframe\ndata = loadDataset('demo')\n\nThe data structure that is returned by the statement is called a DataFrame\n\n# checking the datatype of the data object\ntype(data)\n\npandas.core.frame.DataFrame\n\n\n\n# randomly sample some values\ndata.sample(n=5)\n\n\n\n\n\n\n\n\n\nAge\nGender\nMarital Status\nAddress\nIncome\nIncome Category\nJob Category\n\n\n\n\n171\n24\nm\n0\n5\n13\n1\n1\n\n\n92\n61\nm\n1\n18\n23\n1\n3\n\n\n72\n36\nm\n0\n8\n51\n3\n1\n\n\n3\n24\nm\n1\n4\n26\n2\n1\n\n\n21\n36\nf\n0\n6\n39\n2\n1\n\n\n\n\n\n\n\n\n\n\nCreating a DataFrame from a Dictionary\nWe can create pandas dataframes using two major ways: - Using a dictionary - Using a 2D list\n\n# create a pandas dataframe using a dictionary\ndata_dictionary = {\n    'age': [65, 51, 45, 38, 40],\n    'gender': ['m', 'm', 'm', 'f', 'm'],\n    'income': [42, 148, 147, 43, 89]\n}\n\ndataframe_from_dict = pd.DataFrame(data=data_dictionary)\n\n\n# display the dataframe\ndataframe_from_dict\n\n\n\n\n\n\n\n\n\nage\ngender\nincome\n\n\n\n\n0\n65\nm\n42\n\n\n1\n51\nm\n148\n\n\n2\n45\nm\n147\n\n\n3\n38\nf\n43\n\n\n4\n40\nm\n89\n\n\n\n\n\n\n\n\n\n# creating a dataframe from a 2D list\ndata_list = [\n    [28, 'm', 24],\n    [59, 'm', 841],\n    [54, 'm', 741],\n    [83, 'f', 34],\n    [34, 'm', 98]\n]\n\ndataframe_from_list = pd.DataFrame(data=data_list, \n                                   columns=['age', 'gender', 'income'])\n\n\n# display the dataframe\ndataframe_from_list\n\n\n\n\n\n\n\n\n\nage\ngender\nincome\n\n\n\n\n0\n28\nm\n24\n\n\n1\n59\nm\n841\n\n\n2\n54\nm\n741\n\n\n3\n83\nf\n34\n\n\n4\n34\nm\n98\n\n\n\n\n\n\n\n\n\n# Finding more information\n# help(pd.DataFrame)\n\n\n## Another way to find more information\n# ?pd.DataFrame",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Pandas"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/pandas.html#concatenating-dataframes",
    "href": "Python Data Analysis/Week1/pandas.html#concatenating-dataframes",
    "title": "Pandas",
    "section": "Concatenating DataFrames",
    "text": "Concatenating DataFrames\nSometimes there’s a need to add two or more dataframes. To perform this, for the start, we can use the pd.concat(). Below is some illustration\n\nconcatenated_dataframe = pd.concat([dataframe_from_dict, dataframe_from_list], ignore_index=True)\n\n\nconcatenated_dataframe\n\n\n\n\n\n\n\n\n\nage\ngender\nincome\n\n\n\n\n0\n65\nm\n42\n\n\n1\n51\nm\n148\n\n\n2\n45\nm\n147\n\n\n3\n38\nf\n43\n\n\n4\n40\nm\n89\n\n\n5\n28\nm\n24\n\n\n6\n59\nm\n841\n\n\n7\n54\nm\n741\n\n\n8\n83\nf\n34\n\n\n9\n34\nm\n98",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Pandas"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/pandas.html#sampling-values-in-the-dataframe",
    "href": "Python Data Analysis/Week1/pandas.html#sampling-values-in-the-dataframe",
    "title": "Pandas",
    "section": "Sampling values in the DataFrame",
    "text": "Sampling values in the DataFrame\n\n# We can have look at the top part \nconcatenated_dataframe.head(n=3)\n\n\n\n\n\n\n\n\n\nage\ngender\nincome\n\n\n\n\n0\n65\nm\n42\n\n\n1\n51\nm\n148\n\n\n2\n45\nm\n147\n\n\n\n\n\n\n\n\n\n# We can look at the bottom part\nconcatenated_dataframe.tail(n=3)\n\n\n\n\n\n\n\n\n\nage\ngender\nincome\n\n\n\n\n7\n54\nm\n741\n\n\n8\n83\nf\n34\n\n\n9\n34\nm\n98\n\n\n\n\n\n\n\n\n\n# We can also randomly sample out some values in a DataFrame\nconcatenated_dataframe.sample(n=3)\n\n\n\n\n\n\n\n\n\nage\ngender\nincome\n\n\n\n\n5\n28\nm\n24\n\n\n4\n40\nm\n89\n\n\n0\n65\nm\n42",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Pandas"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/pandas.html#selection",
    "href": "Python Data Analysis/Week1/pandas.html#selection",
    "title": "Pandas",
    "section": "Selection",
    "text": "Selection\n\nSelecting, Boolean Indexing and Setting\n\ncountry_data = pd.DataFrame(data={\n    'Country': ['Belgium', 'India', 'Brazil'],\n    'Capital': ['Brussels', 'New Delhi', 'Brasilia'],\n    'Population': [11190846, 1303171035, 207847528]\n    })\ncountry_data\n\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\n\n\n\n\n0\nBelgium\nBrussels\n11190846\n\n\n1\nIndia\nNew Delhi\n1303171035\n\n\n2\nBrazil\nBrasilia\n207847528\n\n\n\n\n\n\n\n\n\n# position 1\nprint(country_data.iloc[0, 0])\nprint(country_data.iloc[2, 1])\n\nBelgium\nBrasilia\n\n\n\n# position 2\nprint(country_data.iat[0, 0])\nprint(country_data.iat[2, 1])\n\nBelgium\nBrasilia\n\n\nPonder:\n\nHow can you use the pd.DataFrame.iat method to replace (or modify) a specific value in a dataframe\n\n\ndef locate(dataframe:pd.core.frame.DataFrame=None, row:int=0, column:str=None):\n    '''\n    Selects specific item by row index and column name\n    '''\n    return dataframe.loc[row, column]\n\n\nlocate(country_data, 0, 'Capital')\n\n'Brussels'\n\n\n\n?locate\n\n\nSignature:\nlocate(\n    dataframe: pandas.core.frame.DataFrame = None,\n    row: int = 0,\n    column: str = None,\n)\nDocstring: Selects specific item by row index and column name\nFile:      /tmp/ipykernel_4522/2733881718.py\nType:      function\n\n\n\n\n# using label\nprint(country_data.loc[0, 'Capital'])\nprint(country_data.loc[1, 'Population'])\n\nBrussels\n1303171035\n\n\n\n# using label\nprint(country_data.at[2, 'Population'])\nprint(country_data.at[1, 'Capital'])\n\n207847528\nNew Delhi\n\n\n\n# picking out data from a specific column\ncountry_data.Country\n\n0    Belgium\n1      India\n2     Brazil\nName: Country, dtype: object\n\n\n\n# another way to pick data from a specific column\ncountry_data['Capital']\n\n0     Brussels\n1    New Delhi\n2     Brasilia\nName: Capital, dtype: object\n\n\nThe data structure that is returned by the statement is called a Series\n\n# lets check it\ntype(country_data['Capital'])\n\npandas.core.series.Series\n\n\n\n# Get specific row data (using index)\ncountry_data.iloc[0]\n\nCountry        Belgium\nCapital       Brussels\nPopulation    11190846\nName: 0, dtype: object\n\n\n\n# get all rows that have a column-value matching a specific value\n# eg where country is Belgium\ncountry_data[country_data['Country'] == 'Belgium']\n\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\n\n\n\n\n0\nBelgium\nBrussels\n11190846\n\n\n\n\n\n\n\n\n\n# Think about this\ncountry_data['Country'] == 'Belgium'\n\n0     True\n1    False\n2    False\nName: Country, dtype: bool",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Pandas"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/pandas.html#dropping",
    "href": "Python Data Analysis/Week1/pandas.html#dropping",
    "title": "Pandas",
    "section": "Dropping",
    "text": "Dropping\n\ncountry_data\n\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\n\n\n\n\n0\nBelgium\nBrussels\n11190846\n\n\n1\nIndia\nNew Delhi\n1303171035\n\n\n2\nBrazil\nBrasilia\n207847528\n\n\n\n\n\n\n\n\n\n# drop a column from a dataframe\ncountry_data.drop('Country', axis=1)\n\n\n\n\n\n\n\n\n\nCapital\nPopulation\n\n\n\n\n0\nBrussels\n11190846\n\n\n1\nNew Delhi\n1303171035\n\n\n2\nBrasilia\n207847528\n\n\n\n\n\n\n\n\n\ndef calculateBMI(weight_kg, height_m, status:bool=True, round_off:bool=True):\n    bmi = weight_kg / height_m ** 2\n    \n    if round_off:\n        bmi = round(bmi)\n\n    if status:\n        status = 'Normal' if 18 &lt;= bmi &lt;= 24 else 'Not Normal'\n        return bmi, status\n    else:\n        return bmi\n\n\ncalculateBMI(67, 1.7)\n\n(23, 'Normal')\n\n\n\n# You can drop many columns by passing in a columns list\ncountry_data.drop(columns=['Country', 'Population'], inplace=True)\n\n\ncountry_data\n\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\n\n\n\n\n0\nBelgium\nBrussels\n11190846\n\n\n1\nIndia\nNew Delhi\n1303171035\n\n\n2\nBrazil\nBrasilia\n207847528\n\n\n\n\n\n\n\n\n\n# how to drop row data\ncountry_data.drop([0, 2], axis=0)\n\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\n\n\n\n\n1\nIndia\nNew Delhi\n1303171035\n\n\n\n\n\n\n\n\n\nResearch on:\n\nsort and rank data",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Pandas"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/pandas.html#retrieving-information-about-dataframe",
    "href": "Python Data Analysis/Week1/pandas.html#retrieving-information-about-dataframe",
    "title": "Pandas",
    "section": "Retrieving information about DataFrame",
    "text": "Retrieving information about DataFrame\n\nBasic Information\n\ncountry_data = pd.DataFrame({\n    'Country': ['Belgium', 'India', 'Brazil'],\n    'Capital': ['Brussels', None, None],\n    'Population': [11190846, 1303171035, 207847528]\n    })\n\ncountry_data\n\n\n\n\n\n\n\n\n\nCountry\nCapital\nPopulation\n\n\n\n\n0\nBelgium\nBrussels\n11190846\n\n\n1\nIndia\nNone\n1303171035\n\n\n2\nBrazil\nNone\n207847528\n\n\n\n\n\n\n\n\n\n# shape of a dataframe (ie rows and columns)\ncountry_data.shape\n\n(3, 3)\n\n\n\n# Get all columns in a dataframe\ncountry_data.columns\n\nIndex(['Country', 'Capital', 'Population'], dtype='object')\n\n\n\n# get some basic info about the dataframe\ncountry_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3 entries, 0 to 2\nData columns (total 3 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Country     3 non-null      object\n 1   Capital     1 non-null      object\n 2   Population  3 non-null      int64 \ndtypes: int64(1), object(2)\nmemory usage: 200.0+ bytes\n\n\n\n# Count non-null values in each column\ncountry_data.count()\n\nCountry       3\nCapital       1\nPopulation    3\ndtype: int64\n\n\n\n\nSummary\n\n# summary statistics\ncountry_data.describe()\n\n\n\n\n\n\n\n\n\nPopulation\n\n\n\n\ncount\n3.000000e+00\n\n\nmean\n5.074031e+08\n\n\nstd\n6.961346e+08\n\n\nmin\n1.119085e+07\n\n\n25%\n1.095192e+08\n\n\n50%\n2.078475e+08\n\n\n75%\n7.555093e+08\n\n\nmax\n1.303171e+09\n\n\n\n\n\n\n\n\n\n\nResearch\nFind out how to get for specific columns: - mean - median - cummulative sum - min - max",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Pandas"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/pandas.html#applying-function",
    "href": "Python Data Analysis/Week1/pandas.html#applying-function",
    "title": "Pandas",
    "section": "Applying Function",
    "text": "Applying Function",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Pandas"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week4/41_overview_of_machine_learning.html",
    "href": "Python Data Analysis/Week4/41_overview_of_machine_learning.html",
    "title": "Overview of Machine Learning",
    "section": "",
    "text": "from dataidea.packages import * # imports np, pd, plt etc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom dataidea.datasets import loadDataset\n\n\ndemo_df = loadDataset('demo')\n\n\ncols = {\"Age\":\"age\", \"Gender\":\"gender\", \"Marital Status\":\"marital_status\", \"Address\":\"address\",\n       \"Income\":\"income\",\"Income Category\":\"income_category\", \"Job Category\":\"job_category\",}\n\ndemo_df.rename(columns=cols, inplace=True)\n\n\ndemo_df.columns\n\nIndex(['age', 'gender', 'marital_status', 'address', 'income',\n       'income_category', 'job_category'],\n      dtype='object')\n\n\n\ndemo_df.describe() #will only give us numerical values\n\n\n\n\n\n\n\n\n\nage\naddress\nincome\nincome_category\njob_category\n\n\n\n\ncount\n200.000000\n200.000000\n200.000000\n200.000000\n200.000000\n\n\nmean\n42.475000\n11.485000\n76.305000\n2.520000\n1.950000\n\n\nstd\n12.801122\n10.365665\n107.554647\n1.065493\n0.781379\n\n\nmin\n19.000000\n0.000000\n11.000000\n1.000000\n1.000000\n\n\n25%\n32.000000\n3.000000\n27.000000\n2.000000\n1.000000\n\n\n50%\n43.000000\n9.000000\n44.500000\n2.000000\n2.000000\n\n\n75%\n51.000000\n17.000000\n76.000000\n4.000000\n3.000000\n\n\nmax\n76.000000\n51.000000\n873.000000\n4.000000\n3.000000\n\n\n\n\n\n\n\n\n\ndemo_df.select_dtypes(include=[\"object\"])\n\n\n\n\n\n\n\n\n\ngender\nmarital_status\n\n\n\n\n0\nf\n1\n\n\n1\nm\n0\n\n\n2\nf\nno answer\n\n\n3\nm\n1\n\n\n4\nm\nno answer\n\n\n...\n...\n...\n\n\n195\nf\n0\n\n\n196\nf\n1\n\n\n197\nf\n1\n\n\n198\nm\n0\n\n\n199\nm\n0\n\n\n\n\n200 rows × 2 columns\n\n\n\n\n\ndemo_df.select_dtypes(include=[\"object\"]).describe()\n\n\n\n\n\n\n\n\n\ngender\nmarital_status\n\n\n\n\ncount\n200\n200\n\n\nunique\n4\n3\n\n\ntop\nf\n0\n\n\nfreq\n99\n102\n\n\n\n\n\n\n\n\n\ndemo_df[\"gender\"].value_counts().index\n\nIndex(['f', 'm', '  f', '   m'], dtype='object', name='gender')\n\n\n\ndemo_df.gender.unique()\n\narray(['f', 'm', '  f', '   m'], dtype=object)\n\n\n\ndemo_df2 = demo_df.replace(to_replace=\"  f\", value=\"f\")\n\n\ndemo_df2.gender.unique()\n\narray(['f', 'm', '   m'], dtype=object)\n\n\n\ngender_col = demo_df2.gender.replace(to_replace=\"   m\", value=\"m\")\ngender_col\n\n0      f\n1      m\n2      f\n3      m\n4      m\n      ..\n195    f\n196    f\n197    f\n198    m\n199    m\nName: gender, Length: 200, dtype: object\n\n\n\ngender_col.unique()\n\narray(['f', 'm'], dtype=object)\n\n\n\ndemo_df2[\"gender\"] = gender_col\n\n\ndemo_df2.gender.unique()\n\narray(['f', 'm'], dtype=object)\n\n\n\ndemo_df2.marital_status.unique()\n\narray(['1', '0', 'no answer'], dtype=object)\n\n\n\ndemo_df2.marital_status.value_counts()\n\nmarital_status\n0            102\n1             93\nno answer      5\nName: count, dtype: int64\n\n\n\ndemo_df2.select_dtypes(include=[\"number\"]) #\"float64\",\"int64\"\n\n\n\n\n\n\n\n\n\nage\naddress\nincome\nincome_category\njob_category\n\n\n\n\n0\n55\n12\n72.0\n3.0\n3\n\n\n1\n56\n29\n153.0\n4.0\n3\n\n\n2\n28\n9\n28.0\n2.0\n1\n\n\n3\n24\n4\n26.0\n2.0\n1\n\n\n4\n25\n2\n23.0\n1.0\n2\n\n\n...\n...\n...\n...\n...\n...\n\n\n195\n45\n3\n86.0\n4.0\n3\n\n\n196\n23\n2\n27.0\n2.0\n1\n\n\n197\n66\n32\n11.0\n1.0\n2\n\n\n198\n49\n4\n30.0\n2.0\n1\n\n\n199\n45\n1\n147.0\n4.0\n3\n\n\n\n\n200 rows × 5 columns\n\n\n\n\n\ndemo_df2.isna().sum()\n\nage                0\ngender             0\nmarital_status     0\naddress            0\nincome             0\nincome_category    0\njob_category       0\ndtype: int64\n\n\n\nplt.boxplot(demo_df2[\"income\"])\n\n{'whiskers': [&lt;matplotlib.lines.Line2D&gt;,\n  &lt;matplotlib.lines.Line2D&gt;],\n 'caps': [&lt;matplotlib.lines.Line2D&gt;,\n  &lt;matplotlib.lines.Line2D&gt;],\n 'boxes': [&lt;matplotlib.lines.Line2D&gt;],\n 'medians': [&lt;matplotlib.lines.Line2D&gt;],\n 'fliers': [&lt;matplotlib.lines.Line2D&gt;],\n 'means': []}\n\n\n\n\n\n\n\n\n\n\n#exercise: function to calucate outliers:\n#lower fence = Q1 - 1.5(Q3-Q1)\n#upper fence = Q3 + 1.5(Q3-Q1)\n\n\ndef getOutliers(column):\n\n    q1 = np.quantile(column, 0.25)\n    q3 = np.quantile(column, 0.75)\n    interquantile_range = q3-q1\n    lower_fence = q1 - 1.5*interquantile_range\n    upper_fence = q3 + 1.5*interquantile_range\n\n    outlier_indicies = np.where((column &lt; lower_fence) | (column &gt; upper_fence))[0]\n    outliers = np.array(column[outlier_indicies])\n    return outliers, outlier_indicies\n\n\noutliers, indexes = getOutliers(demo_df2.income)\n\n\ndemo_df3 = demo_df2.drop(indexes)\n\n\nplt.hist(demo_df2.age, bins = 20, edgecolor = \"black\")\n\n(array([ 7., 12., 11., 17., 11., 10., 10., 17., 14., 18., 14., 17., 14.,\n         6.,  7.,  6.,  5.,  2.,  1.,  1.]),\n array([19.  , 21.85, 24.7 , 27.55, 30.4 , 33.25, 36.1 , 38.95, 41.8 ,\n        44.65, 47.5 , 50.35, 53.2 , 56.05, 58.9 , 61.75, 64.6 , 67.45,\n        70.3 , 73.15, 76.  ]),\n &lt;BarContainer object of 20 artists&gt;)\n\n\n\n\n\n\n\n\n\n\nplt.hist(demo_df3.income, bins=20, edgecolor=\"black\")\n\n(array([11., 24., 37., 12., 18., 18.,  8.,  9.,  9.,  9.,  6.,  2.,  2.,\n         2.,  4.,  2.,  0.,  1.,  2.,  5.]),\n array([ 11.  ,  17.85,  24.7 ,  31.55,  38.4 ,  45.25,  52.1 ,  58.95,\n         65.8 ,  72.65,  79.5 ,  86.35,  93.2 , 100.05, 106.9 , 113.75,\n        120.6 , 127.45, 134.3 , 141.15, 148.  ]),\n &lt;BarContainer object of 20 artists&gt;)\n\n\n\n\n\n\n\n\n\n\nplt.scatter(demo_df2.age, demo_df2.income)\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.scatter(demo_df3.age, demo_df3.income)\nplt.show()\n\n\n\n\n\n\n\n\n\ndemo_df2 = demo_df2[demo_df.income&lt;600]\n\n\ndemo_df2.isna().sum()\n\nage                0\ngender             0\nmarital_status     0\naddress            0\nincome             0\nincome_category    0\njob_category       0\ndtype: int64\n\n\n\ndemo_df2.head()\n\n\n\n\n\n\n\n\n\nage\ngender\nmarital_status\naddress\nincome\nincome_category\njob_category\n\n\n\n\n0\n55\nf\n1\n12\n72.0\n3.0\n3\n\n\n1\n56\nm\n0\n29\n153.0\n4.0\n3\n\n\n2\n28\nf\nno answer\n9\n28.0\n2.0\n1\n\n\n3\n24\nm\n1\n4\n26.0\n2.0\n1\n\n\n4\n25\nm\nno answer\n2\n23.0\n1.0\n2\n\n\n\n\n\n\n\n\n\ndemo_df4 = demo_df2[demo_df2.marital_status != 'no answer'].copy()\n\n\ndemo_df4.to_csv('../assets/demo_cleaned.csv', index=False)\n\n\ndemo_df4.sample(n=5)\n\n\n\n\n\n\n\n\n\nage\ngender\nmarital_status\naddress\nincome\nincome_category\njob_category\n\n\n\n\n119\n53\nf\n0\n34\n136.0\n4.0\n3\n\n\n6\n44\nm\n1\n17\n144.0\n4.0\n3\n\n\n80\n38\nm\n0\n7\n42.0\n2.0\n1\n\n\n76\n19\nf\n1\n0\n13.0\n1.0\n1\n\n\n59\n28\nm\n0\n9\n28.0\n2.0\n2\n\n\n\n\n\n\n\n\n\ndemo_df4.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 193 entries, 0 to 199\nData columns (total 7 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   age              193 non-null    int64  \n 1   gender           193 non-null    object \n 2   marital_status   193 non-null    object \n 3   address          193 non-null    int64  \n 4   income           193 non-null    float64\n 5   income_category  193 non-null    float64\n 6   job_category     193 non-null    int64  \ndtypes: float64(2), int64(3), object(2)\nmemory usage: 12.1+ KB\n\n\n\ndemo_df4['marital_status'] = demo_df4.marital_status.astype('int')\n\n\ndemo_df5 = demo_df4.copy()\n\n\ndemo_df5 = pd.get_dummies(data=demo_df5, \n                          columns=['gender'], \n                          drop_first=True, \n                          dtype='int'\n                         )\n\n\ndemo_df5.sample(n=5)\n\n\n\n\n\n\n\n\n\nage\nmarital_status\naddress\nincome\nincome_category\njob_category\ngender_m\n\n\n\n\n51\n48\n0\n22\n109.0\n4.0\n2\n1\n\n\n183\n38\n1\n18\n77.0\n4.0\n3\n0\n\n\n85\n30\n0\n4\n23.0\n1.0\n1\n0\n\n\n17\n21\n0\n1\n37.0\n2.0\n1\n1\n\n\n156\n43\n1\n5\n144.0\n4.0\n3\n1\n\n\n\n\n\n\n\n\n\nlogistic_regression_model = LogisticRegression()\n\n\nX = demo_df5.drop('marital_status', axis=1)\ny = demo_df5.marital_status\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\n\nlogistic_regression_model.fit(X, y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\nlogistic_regression_model.score(X, y) * 100\n\n54.40414507772021\n\n\n\nlogistic_regression_model.fit(X_train, y_train)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\nlogistic_regression_model.score(X_test, y_test)\n\n0.42857142857142855\n\n\n\ndemo_df2[demo_df2.marital_status == 'no answer']\n\n\n\n\n\n\n\n\n\nage\ngender\nmarital_status\naddress\nincome\nincome_category\njob_category\n\n\n\n\n2\n28\nf\nno answer\n9\n28.0\n2.0\n1\n\n\n4\n25\nm\nno answer\n2\n23.0\n1.0\n2\n\n\n7\n46\nm\nno answer\n20\n75.0\n4.0\n3\n\n\n8\n41\nm\nno answer\n10\n26.0\n2.0\n2\n\n\n9\n29\nf\nno answer\n4\n19.0\n1.0\n2\n\n\n\n\n\n\n\n\n\nlogistic_regression_model.predict([[28, 9, 28, 2, 1, 0]])\n\n/home/jumashafara/venvs/programming_for_data_science/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n  warnings.warn(\n\n\narray([0])\n\n\n\npredictions = logistic_regression_model.predict(X_test)\n\n\n# X_test['predicted_marial_status'] = predictions\n\n\ndecision_tree_classifier = DecisionTreeClassifier()\n\n\ndecision_tree_classifier.fit(X_train, y_train)\n\nDecisionTreeClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier() \n\n\n\ndecision_tree_classifier.score(X_test, y_test)\n\n0.4897959183673469\n\n\n\ndecision_tree_classifier.predict(X=[[28, 9, 28, 2, 1, 0]])\n\n/home/jumashafara/venvs/programming_for_data_science/lib/python3.10/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but DecisionTreeClassifier was fitted with feature names\n  warnings.warn(\n\n\narray([0])\n\n\n\ndecision_tree_classifier.predict(X=X_test)\n\narray([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n       0, 0, 0, 1, 1])\n\n\n\n# take in X_test, y_test\n# predictions on X_test\n# true values ie y_test\n# match which are correct\n# correct/total\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week4",
      "Overview of Machine Learning"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week4/43_sklearn-unsupervised-learning.html",
    "href": "Python Data Analysis/Week4/43_sklearn-unsupervised-learning.html",
    "title": "Unsupervised Learning using Scikit Learn - Machine Learning with Python",
    "section": "",
    "text": "This tutorial is a part of the Data Analysis with Python and Programming for Data Science Course by DATAIDEA.\nThe following topics are covered in this tutorial:\nLet’s install the required libraries.\n# # uncomment and run this cell to install the packages and libraries\n# !pip install dataidea",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week4",
      "Unsupervised Learning using Scikit Learn - Machine Learning with Python"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week4/43_sklearn-unsupervised-learning.html#introduction-to-unsupervised-learning",
    "href": "Python Data Analysis/Week4/43_sklearn-unsupervised-learning.html#introduction-to-unsupervised-learning",
    "title": "Unsupervised Learning using Scikit Learn - Machine Learning with Python",
    "section": "Introduction to Unsupervised Learning",
    "text": "Introduction to Unsupervised Learning\nUnsupervised machine learning refers to the category of machine learning techniques where models are trained on a dataset without labels. Unsupervised learning is generally use to discover patterns in data and reduce high-dimensional data to fewer dimensions. Here’s how unsupervised learning fits into the landscape of machine learning algorithms(source):\n\nHere are the topics in machine learning that we’re studying in this course (source):\n\nScikit-learn offers the following cheatsheet to decide which model to pick for a given problem. Can you identify the unsupervised learning algorithms?\n\nHere is a full list of unsupervised learning algorithms available in Scikit-learn: https://scikit-learn.org/stable/unsupervised_learning.html",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week4",
      "Unsupervised Learning using Scikit Learn - Machine Learning with Python"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week4/43_sklearn-unsupervised-learning.html#clustering",
    "href": "Python Data Analysis/Week4/43_sklearn-unsupervised-learning.html#clustering",
    "title": "Unsupervised Learning using Scikit Learn - Machine Learning with Python",
    "section": "Clustering",
    "text": "Clustering\nClustering is the process of grouping objects from a dataset such that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (Wikipedia). Scikit-learn offers several clustering algorithms. You can learn more about them here: https://scikit-learn.org/stable/modules/clustering.html\nHere is a visual representation of clustering:\n\nHere are some real-world applications of clustering:\n\nCustomer segmentation\nProduct recommendation\nFeature engineering\nAnomaly/fraud detection\nTaxonomy creation\n\nWe’ll use the Iris flower dataset to study some of the clustering algorithms available in scikit-learn. It contains various measurements for 150 flowers belonging to 3 different species.\n\nfrom dataidea.packages import sns, plt\nfrom sklearn.cluster import KMeans\n\nsns.set_style('darkgrid')\n\n\niris_df = sns.load_dataset('iris')\n\n\niris_df\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n\nsns.get_dataset_names()\nping_df = sns.load_dataset('penguins')\nping_df.columns\n\nIndex(['species', 'island', 'bill_length_mm', 'bill_depth_mm',\n       'flipper_length_mm', 'body_mass_g', 'sex'],\n      dtype='object')\n\n\n\nping_df\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nGentoo\nBiscoe\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n340\nGentoo\nBiscoe\n46.8\n14.3\n215.0\n4850.0\nFemale\n\n\n341\nGentoo\nBiscoe\n50.4\n15.7\n222.0\n5750.0\nMale\n\n\n342\nGentoo\nBiscoe\n45.2\n14.8\n212.0\n5200.0\nFemale\n\n\n343\nGentoo\nBiscoe\n49.9\n16.1\n213.0\n5400.0\nMale\n\n\n\n\n344 rows × 7 columns\n\n\n\n\n\nsns.scatterplot(data=ping_df, x='bill_length_mm', y='bill_depth_mm', hue='species');\n\n\n\n\n\n\n\n\n\nsns.scatterplot(data=iris_df, x='sepal_length', y='petal_length', hue='species');\nplt.show()\n\n\n\n\n\n\n\n\nWe’ll attempt to cluster observations using numeric columns in the data.\n\nnumeric_cols = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n\n\nX = iris_df[numeric_cols]\n\n\nX\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\n\nK Means Clustering\nThe K-means algorithm attempts to classify objects into a pre-determined number of clusters by finding optimal central points (called centroids) for each cluster. Each object is classifed as belonging the cluster represented by the closest centroid.\n\nHere’s how the K-means algorithm works:\n\nPick K random objects as the initial cluster centers.\nClassify each object into the cluster whose center is closest to the point.\nFor each cluster of classified objects, compute the centroid (mean).\nNow reclassify each object using the centroids as cluster centers.\nCalculate the total variance of the clusters (this is the measure of goodness).\nRepeat steps 1 to 6 a few more times and pick the cluster centers with the lowest total variance.\n\nHere’s a video showing the above steps: https://www.youtube.com/watch?v=4b5d3muPQmA\nLet’s apply K-means clustering to the Iris dataset.\n\nfrom sklearn.cluster import KMeans\n\n\nmodel = KMeans(n_clusters=3, random_state=42)\n\n\nmodel.fit(X)\n\nKMeans(n_clusters=3, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  KMeans?Documentation for KMeansiFittedKMeans(n_clusters=3, random_state=42) \n\n\nWe can check the cluster centers for each cluster.\n\nmodel.cluster_centers_\n\narray([[6.85384615, 3.07692308, 5.71538462, 2.05384615],\n       [5.006     , 3.428     , 1.462     , 0.246     ],\n       [5.88360656, 2.74098361, 4.38852459, 1.43442623]])\n\n\nWe can now classify points using the model.\n\nX\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\n\n\n\n\npreds = model.predict(X)\npreds\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0,\n       0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0,\n       0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2], dtype=int32)\n\n\n\nX['clusters'] = preds\n\n\nX\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nclusters\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n1\n\n\n1\n4.9\n3.0\n1.4\n0.2\n1\n\n\n2\n4.7\n3.2\n1.3\n0.2\n1\n\n\n3\n4.6\n3.1\n1.5\n0.2\n1\n\n\n4\n5.0\n3.6\n1.4\n0.2\n1\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n0\n\n\n146\n6.3\n2.5\n5.0\n1.9\n2\n\n\n147\n6.5\n3.0\n5.2\n2.0\n0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n0\n\n\n149\n5.9\n3.0\n5.1\n1.8\n2\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n\nsns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds);\ncenters_x, centers_y = model.cluster_centers_[:,0], model.cluster_centers_[:,2]\nplt.plot(centers_x, centers_y, 'xb')\n\n\n\n\n\n\n\n\nAs you can see, K-means algorithm was able to classify (for the most part) different specifies of flowers into separate clusters. Note that we did not provide the “species” column as an input to KMeans.\nWe can check the “goodness” of the fit by looking at model.inertia_, which contains the sum of squared distances of samples to their closest cluster center. Lower the inertia, better the fit.\n\nmodel.inertia_\n\n78.8556658259773\n\n\nLet’s try creating 6 clusters.\n\nmodel = KMeans(n_clusters=6, random_state=42).fit(X)\n\n\npreds = model.predict(X)\npreds\n\narray([5, 1, 1, 1, 5, 4, 1, 5, 1, 1, 4, 5, 1, 1, 4, 4, 4, 5, 4, 4, 5, 5,\n       1, 5, 5, 1, 5, 5, 5, 1, 1, 5, 4, 4, 1, 5, 4, 5, 1, 5, 5, 1, 1, 5,\n       4, 1, 4, 1, 4, 5, 3, 0, 3, 2, 0, 0, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0,\n       0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 3, 0, 2, 2, 2, 2, 0, 2, 0, 0, 0,\n       2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 3, 0, 3, 3, 3, 3, 2, 3, 3, 3,\n       3, 3, 3, 0, 0, 3, 3, 3, 3, 0, 3, 0, 3, 0, 3, 3, 0, 0, 3, 3, 3, 3,\n       3, 0, 3, 3, 3, 3, 0, 3, 3, 3, 0, 3, 3, 3, 0, 3, 3, 0], dtype=int32)\n\n\n\nsns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=preds);\n\n\n\n\n\n\n\n\n\n# Let's calculate the new model inertia\n\nmodel.inertia_\n\n50.560990643274856\n\n\nIn most real-world scenarios, there’s no predetermined number of clusters. In such a case, you can create a plot of “No. of clusters” vs “Inertia” to pick the right number of clusters.\n\noptions = range(2, 11)\ninertias = []\n\nfor n_clusters in options:\n    model = KMeans(n_clusters, random_state=42).fit(X)\n    inertias.append(model.inertia_)\n    \nplt.title(\"No. of clusters vs. Inertia\")\nplt.plot(options, inertias, '-o')\nplt.xlabel('No. of clusters (K)')\nplt.ylabel('Inertia');\n\n\n\n\n\n\n\n\nThe chart is creates an “elbow” plot, and you can pick the number of clusters beyond which the reduction in inertia decreases sharply.\nMini Batch K Means: The K-means algorithm can be quite slow for really large dataset. Mini-batch K-means is an iterative alternative to K-means that works well for large datasets. Learn more about it here: https://scikit-learn.org/stable/modules/clustering.html#mini-batch-kmeans\n\nEXERCISE: Perform clustering on the Mall customers dataset on Kaggle. Study the segments carefully and report your observations.\n\n\n\nDBSCAN\nDensity-based spatial clustering of applications with noise (DBSCAN) uses the density of points in a region to form clusters. It has two main parameters: “epsilon” and “min samples” using which it classifies each point as a core point, reachable point or noise point (outlier).\n\nHere’s a video explaining how the DBSCAN algorithm works: https://www.youtube.com/watch?v=C3r7tGRe2eI\n\nfrom sklearn.cluster import DBSCAN\n\n\nmodel = DBSCAN(eps=1.1, min_samples=4)\n\n\nmodel.fit(X)\n\nDBSCAN(eps=1.1, min_samples=4)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DBSCAN?Documentation for DBSCANiFittedDBSCAN(eps=1.1, min_samples=4) \n\n\nIn DBSCAN, there’s no prediction step. It directly assigns labels to all the inputs.\n\nmodel.labels_\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1,\n       1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 1,\n       1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 2])\n\n\n\nsns.scatterplot(data=X, x='sepal_length', y='petal_length', hue=model.labels_);\n\n\n\n\n\n\n\n\n\nEXERCISE: Try changing the values of eps and min_samples and observe how the number of clusters the classification changes.\n\nHere’s how the results of DBSCAN and K Means differ:\n\n\n\nHierarchical Clustering\nHierarchical clustering, as the name suggests, creates a hierarchy or a tree of clusters.\n\nWhile there are several approaches to hierarchical clustering, the most common approach works as follows:\n\nMark each point in the dataset as a cluster.\nPick the two closest cluster centers without a parent and combine them into a new cluster.\nThe new cluster is the parent cluster of the two clusters, and its center is the mean of all the points in the cluster.\nRepeat steps 2 and 3 till there’s just one cluster left.\n\nWatch this video for a visual explanation of hierarchical clustering: https://www.youtube.com/watch?v=7xHsRkOdVwo\n\nEXERCISE: Implement hierarchical clustering for the Iris dataset using scikit-learn.\n\nThere are several other clustering algorithms in Scikit-learn. You can learn more about them and when to use them here: https://scikit-learn.org/stable/modules/clustering.html\nLet’s save our work before continuing.",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week4",
      "Unsupervised Learning using Scikit Learn - Machine Learning with Python"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week4/43_sklearn-unsupervised-learning.html#dimensionality-reduction-and-manifold-learning",
    "href": "Python Data Analysis/Week4/43_sklearn-unsupervised-learning.html#dimensionality-reduction-and-manifold-learning",
    "title": "Unsupervised Learning using Scikit Learn - Machine Learning with Python",
    "section": "Dimensionality Reduction and Manifold Learning",
    "text": "Dimensionality Reduction and Manifold Learning\nIn machine learning problems, we often encounter datasets with a very large number of dimensions (features or columns). Dimensionality reduction techniques are used to reduce the number of dimensions or features within the data to a manageable or convenient number.\nApplications of dimensionality reduction:\n\nReducing size of data without loss of information\nTraining machine learning models efficiently\nVisualizing high-dimensional data in 2/3 dimensions\n\n\nPrincipal Component Analysis (PCA)\nPrincipal component is a dimensionality reduction technique that uses linear projections of data to reduce their dimensions, while attempting to maximize the variance of data in the projection. Watch this video to learn how PCA works: https://www.youtube.com/watch?v=FgakZw6K1QQ\nHere’s an example of PCA to reduce 2D data to 1D:\n\nHere’s an example of PCA to reduce 3D data to 2D:\n\nLet’s apply Principal Component Analysis to the Iris dataset.\n\niris_df = sns.load_dataset('iris')\niris_df\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\nvirginica\n\n\n146\n6.3\n2.5\n5.0\n1.9\nvirginica\n\n\n147\n6.5\n3.0\n5.2\n2.0\nvirginica\n\n\n148\n6.2\n3.4\n5.4\n2.3\nvirginica\n\n\n149\n5.9\n3.0\n5.1\n1.8\nvirginica\n\n\n\n\n150 rows × 5 columns\n\n\n\n\n\nnumeric_cols\n\n['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n\n\n\nfrom sklearn.decomposition import PCA\n\n\npca = PCA(n_components=2)\n\n\npca.fit(iris_df[numeric_cols])\n\nPCA(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  PCA?Documentation for PCAiFittedPCA(n_components=2) \n\n\n\npca\n\nPCA(n_components=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  PCA?Documentation for PCAiFittedPCA(n_components=2) \n\n\n\ntransformed = pca.transform(iris_df[numeric_cols])\n\n\nsns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=iris_df['species']);\n\n\n\n\n\n\n\n\nAs you can see, the PCA algorithm has done a very good job of separating different species of flowers using just 2 measures.\n\nEXERCISE: Apply Principal Component Analysis to a large high-dimensional dataset and train a machine learning model using the low-dimensional results. Observe the changes in the loss and training time for different numbers of target dimensions.\n\nLearn more about Principal Component Analysis here: https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n\n\nt-Distributed Stochastic Neighbor Embedding (t-SNE)\nManifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high. Scikit-learn provides many algorithms for manifold learning: https://scikit-learn.org/stable/modules/manifold.html . A commonly-used manifold learning technique is t-Distributed Stochastic Neighbor Embedding or t-SNE, used to visualize high dimensional data in one, two or three dimensions.\nHere’s a visual representation of t-SNE applied to visualize 2 dimensional data in 1 dimension:\n\nHere’s a visual representation of t-SNE applied to the MNIST dataset, which contains 28px x 28px images of handrwritten digits 0 to 9, a reduction from 784 dimensions to 2 dimensions (source):\n\nHere’s a video explaning how t-SNE works: https://www.youtube.com/watch?v=NEaUSP4YerM\n\nfrom sklearn.manifold import TSNE\n\n\ntsne = TSNE(n_components=2)\n\n\ntransformed = tsne.fit_transform(iris_df[numeric_cols])\n\n\nsns.scatterplot(x=transformed[:,0], y=transformed[:,1], hue=iris_df['species']);\n\n\n\n\n\n\n\n\nAs you can see, the flowers from the same species are clustered very closely together. The relative distance between the species is also conveyed by the gaps between the clusters.\n\nEXERCISE: Use t-SNE to visualize the MNIST handwritten digits dataset.",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week4",
      "Unsupervised Learning using Scikit Learn - Machine Learning with Python"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week4/43_sklearn-unsupervised-learning.html#summary-and-references",
    "href": "Python Data Analysis/Week4/43_sklearn-unsupervised-learning.html#summary-and-references",
    "title": "Unsupervised Learning using Scikit Learn - Machine Learning with Python",
    "section": "Summary and References",
    "text": "Summary and References\n\nThe following topics were covered in this tutorial:\n\nOverview of unsupervised learning algorithms in Scikit-learn\nClustering algorithms: K Means, DBScan, Hierarchical clustering etc.\nDimensionality reduction (PCA) and manifold learning (t-SNE)\n\nCheck out these resources to learn more:\n\nhttps://www.coursera.org/learn/machine-learning\nhttps://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\nhttps://scikit-learn.org/stable/unsupervised_learning.html\nhttps://scikit-learn.org/stable/modules/clustering.html",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week4",
      "Unsupervised Learning using Scikit Learn - Machine Learning with Python"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week4/43_sklearn-unsupervised-learning.html#credit",
    "href": "Python Data Analysis/Week4/43_sklearn-unsupervised-learning.html#credit",
    "title": "Unsupervised Learning using Scikit Learn - Machine Learning with Python",
    "section": "Credit",
    "text": "Credit\n\nDo you seriously want to learn Programming and Data Analysis with Python?\nIf you’re serious about learning Programming, Data Analysis with Python and getting prepared for Data Science roles, I highly encourage you to enroll in my Programming for Data Science Course, which I’ve taught to hundreds of students. Don’t waste your time following disconnected, outdated tutorials\nMy Complete Programming for Data Science Course has everything you need in one place.\nThe course offers:\n\nDuration: Usually 3-4 months\nSessions: Four times a week (one on one)\nLocation: Online or/and at UMF House, Sir Apollo Kagwa Road\n\nWhat you’l learn:\n\nFundamentals of programming\nData manipulation and analysis\nVisualization techniques\nIntroduction to machine learning\nDatabase Management with SQL (optional)\nWeb Development with Django (optional)\n\nBest\nJuma Shafara\nData Scientist, Instructor\njumashafara0@gmail.com / dataideaorg@gmail.com\n+256701520768 / +256771754118",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week4",
      "Unsupervised Learning using Scikit Learn - Machine Learning with Python"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/73_normailization_and_standardization.html",
    "href": "Python Data Analysis/Week7/73_normailization_and_standardization.html",
    "title": "Normalization vs Standardization",
    "section": "",
    "text": "In data analysis and machine learning, preprocessing steps such as data normalization and standardization are crucial for improving the performance and interpretability of models. This Jupyter Notebook provides an overview of the importance of data normalization and standardization in preparing data for analysis and modeling.",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Normalization vs Standardization"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/73_normailization_and_standardization.html#normalization",
    "href": "Python Data Analysis/Week7/73_normailization_and_standardization.html#normalization",
    "title": "Normalization vs Standardization",
    "section": "Normalization",
    "text": "Normalization\n\nNormalization: Normalization typically refers to scaling numerical features to a common scale, often between 0 and 1. This is usually done by subtracting the minimum value and then dividing by the range (maximum - minimum). Normalization is useful when the distribution of the data does not follow a Gaussian distribution (Normal Distribution).\n\n\n# Data Normalization without libraries:\ndef minMaxScaling(data):\n    min_val = min(data)\n    max_val = max(data)\n    \n    scaled_data = []\n    for value in data:\n        scaled = (value - min_val) / (max_val - min_val)\n        scaled_data.append(scaled)\n    return scaled_data\n\n\n# Example data\ndata = np.array([10, 20, 30, 40, 50])\nnormalized_data = minMaxScaling(data)\nprint(\"Normalized data (Min-Max Scaling):\", normalized_data)\n\nNormalized data (Min-Max Scaling): [0.0, 0.25, 0.5, 0.75, 1.0]\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Sample data\ndata = np.array([[1, 2], [3, 4], [5, 6]])\n\n# Create the scaler\nscaler = MinMaxScaler()\n\n# Fit the scaler to the data and transform the data\nnormalized_data = scaler.fit_transform(data)\n\nprint(\"Original data:\")\nprint(data)\nprint(\"\\nNormalized data:\")\nprint(normalized_data)\n\nOriginal data:\n[[1 2]\n [3 4]\n [5 6]]\n\nNormalized data:\n[[0.  0. ]\n [0.5 0.5]\n [1.  1. ]]",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Normalization vs Standardization"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/73_normailization_and_standardization.html#standardization",
    "href": "Python Data Analysis/Week7/73_normailization_and_standardization.html#standardization",
    "title": "Normalization vs Standardization",
    "section": "Standardization",
    "text": "Standardization\n\nStandardization: Standardization, often implemented with a method like z-score standardization, transforms the data to have a mean of 0 and a standard deviation of 1. This means that the data will have a Gaussian distribution (if the original data had a Gaussian distribution). In Python, you typically use the StandardScaler from the sklearn.preprocessing module to standardize data.\n\n\ndef zScoreNormalization(data):\n    mean = sum(data) / len(data)\n    variance = sum((x - mean) ** 2 for x in data) / len(data)\n    std_dev = variance ** 0.5\n    standardized_data = [(x - mean) / std_dev for x in data]\n    return standardized_data\n\n\n# Example data\ndata = [10, 20, 30, 40, 50]\nstandardized_data = zScoreNormalization(data)\nprint(\"Standardized data (Z-Score Normalization):\", standardized_data)\n\nStandardized data (Z-Score Normalization): [-1.414213562373095, -0.7071067811865475, 0.0, 0.7071067811865475, 1.414213562373095]\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Sample data\ndata = np.array([[1, 2, 3], [3, 4, 5], [5, 6, 7]])\n\n# Create the scaler\nscaler = StandardScaler()\n\n# Fit the scaler to the data and transform the data\nstandardized_data = scaler.fit_transform(data)\n\nprint(\"Original data:\")\nprint(data)\nprint(\"\\nStandardized data:\")\nprint(standardized_data)\n\nOriginal data:\n[[1 2 3]\n [3 4 5]\n [5 6 7]]\n\nStandardized data:\n[[-1.22474487 -1.22474487 -1.22474487]\n [ 0.          0.          0.        ]\n [ 1.22474487  1.22474487  1.22474487]]",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Normalization vs Standardization"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/73_normailization_and_standardization.html#which-one",
    "href": "Python Data Analysis/Week7/73_normailization_and_standardization.html#which-one",
    "title": "Normalization vs Standardization",
    "section": "Which one?",
    "text": "Which one?\nThe choice between normalization and standardization depends on your data and the requirements of your analysis. Here are some guidelines to help you decide:\n\nNormalization:\n\nUse normalization when the scale of features is meaningful and should be preserved.\nNormalize data when you’re working with algorithms that require input features to be on a similar scale, such as algorithms using distance metrics like k-nearest neighbors or clustering algorithms like K-means.\nIf the distribution of your data is not Gaussian and you want to scale the features to a fixed range, normalization might be a better choice.\n\nStandardization:\n\nUse standardization when the distribution of your data is Gaussian or when you’re unsure about the distribution.\nStandardization is less affected by outliers compared to normalization, making it more suitable when your data contains outliers.\nIf you’re working with algorithms that assume your data is normally distributed, such as linear regression, logistic regression, standardization is typically preferred.\n\n\nIn some cases, you might experiment with both approaches and see which one yields better results for your specific dataset and analysis. Additionally, it’s always a good practice to understand your data and the underlying assumptions of the algorithms you’re using to make informed decisions about data preprocessing techniques.",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Normalization vs Standardization"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/72_why_scaling_fpl.html",
    "href": "Python Data Analysis/Week7/72_why_scaling_fpl.html",
    "title": "Why re-scale data (fpl)?",
    "section": "",
    "text": "In this notebook, we’ll use Kmeans clustering to demonstrate the importance of scaling data\nK-means clustering is an unsupervised machine learning algorithm used for partitioning a dataset into K distinct, non-overlapping clusters. The goal of K-means is to minimize the sum of squared distances between data points and their respective cluster centroids.\nHere’s how the K-means algorithm works:\nHere’s a video showing the above steps: https://www.youtube.com/watch?v=4b5d3muPQmA\nFirst, let’s import the necessary libraries and load the Iris dataset:\nfrom dataidea.packages import pd, plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom dataidea.datasets import loadDataset\nfpl_data = loadDataset('fpl')\nWe can be able to load it like this because it inbuilt in the dataidea package\nNow let’s pick out a few numeric columns that we might consider moving forward\nfpl_sample = fpl_data[['Goals_Scored', 'Assists','Total_Points', \n                       'Minutes', 'Saves', 'Goals_Conceded', \n                       'Creativity', 'Influence'\n                      ]]\nfpl_sample.head()\n\n\n\n\n\n\n\n\n\nGoals_Scored\nAssists\nTotal_Points\nMinutes\nSaves\nGoals_Conceded\nCreativity\nInfluence\n\n\n\n\n0\n18\n14\n244\n3101\n0\n36\n1414.9\n1292.6\n\n\n1\n23\n14\n242\n3083\n0\n39\n659.1\n1318.2\n\n\n2\n22\n6\n231\n3077\n0\n41\n825.7\n1056.0\n\n\n3\n17\n11\n228\n3119\n0\n36\n1049.9\n1052.2\n\n\n4\n17\n11\n194\n3052\n0\n50\n371.0\n867.2",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Why re-scale data (fpl)?"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/72_why_scaling_fpl.html#clustering-without-scaling",
    "href": "Python Data Analysis/Week7/72_why_scaling_fpl.html#clustering-without-scaling",
    "title": "Why re-scale data (fpl)?",
    "section": "Clustering without Scaling",
    "text": "Clustering without Scaling\nNext, let’s perform K-means clustering on the original dataset without scaling the features:\n\n# Apply K-means clustering without scaling\nkmeans_unscaled = KMeans(n_clusters=4, random_state=42)\nkmeans_unscaled.fit(fpl_sample)\n\n# Get the cluster centers and labels\ncentroids_unscaled = kmeans_unscaled.cluster_centers_\nlabels_unscaled = kmeans_unscaled.labels_\n\nLet’s see the performance\n\n# Visualize clusters without scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(fpl_sample.Assists, fpl_sample.Goals_Scored, c=labels_unscaled, cmap='viridis',)\n\nplt.show()\n\n\n\n\n\n\n\n\nYou’ll notice that the clusters may not seem well-separated or meaningful. This is because the features of the Iris dataset have different scales,",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Why re-scale data (fpl)?"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/72_why_scaling_fpl.html#clustering-after-scaling",
    "href": "Python Data Analysis/Week7/72_why_scaling_fpl.html#clustering-after-scaling",
    "title": "Why re-scale data (fpl)?",
    "section": "Clustering after Scaling",
    "text": "Clustering after Scaling\nNow, let’s repeat the process after scaling the features using StandardScaler:\n\n# Scale the features\nscaler = StandardScaler()\nfpl_sample_scaled = scaler.fit_transform(fpl_sample)\n\n# Transform scaled features back to DataFrame\nfpl_sample_scaled_dataframe = pd.DataFrame(fpl_sample_scaled, columns=fpl_sample.columns)\n\n# Apply K-means clustering on scaled features\nkmeans_scaled = KMeans(n_clusters=4, random_state=42)\nkmeans_scaled.fit(fpl_sample_scaled)\n\n# Get the cluster centers and labels\ncentroids_scaled = kmeans_scaled.cluster_centers_\nlabels_scaled = kmeans_scaled.labels_\n\n\n# Visualize clusters without scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(fpl_sample_scaled_dataframe.Assists, \n            fpl_sample_scaled_dataframe.Goals_Scored, \n            c=labels_scaled, cmap='viridis')\n\nplt.show()\n\n\n\n\n\n\n\n\nYou should see clearer and more meaningful clusters after scaling the features, demonstrating the importance of feature scaling for K-means clustering, especially when dealing with datasets with features of different scales.",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Why re-scale data (fpl)?"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/72_why_scaling_fpl.html#take-away",
    "href": "Python Data Analysis/Week7/72_why_scaling_fpl.html#take-away",
    "title": "Why re-scale data (fpl)?",
    "section": "Take away",
    "text": "Take away\nIf the data doesn’t follow a standard scale, meaning the features have different scales or variances, it can lead to some issues when applying K-means clustering:\n\nUnequal feature influence: Features with larger scales or variances can dominate the clustering process. Since K-means relies on Euclidean distance, features with larger scales will contribute more to the distance calculation, potentially biasing the clustering results towards those features.\nIncorrect cluster shapes: K-means assumes that clusters are isotropic (spherical) and have similar variances along all dimensions. If the data has features with different scales, clusters may be stretched along certain dimensions, leading to suboptimal cluster assignments.\nConvergence speed: Features with larger scales can cause centroids to move more quickly towards areas with denser data, potentially affecting the convergence speed of the algorithm.\n\nBy scaling the data before clustering, you ensure that each feature contributes equally to the distance calculations, helping to mitigate the issues associated with different feature scales. This can lead to more accurate and reliable clustering results.",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Why re-scale data (fpl)?"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/76_handling_missing_data.html",
    "href": "Python Data Analysis/Week7/76_handling_missing_data.html",
    "title": "Handling Missing Data",
    "section": "",
    "text": "Missing data is a common hurdle in data analysis, impacting the reliability of insights drawn from datasets. Python offers a range of solutions to address this issue, some of which we discussed in the earlier weeks. In this notebook, we look into the top three missing data imputation methods in Python—SimpleImputer, KNNImputer, and IterativeImputer from scikit-learn—providing insights into their functionalities and practical considerations. We’ll explore these essential techniques, using the weather dataset.\n\n# install the libraries for this demonstration\n# ! pip install dataidea==0.2.5\n\n\nimport pandas as pd\nfrom dataidea.datasets import loadDataset\n\nfrom dataidea.packages import * imports for us np, pd, plt, etc. loadDataset allows us to load datasets inbuilt in the dataidea library\n\nweather = loadDataset('weather')\n\n\nweather\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nNaN\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\n\nweather.isna().sum()\n\nday            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64\n\n\nLet’s demonstrate how to use the top three missing data imputation methods—SimpleImputer, KNNImputer, and IterativeImputer—using the simple weather dataset.\n\n# select age from the data\ntemp_wind = weather[['temperature', 'windspead']].copy()\n\n\ntemp_wind_imputed = temp_wind.copy()",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/76_handling_missing_data.html#introduction",
    "href": "Python Data Analysis/Week7/76_handling_missing_data.html#introduction",
    "title": "Handling Missing Data",
    "section": "",
    "text": "Missing data is a common hurdle in data analysis, impacting the reliability of insights drawn from datasets. Python offers a range of solutions to address this issue, some of which we discussed in the earlier weeks. In this notebook, we look into the top three missing data imputation methods in Python—SimpleImputer, KNNImputer, and IterativeImputer from scikit-learn—providing insights into their functionalities and practical considerations. We’ll explore these essential techniques, using the weather dataset.\n\n# install the libraries for this demonstration\n# ! pip install dataidea==0.2.5\n\n\nimport pandas as pd\nfrom dataidea.datasets import loadDataset\n\nfrom dataidea.packages import * imports for us np, pd, plt, etc. loadDataset allows us to load datasets inbuilt in the dataidea library\n\nweather = loadDataset('weather')\n\n\nweather\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nNaN\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\n\nweather.isna().sum()\n\nday            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64\n\n\nLet’s demonstrate how to use the top three missing data imputation methods—SimpleImputer, KNNImputer, and IterativeImputer—using the simple weather dataset.\n\n# select age from the data\ntemp_wind = weather[['temperature', 'windspead']].copy()\n\n\ntemp_wind_imputed = temp_wind.copy()",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/76_handling_missing_data.html#simpleimputer-from-scikit-learn",
    "href": "Python Data Analysis/Week7/76_handling_missing_data.html#simpleimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "SimpleImputer from scikit-learn:",
    "text": "SimpleImputer from scikit-learn:\n\nUsage: SimpleImputer is a straightforward method for imputing missing values by replacing them with a constant, mean, median, or most frequent value along each column.\nPros:\n\nEasy to use and understand.\nCan handle both numerical and categorical data.\nOffers flexibility with different imputation strategies.\n\nCons:\n\nIt doesn’t consider relationships between features.\nMay not be the best choice for datasets with complex patterns of missingness.\n\nExample:\n\n\nfrom sklearn.impute import SimpleImputer\n\nsimple_imputer = SimpleImputer(strategy='mean')\ntemp_wind_simple_imputed = simple_imputer.fit_transform(temp_wind)\n\ntemp_wind_simple_imputed_df = pd.DataFrame(temp_wind_simple_imputed, columns=temp_wind.columns)\n\nLet’s have a look at the outcome\n\ntemp_wind_simple_imputed_df\n\n\n\n\n\n\n\n\n\ntemperature\nwindspead\n\n\n\n\n0\n32.0\n6.0\n\n\n1\n33.2\n9.0\n\n\n2\n28.0\n8.4\n\n\n3\n33.2\n7.0\n\n\n4\n32.0\n8.4\n\n\n5\n33.2\n8.4\n\n\n6\n33.2\n8.4\n\n\n7\n34.0\n8.0\n\n\n8\n40.0\n12.0",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/76_handling_missing_data.html#knnimputer-from-scikit-learn",
    "href": "Python Data Analysis/Week7/76_handling_missing_data.html#knnimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "KNNImputer from scikit-learn:",
    "text": "KNNImputer from scikit-learn:\n\nUsage: KNNImputer imputes missing values using k-nearest neighbors, replacing them with the mean value of the nearest neighbors.\nPros:\n\nConsiders relationships between features, making it suitable for datasets with complex patterns of missingness.\nCan handle both numerical and categorical data.\n\nCons:\n\nComputationally expensive for large datasets.\nRequires careful selection of the number of neighbors (k).\n\nExample:\n\n\nfrom sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=2)\ntemp_wind_knn_imputed = knn_imputer.fit_transform(temp_wind)\n\ntemp_wind_knn_imputed_df = pd.DataFrame(temp_wind_knn_imputed, columns=temp_wind.columns)\n\nIf we take a look at the outcome\n\ntemp_wind_knn_imputed_df\n\n\n\n\n\n\n\n\n\ntemperature\nwindspead\n\n\n\n\n0\n32.0\n6.0\n\n\n1\n33.0\n9.0\n\n\n2\n28.0\n7.0\n\n\n3\n33.0\n7.0\n\n\n4\n32.0\n7.0\n\n\n5\n33.2\n8.4\n\n\n6\n33.2\n8.4\n\n\n7\n34.0\n8.0\n\n\n8\n40.0\n12.0",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/76_handling_missing_data.html#iterativeimputer-from-scikit-learn",
    "href": "Python Data Analysis/Week7/76_handling_missing_data.html#iterativeimputer-from-scikit-learn",
    "title": "Handling Missing Data",
    "section": "IterativeImputer from scikit-learn:",
    "text": "IterativeImputer from scikit-learn:\n\nUsage: IterativeImputer models each feature with missing values as a function of other features and uses that estimate for imputation. It iteratively estimates the missing values.\nPros:\n\nTakes into account relationships between features, making it suitable for datasets with complex missing patterns.\nMore robust than SimpleImputer for handling missing data.\n\nCons:\n\nCan be computationally intensive and slower than SimpleImputer.\nRequires careful tuning of model parameters.\n\nExample:\n\n\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\niterative_imputer = IterativeImputer()\ntemp_wind_iterative_imputed = iterative_imputer.fit_transform(temp_wind)\n\ntemp_wind_iterative_imputed_df = pd.DataFrame(temp_wind_iterative_imputed, columns=temp_wind.columns)\n\nLet’s take a look at the outcome\n\ntemp_wind_iterative_imputed_df\n\n\n\n\n\n\n\n\n\ntemperature\nwindspead\n\n\n\n\n0\n32.000000\n6.000000\n\n\n1\n35.773287\n9.000000\n\n\n2\n28.000000\n3.321648\n\n\n3\n33.042537\n7.000000\n\n\n4\n32.000000\n6.238915\n\n\n5\n33.545118\n7.365795\n\n\n6\n33.545118\n7.365795\n\n\n7\n34.000000\n8.000000\n\n\n8\n40.000000\n12.000000",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/76_handling_missing_data.html#datawig",
    "href": "Python Data Analysis/Week7/76_handling_missing_data.html#datawig",
    "title": "Handling Missing Data",
    "section": "Datawig:",
    "text": "Datawig:\nDatawig is a library specifically designed for imputing missing values in tabular data using deep learning models.\n\n# import datawig\n\n# # Impute missing values\n# df_imputed = datawig.SimpleImputer.complete(weather)\n\nThese top imputation methods offer different trade-offs in terms of computational complexity, handling of missing data patterns, and ease of use. The choice between them depends on the specific characteristics of the dataset and the requirements of the analysis.",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/76_handling_missing_data.html#homework",
    "href": "Python Data Analysis/Week7/76_handling_missing_data.html#homework",
    "title": "Handling Missing Data",
    "section": "Homework",
    "text": "Homework\n\nTry out these techniques for categorical data",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Handling Missing Data"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week8/81_regression_metrics.html",
    "href": "Python Data Analysis/Week8/81_regression_metrics.html",
    "title": "Regression Metrics",
    "section": "",
    "text": "In regression tasks, the goal is to predict continuous numerical values. Scikit-learn provides several metrics to evaluate the performance of regression models.\n\nfrom dataidea.datasets import loadDataset\n\nvg= loadDataset('vgsales')\n\n\nvg[['Publisher', 'Genre']]\n\n\n\n\n\n\n\n\n\nPublisher\nGenre\n\n\n\n\n0\nNintendo\nSports\n\n\n1\nNintendo\nPlatform\n\n\n2\nNintendo\nRacing\n\n\n3\nNintendo\nSports\n\n\n4\nNintendo\nRole-Playing\n\n\n...\n...\n...\n\n\n16593\nKemco\nPlatform\n\n\n16594\nInfogrames\nShooter\n\n\n16595\nActivision\nRacing\n\n\n16596\n7G//AMES\nPuzzle\n\n\n16597\nWanadoo\nPlatform\n\n\n\n\n16598 rows × 2 columns\n\n\n\n\n\n# True labels\ny_true = [2.5, 3.7, 5.1, 4.2, 6.8]\n# Predicted labels\ny_pred = [2.3, 3.5, 4.9, 4.0, 6.5]\n\n\nMean Absolute Error (MAE):\n\nMAE measures the average absolute errors between predicted values and actual values.\nImagine you’re trying to hit a target with darts. The MAE is like calculating the average distance between where your darts hit and the bullseye. You just sum up how far each dart landed from the center (without caring if it was too short or too far) and then find the average. The smaller the MAE, the closer your predictions are to the actual values.\nFormula: $ = {i=1}^{n} |y{} - y_{}| $\n\n\nfrom sklearn.metrics import mean_absolute_error\n\n# Calculate Mean Absolute Error (MAE)\nmae = mean_absolute_error(y_true, y_pred)\nprint(\"Mean Absolute Error (MAE):\", mae)\n\nMean Absolute Error (MAE): 0.21999999999999992\n\n\n\n\nMean Squared Error (MSE):\n\nMSE measures the average of the squares of the errors between predicted values and actual values.\nThis is similar to MAE, but instead of just adding up the distances, you square them before averaging. Squaring makes bigger differences more noticeable (by making them even bigger), so MSE penalizes larger errors more than smaller ones.\nFormula: $ = {i=1}^{n} (y{} - y_{})^2 $\n\n\nfrom sklearn.metrics import mean_squared_error\n\n# Calculate Mean Squared Error (MSE)\nmse = mean_squared_error(y_true, y_pred)\nprint(\"Mean Squared Error (MSE):\", mse)\n\nMean Squared Error (MSE): 0.04999999999999997\n\n\n\n\nRoot Mean Squared Error (RMSE):\n\nRMSE is the square root of the MSE, providing a more interpretable scale since it’s in the same units as the target variable.\nIt’s just like MSE, but we take the square root of the result. This brings the error back to the same scale as the original target variable, which makes it easier to interpret. RMSE gives you an idea of how spread out your errors are in the same units as your data.\nFormula: $ = $\n\n\nfrom sklearn.metrics import root_mean_squared_error\n\n# Calculate Root Mean Squared Error (RMSE)\nrmse = root_mean_squared_error(y_true, y_pred,)\nprint(\"Root Mean Squared Error (RMSE):\", rmse)\n\nRoot Mean Squared Error (RMSE): 0.2236067977499789\n\n\n\nR-squared (Coefficient of Determination):\n\nR-squared measures the proportion of the variance in the dependent variable that is predictable from the independent variables.\nThis tells you how well your model’s predictions match the actual data compared to a simple average. If R-squared is 1, it means your model perfectly predicts the target variable. If it’s 0, it means your model is no better than just predicting the mean of the target variable. So, the closer R-squared is to 1, the better your model fits the data.\nFormula: $ R^2 = 1 - $, where $ {y}_{} $ is the mean of the observed data.\n\n\n\nfrom sklearn.metrics import r2_score\n\n# Calculate R-squared (Coefficient of Determination)\nr2 = r2_score(y_true, y_pred)\nprint(\"R-squared (R2 Score):\", r2)\n\nR-squared (R2 Score): 0.975896644812958\n\n\nUnderstanding these metrics can help you assess the performance of your regression model and make necessary adjustments to improve its accuracy.\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week8",
      "Regression Metrics"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week6/63.visual_eda.html",
    "href": "Python Data Analysis/Week6/63.visual_eda.html",
    "title": "Visual EDA",
    "section": "",
    "text": "# Import necessary libraries\nfrom dataidea.packages import *\nfrom dataidea.datasets import loadDataset\nfrom sklearn.ensemble import RandomForestClassifier\n# Load the dataset\ndemo_df = loadDataset('../assets/demo_cleaned.csv', inbuilt=False, file_type='csv')\ndemo_df.head()\n\n\n\n\n\n\n\n\n\nage\ngender\nmarital_status\naddress\nincome\nincome_category\njob_category\n\n\n\n\n0\n55\nf\n1\n12\n72.0\n3.0\n3\n\n\n1\n56\nm\n0\n29\n153.0\n4.0\n3\n\n\n2\n24\nm\n1\n4\n26.0\n2.0\n1\n\n\n3\n45\nm\n0\n9\n76.0\n4.0\n2\n\n\n4\n44\nm\n1\n17\n144.0\n4.0\n3\ndemo_df2 = pd.get_dummies(demo_df, ['gender'], dtype=int, drop_first=1)\ndemo_df2.head()\n\n\n\n\n\n\n\n\n\nage\nmarital_status\naddress\nincome\nincome_category\njob_category\ngender_m\n\n\n\n\n0\n55\n1\n12\n72.0\n3.0\n3\n0\n\n\n1\n56\n0\n29\n153.0\n4.0\n3\n1\n\n\n2\n24\n1\n4\n26.0\n2.0\n1\n1\n\n\n3\n45\n0\n9\n76.0\n4.0\n2\n1\n\n\n4\n44\n1\n17\n144.0\n4.0\n3\n1",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week6",
      "Visual EDA"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week6/63.visual_eda.html#correlation-heatmap",
    "href": "Python Data Analysis/Week6/63.visual_eda.html#correlation-heatmap",
    "title": "Visual EDA",
    "section": "Correlation Heatmap:",
    "text": "Correlation Heatmap:\n\nA heatmap showing the correlation between numerical features.\n\n\n# 3. Correlation Heatmap\nplt.figure(figsize=(8, 6))\nsns.heatmap(demo_df2[['age', 'income']].corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n\n\n\n\n\n\n\n\n\nMissing Values Matrix:\n\nA matrix indicating missing values in different features.\n\n\n# 4. Missing Values Matrix\nplt.figure(figsize=(8, 6))\nsns.heatmap(demo_df2.isnull(), cmap='viridis')\n\n\n\n\n\n\n\n\n\nFeature Importance Plot:\n\nAfter training a model (e.g., random forest), we can visualize feature importances to see which features contribute the most to predicting survival.\n\n\n\n# 5. Feature Importance Plot\n# Prepare data for training\nX = demo_df2.drop(['marital_status'], axis=1)\ny = demo_df2['marital_status']\n\n# Train Random Forest Classifier\nrf_classifier = RandomForestClassifier()\nrf_classifier.fit(X, y)\n\n# Plot feature importances\nplt.figure(figsize=(10, 6))\nimportances = rf_classifier.feature_importances_\nindices = np.argsort(importances)[::-1]\nsns.barplot(x=importances[indices], y=X.columns[indices], palette='viridis', hue=X.columns[indices])\n\nplt.show()",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week6",
      "Visual EDA"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week6/62_seaborn_part2.html",
    "href": "Python Data Analysis/Week6/62_seaborn_part2.html",
    "title": "Seaborn Part 2",
    "section": "",
    "text": "The Python visualization library Seaborn is based on matplotlib and provides a high-level interface for drawing attractive statistical graphics.\nMake use of the following aliases to import the libraries:\nfrom dataidea.packages import plt, sns, np, pd\nThe basic steps to creating plots with Seaborn are\ntips = sns.load_dataset('tips')\ntips.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\nsns.set_style(\"whitegrid\")\ng = sns.lmplot(x=\"tip\", y=\"total_bill\", data=tips, aspect=2)\ng.set_axis_labels(\"Tip\",\"Total bill(USD)\")\nplt.show()",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week6",
      "Seaborn Part 2"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week6/62_seaborn_part2.html#data",
    "href": "Python Data Analysis/Week6/62_seaborn_part2.html#data",
    "title": "Seaborn Part 2",
    "section": "Data",
    "text": "Data\nSeaborn also offers built-in data sets:\n\nuniform_data = np.random.rand(10, 12)\n\n\ndata = pd.DataFrame({'x':np.arange(1,101),\n'y':np.random.normal(0,4,100)})\n\n\ntitanic = sns.load_dataset(\"titanic\")\niris = sns.load_dataset(\"iris\")\n\n\nfig, ax = plt.subplots()",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week6",
      "Seaborn Part 2"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week6/62_seaborn_part2.html#plotting-with-seaborn",
    "href": "Python Data Analysis/Week6/62_seaborn_part2.html#plotting-with-seaborn",
    "title": "Seaborn Part 2",
    "section": "Plotting with Seaborn",
    "text": "Plotting with Seaborn\n\nAxis Grids\n\n# Subplot grid for plotting conditional relationships\ng = sns.FacetGrid(titanic, col=\"survived\", row=\"sex\")\ng = g.map(plt.hist,\"age\")\n#Draw a categorical plot onto a Facetgrid\n\n\n\n\n\n\n\n\nSubplot grid for plotting pairwise relationships\n\nh = sns.PairGrid(iris)\nh = h.map(plt.scatter)\n\n\n\n\n\n\n\n\n\nsns.pairplot(iris)\nplt.show()\n\n\n\n\n\n\n\n\nGrid for bivariate plot with marginal univariate plots\n\ni = sns.JointGrid(x=\"x\",\ny=\"y\",\ndata=data)\ni = i.plot(sns.regplot,\nsns.distplot)\n\n/home/jumashafara/venvs/programming_for_data_science/lib/python3.10/site-packages/seaborn/axisgrid.py:1886: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(self.x, **orient_kw_x, **kwargs)\n/home/jumashafara/venvs/programming_for_data_science/lib/python3.10/site-packages/seaborn/axisgrid.py:1892: UserWarning: \n\n`distplot` is a deprecated function and will be removed in seaborn v0.14.0.\n\nPlease adapt your code to use either `displot` (a figure-level function with\nsimilar flexibility) or `histplot` (an axes-level function for histograms).\n\nFor a guide to updating your code to use the new functions, please see\nhttps://gist.github.com/mwaskom/de44147ed2974457ad6372750bbe5751\n\n  func(self.y, **orient_kw_y, **kwargs)\n\n\n\n\n\n\n\n\n\n\n# Plot data and regression model fitsacross a FacetGrid\nsns.lmplot(x=\"sepal_width\",\ny=\"sepal_length\",\nhue=\"species\",\nx_ci = 'sd',\ndata=iris)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCategorical Plots\n\nScatterplot\nScatterplot with one categorical variable\n\nsns.stripplot(x=\"species\",\ny=\"petal_length\",\ndata=iris, hue='species')\nplt.show()\n\n\n\n\n\n\n\n\nCategorical scatterplot with non-overlapping points\n\nsns.swarmplot(x=\"species\",\ny=\"petal_length\",\ndata=iris, hue='species')\nplt.show()\n\n/home/jumashafara/venvs/programming_for_data_science/lib/python3.10/site-packages/seaborn/categorical.py:3399: UserWarning: 12.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n\n\n\n\n\nBar Chart\nShow point estimates and confidence intervals with scatterplot glyphs\n\nsns.barplot(x=\"sex\",\ny=\"survived\",\nhue=\"class\",\ndata=titanic)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCount Plot\nShow count of observations\n\nsns.countplot(\n    x=\"deck\", \n    data=titanic, \n    palette=\"Greens_d\", \n    hue='survived')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPoint Plot\nShow point estimates and confidence intervals as rectangular bars\n\nsns.pointplot(x=\"class\", y=\"survived\", hue=\"sex\", data=titanic,\n            palette={\"male\":\"g\",\"female\":\"m\"}, markers=[\"^\",\"o\"], linestyles=[\"-\",\"--\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBoxplot\n\nsns.boxplot(x=\"alive\", y=\"age\", hue=\"adult_male\", data=titanic)\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.boxplot(data=iris,orient=\"h\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nViolin Plot\n\nsns.violinplot(x=\"age\",\ny=\"sex\",\nhue=\"survived\",\ndata=titanic)\n\n\n\n\n\n\n\n\n\n\n\nDistribution Plots\nPlot univariate distribution\n\nplot = sns.displot(data.y, kde=False, color=\"b\")\n\n\n\n\n\n\n\n\n\nsns.displot(data.y, kde=True, color=\"b\")\n\n\n\n\n\n\n\n\n\n\nMatrix Plots\nHeatmap\n\n# Exclude non-numeric columns from correlation calculation\nnumeric_cols = tips.select_dtypes(include=['float64', 'int64'])\ncorrelation_matrix = numeric_cols.corr()\n\n# Plotting the heatmap\nsns.heatmap(correlation_matrix, annot=True)\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.show()\nplt.savefig(\"foo.png\")\n\n# You can save a transparent figure\nplt.savefig(\"foo.png\", transparent=True)\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\nfig3, ax = plt.subplots()\n# Create the regression plot\nsns.regplot(x=\"petal_width\", y=\"petal_length\", data=iris, ax=ax)\n\n# Display the plot\nplt.show()",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week6",
      "Seaborn Part 2"
    ]
  },
  {
    "objectID": "Python Data Analysis/outline.html",
    "href": "Python Data Analysis/outline.html",
    "title": "Data Analysis Outline",
    "section": "",
    "text": "Understanding the role of data analysis in decision-making\nIntroduction to Python for data analysis (Numpy and Pandas)\nExploring data types, data structures, and data manipulation",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python Data Analysis/outline.html#week-1-introduction-to-data-analysis",
    "href": "Python Data Analysis/outline.html#week-1-introduction-to-data-analysis",
    "title": "Data Analysis Outline",
    "section": "",
    "text": "Understanding the role of data analysis in decision-making\nIntroduction to Python for data analysis (Numpy and Pandas)\nExploring data types, data structures, and data manipulation",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python Data Analysis/outline.html#week-2-introduction-to-data-cleaning-and-preprocessing",
    "href": "Python Data Analysis/outline.html#week-2-introduction-to-data-cleaning-and-preprocessing",
    "title": "Data Analysis Outline",
    "section": "Week 2: Introduction to Data Cleaning and Preprocessing",
    "text": "Week 2: Introduction to Data Cleaning and Preprocessing\n\nData quality assurance\nIdentifying and handling missing data\nDealing with outliers and other data anomalies",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python Data Analysis/outline.html#week-3-introduction-to-data-visualization",
    "href": "Python Data Analysis/outline.html#week-3-introduction-to-data-visualization",
    "title": "Data Analysis Outline",
    "section": "Week 3: Introduction to Data Visualization",
    "text": "Week 3: Introduction to Data Visualization\n\nBasic plotting techniques using Matplotlib\nExtracting insights from data distributions and relationships\nPerforming EDA using Pandas and visualizations",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python Data Analysis/outline.html#week-4-introduction-to-machine-learning",
    "href": "Python Data Analysis/outline.html#week-4-introduction-to-machine-learning",
    "title": "Data Analysis Outline",
    "section": "Week 4: Introduction to Machine Learning",
    "text": "Week 4: Introduction to Machine Learning\n\nOverview of machine learning concepts\nSupervised vs. unsupervised learning\nHands-on exercises with Scikit-Learn for classification and regression",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python Data Analysis/outline.html#week-5-introduction-to-statistical-analysis",
    "href": "Python Data Analysis/outline.html#week-5-introduction-to-statistical-analysis",
    "title": "Data Analysis Outline",
    "section": "Week 5: Introduction to Statistical Analysis",
    "text": "Week 5: Introduction to Statistical Analysis\n\nDescriptive statistics and summary metrics\nHypothesis testing and p-values\nImplementing statistical analysis in Python using SciPy",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python Data Analysis/outline.html#week-6-more-data-visualization-and-exploratory-data-analysis-eda",
    "href": "Python Data Analysis/outline.html#week-6-more-data-visualization-and-exploratory-data-analysis-eda",
    "title": "Data Analysis Outline",
    "section": "Week 6: More Data Visualization and Exploratory Data Analysis (EDA)",
    "text": "Week 6: More Data Visualization and Exploratory Data Analysis (EDA)\n\nAdvanced visualization with Seaborn for statistical analysis\nCreating meaningful visualizations for data exploration\nCorrelation analysis and feature selection",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python Data Analysis/outline.html#week-7-data-wrangling-and-feature-engineering-data-transformation-techniques",
    "href": "Python Data Analysis/outline.html#week-7-data-wrangling-and-feature-engineering-data-transformation-techniques",
    "title": "Data Analysis Outline",
    "section": "Week 7: Data Wrangling and Feature Engineering (Data transformation techniques)",
    "text": "Week 7: Data Wrangling and Feature Engineering (Data transformation techniques)\n\nFeature scaling and engineering for model improvement\nData normalization and standardization\nHandling categorical data and encoding techniques",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python Data Analysis/outline.html#week-8-more-machine-learning-model-evaluation-and-validation",
    "href": "Python Data Analysis/outline.html#week-8-more-machine-learning-model-evaluation-and-validation",
    "title": "Data Analysis Outline",
    "section": "Week 8: More Machine Learning (Model Evaluation and Validation)",
    "text": "Week 8: More Machine Learning (Model Evaluation and Validation)\n\nEvaluating machine learning models\nCross-validation and hyperparameter tuning\nModel selection and performance metrics",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python Data Analysis/outline.html#week-9-time-series-analysis",
    "href": "Python Data Analysis/outline.html#week-9-time-series-analysis",
    "title": "Data Analysis Outline",
    "section": "Week 9: Time Series Analysis",
    "text": "Week 9: Time Series Analysis\n\nUnderstanding time series data\nTime series visualization and decomposition\nForecasting techniques with Python",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python Data Analysis/outline.html#week-10-capstone-project",
    "href": "Python Data Analysis/outline.html#week-10-capstone-project",
    "title": "Data Analysis Outline",
    "section": "Week 10: Capstone Project",
    "text": "Week 10: Capstone Project\n\nApplying learned concepts to a real-world dataset\nData analysis, visualization, and modeling\nPresenting findings and insights",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python Data Analysis/outline.html#do-you-seriously-want-to-learn-python-and-data-analysis-with-python",
    "href": "Python Data Analysis/outline.html#do-you-seriously-want-to-learn-python-and-data-analysis-with-python",
    "title": "Data Analysis Outline",
    "section": "Do you seriously want to learn Python and Data Analysis with Python?",
    "text": "Do you seriously want to learn Python and Data Analysis with Python?\nIf you’re serious about learning Python for Data Analysis, I highly encourage you to enroll in my Complete Python and Data Analysis Courses. Don’t waste your time following disconnected, outdated tutorials.\nMy Complete Python Course has everything you need in one place.\n\nOver 10 hours of HD video\nUnlimited access - watch it as many times as you want\nSelf-paced learning - take your time if you prefer\nWatch it online or download and watch it offline\nOne-on-one training with me 2-4 times a week\n\nSincerely,\nJuma Shafara\nData Scientist, Instructor",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Data Analysis Outline"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week3/31_matplotlib_refined.html",
    "href": "Python Data Analysis/Week3/31_matplotlib_refined.html",
    "title": "Matplotlib",
    "section": "",
    "text": "Matplotlib is a powerful plotting library in Python commonly used for data visualization. When working with datasets, you can use Matplotlib to create various plots to explore and visualize the data. Here are some major plots you can create using Matplotlib with the Titanic dataset:\n\n## Uncomment and run this cell to install the libraries\n#!pip install pandas matplotlib\n\n\n# import the libraries, packages and modules\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom dataidea.datasets import loadDataset\n\nLet’s demonstrate each of the plots using the Titanic dataset. We’ll first load the dataset and then create each plot using Matplotlib.\n\n# Load the Titanic dataset\ntitanic_df = loadDataset('titanic')\n\n\ntitanic_df.head(n=5)\n\n\n\n\n\n\n\n\n\npclass\nsurvived\nname\nsex\nage\nsibsp\nparch\nticket\nfare\ncabin\nembarked\nboat\nbody\nhome.dest\n\n\n\n\n0\n1.0\n1.0\nAllen, Miss. Elisabeth Walton\nfemale\n29.0000\n0.0\n0.0\n24160\n211.3375\nB5\nS\n2\nNaN\nSt Louis, MO\n\n\n1\n1.0\n1.0\nAllison, Master. Hudson Trevor\nmale\n0.9167\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\n11\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n2\n1.0\n0.0\nAllison, Miss. Helen Loraine\nfemale\n2.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNaN\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n3\n1.0\n0.0\nAllison, Mr. Hudson Joshua Creighton\nmale\n30.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNaN\n135.0\nMontreal, PQ / Chesterville, ON\n\n\n4\n1.0\n0.0\nAllison, Mrs. Hudson J C (Bessie Waldo Daniels)\nfemale\n25.0000\n1.0\n2.0\n113781\n151.5500\nC22 C26\nS\nNaN\nNaN\nMontreal, PQ / Chesterville, ON\n\n\n\n\n\n\n\n\nWe can load this dataset in this format because it’s inbuilt in the dataidea library\n\nBar Plot: You can create a bar plot to visualize categorical data such as the number of passengers in each class (first class, second class, third class), the number of survivors vs. non-survivors, or the number of passengers embarked from each port (Cherbourg, Queenstown, Southampton).\n\n\n# 1. Bar Plot - Number of passengers in each class\nclass_counts = titanic_df['pclass'].value_counts()\nplt.bar(class_counts.index, class_counts.values)\nplt.xlabel('Passenger Class')\nplt.ylabel('Number of Passengers')\nplt.title('Number of Passengers in Each Class')\nplt.savefig('barplot.pdf')\nplt.show()\n\n\n\n\n\n\n\n\n\nHistogram: Histograms are useful for visualizing the distribution of continuous variables such as age or fare. You can create histograms to see the age distribution of passengers or the fare distribution.\n\n\n# 2. Histogram - Age distribution of passengers\nplt.hist(titanic_df['age'], bins=20, edgecolor='black')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.title('Age Distribution of Passengers')\nplt.show()\n\n\n\n\n\n\n\n\n\nBox Plot: A box plot can be used to show the distribution of a continuous variable across different categories. For example, you can create a box plot to visualize the distribution of age or fare across different passenger classes.\n\n\nfirst_class_ages = titanic_df[titanic_df.pclass == 1].age.dropna()\nsecond_class_ages = titanic_df[titanic_df.pclass == 2].age.dropna()\nthird_class_ages = titanic_df[titanic_df.pclass == 3].age.dropna()\n\n\nplt.boxplot([first_class_ages, second_class_ages, third_class_ages], \n            labels=['First class', 'Second class', 'Third class'], vert=False)\nplt.xlabel('Age')\nplt.ylabel('Passenger Class')\nplt.title('Distribution of Age Across Passenger Classes')\nplt.show()\n\n\n\n\n\n\n\n\n\n# 3. Box Plot - Distribution of age across passenger classes\nplt.boxplot([titanic_df[titanic_df['pclass'] == 1]['age'].dropna(),\n             titanic_df[titanic_df['pclass'] == 2]['age'].dropna(),\n             titanic_df[titanic_df['pclass'] == 3]['age'].dropna()],\n            labels=['1st Class', '2nd Class', '3rd Class'])\nplt.xlabel('Passenger Class')\nplt.ylabel('Age')\nplt.title('Distribution of Age Across Passenger Classes')\nplt.show()\n\n\n\n\n\n\n\n\n\nScatter Plot: Scatter plots are helpful for visualizing the relationship between two continuous variables. You can create scatter plots to explore relationships such as age vs. fare, age vs. survival status, or fare vs. survival status.\n\n\n# 4. Scatter Plot - Age vs. Fare\nplt.scatter(titanic_df['age'], titanic_df['fare'], alpha=0.5)\nplt.xlabel('Age')\nplt.ylabel('Fare')\nplt.title('Age vs. Fare')\nplt.show()\n\n\n\n\n\n\n\n\n\nPie Chart: Pie charts can be used to visualize the proportion of different categories within a dataset. For example, you can create a pie chart to show the proportion of male vs. female passengers or the proportion of survivors vs. non-survivors.\n\n\nclass_counts = titanic_df.sex.value_counts()\ngenders = class_counts.index\nvalues = class_counts.values\n\n\n# 5. Pie Chart - Proportion of male vs. female passengers\ngender_counts = titanic_df['sex'].value_counts()\nplt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=90)\nplt.title('Proportion of Male vs. Female Passengers')\nplt.legend(loc='lower right')\nplt.show()\n\n\n\n\n\n\n\n\n\nStacked Bar Plot: Stacked bar plots can be used to compare the composition of different categories across groups. For example, you can create a stacked bar plot to compare the proportion of survivors and non-survivors within each passenger class.\n\n\n# 6. Stacked Bar Plot - Survival status within each passenger class\nsurvival_counts = titanic_df.groupby(['pclass', 'survived']).size().unstack()\nsurvival_counts.plot(kind='bar', stacked=True)\nplt.xlabel('Passenger Class')\nplt.ylabel('Number of Passengers')\nplt.title('Survival Status Within Each Passenger Class')\nplt.legend(['Did not survive', 'Survived'])\nplt.show()\n\n\n\n\n\n\n\n\n\ntitanic_df.groupby(['pclass', 'survived']).size().unstack()\n\n\n\n\n\n\n\n\nsurvived\n0.0\n1.0\n\n\npclass\n\n\n\n\n\n\n1.0\n123\n200\n\n\n2.0\n158\n119\n\n\n3.0\n528\n181\n\n\n\n\n\n\n\n\n\nLine Plot: Line plots can be useful for visualizing trends over time or continuous variables. While the Titanic dataset may not have explicit time data, you can still use line plots to visualize trends such as the change in survival rate with increasing age or fare.\n\n\n# 7. Line Plot - Mean age of passengers by passenger class\nmean_age_by_class = titanic_df.groupby('pclass')['age'].mean()\nplt.plot(mean_age_by_class.index, mean_age_by_class.values, marker='o')\nplt.xlabel('Passenger Class')\nplt.ylabel('Mean Age')\nplt.title('Mean Age of Passengers by Passenger Class')\nplt.show()\n\n\n\n\n\n\n\n\nThese are some of the major plots you can create using Matplotlib. Each plot serves a different purpose and can help you gain insights into the data and explore relationships between variables.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week3",
      "Matplotlib"
    ]
  },
  {
    "objectID": "Python/10_exercise.html",
    "href": "Python/10_exercise.html",
    "title": "Exercise",
    "section": "",
    "text": "Python Overview:\nWhat are the fundamental components of a Python program, and how does Python execute code?\nPython Variables:\nExplain the differences between mutable and immutable data types in Python and provide examples of each.\nPython Operations:\nDiscuss the importance of operator precedence in Python and how it impacts the evaluation of expressions.\nPython Collections:\nCompare and contrast the usage of lists, tuples, sets, and dictionaries in Python, highlighting their key characteristics and when to use each.\n\n\n\nProblem: Pizza Party\nYou’re organizing a pizza party and want to ensure there’s enough pizza for everyone attending. Each pizza has 8 slices. Write a Python script that calculates the number of pizzas needed based on the number of guests and slices per person.\nConsider the following inputs:\n\nNumber of guests attending the party.\nSlices each guest should have (assume each guest will have the same number of slices).\n\nYour script should:\n\nPrompt the user for the number of guests attending.\nPrompt for the number of slices each guest should have.\nCalculate the total number of slices needed.\nCalculate the total number of pizzas required (round up to the nearest whole pizza).\nFor instance, if 10 guests are attending and each should have 3 slices, your program should output the number of pizzas needed to fulfill the requirement.\n\n\n# Solution\nnumber_of_guests = 10\nnumber_of_slices_for_each = 3\n\ntotal_number_of_slices = number_of_guests * number_of_slices_for_each\ntotal_number_of_pizzas = total_number_of_slices / 8\n\nprint('Number of pizzas needed for ' + \n      str(number_of_guests) + ' is ' + \n      str(total_number_of_pizzas))\n\nNumber of pizzas needed for 10 is 3.75",
    "crumbs": [
      "Home",
      "Python",
      "Exercise"
    ]
  },
  {
    "objectID": "Python/10_exercise.html#homework",
    "href": "Python/10_exercise.html#homework",
    "title": "Exercise",
    "section": "",
    "text": "Python Overview:\nWhat are the fundamental components of a Python program, and how does Python execute code?\nPython Variables:\nExplain the differences between mutable and immutable data types in Python and provide examples of each.\nPython Operations:\nDiscuss the importance of operator precedence in Python and how it impacts the evaluation of expressions.\nPython Collections:\nCompare and contrast the usage of lists, tuples, sets, and dictionaries in Python, highlighting their key characteristics and when to use each.\n\n\n\nProblem: Pizza Party\nYou’re organizing a pizza party and want to ensure there’s enough pizza for everyone attending. Each pizza has 8 slices. Write a Python script that calculates the number of pizzas needed based on the number of guests and slices per person.\nConsider the following inputs:\n\nNumber of guests attending the party.\nSlices each guest should have (assume each guest will have the same number of slices).\n\nYour script should:\n\nPrompt the user for the number of guests attending.\nPrompt for the number of slices each guest should have.\nCalculate the total number of slices needed.\nCalculate the total number of pizzas required (round up to the nearest whole pizza).\nFor instance, if 10 guests are attending and each should have 3 slices, your program should output the number of pizzas needed to fulfill the requirement.\n\n\n# Solution\nnumber_of_guests = 10\nnumber_of_slices_for_each = 3\n\ntotal_number_of_slices = number_of_guests * number_of_slices_for_each\ntotal_number_of_pizzas = total_number_of_slices / 8\n\nprint('Number of pizzas needed for ' + \n      str(number_of_guests) + ' is ' + \n      str(total_number_of_pizzas))\n\nNumber of pizzas needed for 10 is 3.75",
    "crumbs": [
      "Home",
      "Python",
      "Exercise"
    ]
  },
  {
    "objectID": "Python/09_file_handling.html",
    "href": "Python/09_file_handling.html",
    "title": "Python File Handling",
    "section": "",
    "text": "Python allows us to read, write, create and delete files. This process is called file handling.",
    "crumbs": [
      "Home",
      "Python",
      "Python File Handling"
    ]
  },
  {
    "objectID": "Python/09_file_handling.html#python-file-reading",
    "href": "Python/09_file_handling.html#python-file-reading",
    "title": "Python File Handling",
    "section": "Python File reading",
    "text": "Python File reading\nTo better explain this, let us say we have a folder named my_folder.\nInside my_folder we have the following files:\n\ndemo.txt\nmain_code.py\n\nThe content of the demo.txt file is the following\nHello World!\nI love Python\nNow our goal is to read the content of the demo.txt file and then print it using the main_code.py file\nTo achieve this, we will use the open() function with 'r' mode.\n\n# this is main code\n\nfile = open(\n    file='demo.txt',\n    mode='r'\n)\ncontent = file.read()\nprint(content)\n\nHello World!\nI love Python\n\n\n\nReading Lines\nWe can also read each line using the readline() method.\n\n# this is main_code.py\n\nfile = open(\n    file='demo.txt',\n    mode='r'\n)\n\nfirst_line = file.readline()\nsecond_line = file.readline()\n\nprint('First line:', first_line)\nprint('Second line:', second_line)\n\nFirst line: Hello World!\n\nSecond line: I love Python",
    "crumbs": [
      "Home",
      "Python",
      "Python File Handling"
    ]
  },
  {
    "objectID": "Python/09_file_handling.html#writing-a-file",
    "href": "Python/09_file_handling.html#writing-a-file",
    "title": "Python File Handling",
    "section": "Writing a File",
    "text": "Writing a File\nIn simplest terms, writing a file means modifying the content of a file or creating it if it doesnot exist yet.\nIn Python, there are 2 modes to write to file.\n\n'w' - overwrites content of a file, creates file if it does not exist\n'a' - appends content to the end of a file, creates the file if it does not exist\n\nExample To better explain this, lets say we have a folder named my_folder. Inside my_folder we have the following files\n\ndemo.txt\nmain_code.py\n\nThe content of the demo.txt file is the following\nI love Python\nIn this example, we will use the 'w' mode which will overwrite(replace) the content of the file\n\n# this is main_code.py\n\nfile = open(\n    file='demo.txt',\n    mode='w'\n)\nfile.write('I love JavaScript')\nfile.close()\n\nWhen the above code is run, the content of the file demo.txt will be this:\nI love JavaScript\nAnother example, this time we will use the a mode which will append or add content to the end of the file\n\n# this is main_code.py\n\nfile = open(\n    file='demo.txt',\n    mode='a'\n)\nfile.write(' and JavaScript')\nfile.close()\n\nWhen the above script is run, the content of the demo.txt file will be this:\nI love Python and JavaScript",
    "crumbs": [
      "Home",
      "Python",
      "Python File Handling"
    ]
  },
  {
    "objectID": "Python/09_file_handling.html#deleting-a-file",
    "href": "Python/09_file_handling.html#deleting-a-file",
    "title": "Python File Handling",
    "section": "Deleting a file",
    "text": "Deleting a file\nTo delete a file, use the os module. The os modules contains the remove() method which we can use to delete files.\n\n# this is main_code.py\n\nimport os\n\n# os.remove('demo.txt')",
    "crumbs": [
      "Home",
      "Python",
      "Python File Handling"
    ]
  },
  {
    "objectID": "Python/05_containers.html",
    "href": "Python/05_containers.html",
    "title": "Containers",
    "section": "",
    "text": "Containers are objects that contain other objects",
    "crumbs": [
      "Home",
      "Python",
      "Containers"
    ]
  },
  {
    "objectID": "Python/05_containers.html#lists",
    "href": "Python/05_containers.html#lists",
    "title": "Containers",
    "section": "Lists",
    "text": "Lists\n\nA python list is an ordered container\nA list is created by using square brackets ([])\nObjects are poaced inside those brackets and are separated by commas (,)\n\n\npets = ['dog', 'cat', 'rabbit', 'monkey']\nprint(pets)\nprint(type(pets))\n\n['dog', 'cat', 'rabbit', 'monkey']\n&lt;class 'list'&gt;\n\n\n\nIndexing\n\nIndexing is used to access items of a list\nIndexing uses square brackets and numbers to access individual items of a list\nWhere 0 refers to the first item, 1 refers to the second item, and so on\n\n\n# indexing\nprint(pets[2])\n\nrabbit\n\n\n\n#range of indexes\nprint(pets[1:3])\n\n['cat', 'rabbit']\n\n\n\n\nAdding items to a list\n\npets = ['dog', 'cat', 'rabbit', 'monkey']\npets.append('hamster')\nprint(pets)\n\n['dog', 'cat', 'rabbit', 'monkey', 'hamster']\n\n\n\npets = ['dog', 'cat', 'rabbit', 'monkey']\npets.insert(1, 'hamster')\nprint(pets)\n\n['dog', 'hamster', 'cat', 'rabbit', 'monkey']\n\n\n\n\nDeleting Items from a list\n\npets = ['dog', 'cat', 'rabbit', 'monkey']\npets.pop()\nprint(pets)\n\n['dog', 'cat', 'rabbit']\n\n\n\npets = ['dog', 'cat', 'rabbit', 'monkey']\npets.remove('rabbit')\nprint(pets)\n\n['dog', 'cat', 'monkey']\n\n\n\npets = ['dog', 'cat', 'rabbit', 'monkey']\ndel pets [2]\nprint(pets)\n\n['dog', 'cat', 'monkey']\n\n\n\n\nGetting the length of a list\nThe length of a list refers to the number of items in a list, use the len() method\n\n\nHomework\n\nCheck if an item exist\n\n\n\nExtending a list\nThe extend() methods adds all items from one list to another\n\npets = ['dog', 'cat']\nother_pets = ['rabbit', 'monkey']\npets.extend(other_pets)\nprint(pets)\n\n['dog', 'cat', 'rabbit', 'monkey']",
    "crumbs": [
      "Home",
      "Python",
      "Containers"
    ]
  },
  {
    "objectID": "Python/05_containers.html#tuple",
    "href": "Python/05_containers.html#tuple",
    "title": "Containers",
    "section": "Tuple",
    "text": "Tuple\n\nPython tuple is an ordered container\nIts the same as a list but the items of tuples cannot be changed\nWe create a tuple using round brackets ()\n\n\npets = ('dog', 'cat', 'rabbit')\nprint(pets)\nprint(type(pets))\n\n('dog', 'cat', 'rabbit')\n&lt;class 'tuple'&gt;",
    "crumbs": [
      "Home",
      "Python",
      "Containers"
    ]
  },
  {
    "objectID": "Python/05_containers.html#sets",
    "href": "Python/05_containers.html#sets",
    "title": "Containers",
    "section": "Sets",
    "text": "Sets\n\nA set is a container/collection that is unordered and immutable\nWe create a set using {}\n\n\npets = {'dog', 'cat', 'rabbit'}\nprint(pets)\n\n{'rabbit', 'dog', 'cat'}\n\n\n\n# A set can contain objects of different data types\nmixed = {'dog', 21, True}\nprint(mixed)\nprint(type(mixed))\n\n{True, 'dog', 21}\n&lt;class 'set'&gt;\n\n\n\nAccessing set elements\n\nUnlike lists and tuples, you cannot access the items in a set using indexes\nThis is because a set is unordered and not indexed\nHowever, we can use a for loop to access all its items one-by-one\n\nNote: We’ll discuss a for loop in the next chapter\n\n# Accessing\npets = {'dog', 'cat', 'rabbit'}\nfor pet in pets:\n    print(pet)\n\nrabbit\ndog\ncat\n\n\n\n\nAdding elements to a set\n\n# Adding items to a set\npets = {'dog', 'cat', 'rabbit'}\npets.add('fish')\nprint(pets)\n\n{'rabbit', 'dog', 'cat', 'fish'}\n\n\n\n\nRemoving set elements\n\n# Removing items from a set\npets = {'dog', 'cat', 'rabbit'}\npets.remove('cat') # remove\nprint(pets)\n\n{'rabbit', 'dog'}\n\n\n\npets = {'dog', 'cat', 'rabbit'}\npets.discard('rabbit') #discard\nprint(pets)\n\n{'dog', 'cat'}\n\n\n\npets = {'dog', 'cat', 'rabbit'}\npets.pop() # pop removes the last item from the set\nprint(pets)\n\n{'dog', 'cat'}\n\n\n\n\nHomework\n\nFind the length of a set\nCheck if an element exists\nCombine sets\n\n\n\nGetting the difference between sets\n\n# Getting the difference\nfirst_numbers = {1, 2, 3, 4}\nsecond_numbers = {3, 4, 5, 6}\n\ndifference = first_numbers - second_numbers\n# another way\ndifference2 = first_numbers.difference(second_numbers)\nprint(difference)\n\n{1, 2}",
    "crumbs": [
      "Home",
      "Python",
      "Containers"
    ]
  },
  {
    "objectID": "Python/05_containers.html#dictionaries",
    "href": "Python/05_containers.html#dictionaries",
    "title": "Containers",
    "section": "Dictionaries",
    "text": "Dictionaries\nA dictionary is an unordered and mutable colletion of items\n\n# Creating \nperson = {\n    'first_name': 'Voila', \n    'last_name': 'Akullu',\n    'age': 16\n    }\nprint(person)\n\n{'first_name': 'Voila', 'last_name': 'Akullu', 'age': 16}\n\n\n\n# Accessing items\nprint(person['last_name'])\n\nAkullu\n\n\n\n# Adding items \nperson['middle_name'] = 'Vee'\nprint(person)\n\n{'first_name': 'Voila', 'last_name': 'Akullu', 'age': 16, 'middle_name': 'Vee'}\n\n\n\n# Remove items\nperson.pop('age')\nprint(person)\n\n{'first_name': 'Voila', 'last_name': 'Akullu', 'middle_name': 'Vee'}\n\n\n\nHomework\n\nCheck if an element exists\nFind the lenght of a dictionary\n\n\n# Nesting dictionaries\nemployees = {\n    'manager': {\n        'name': 'Akullu Viola',\n        'age': 29\n    },\n    'programmer': {\n        'name': 'Juma Shafara',\n        'age': 30\n    }\n}\n\nprint(employees)\n\n{'manager': {'name': 'Akullu Viola', 'age': 29}, 'programmer': {'name': 'Juma Shafara', 'age': 30}}\n\n\n\n# Accessing nested dictionary\nprogrammer = employees['programmer']\nprint(programmer['name'])\n\nJuma Shafara\n\n\n\n# Using a dictionary constructer\nnames = ('a1', 'b2', 'c3')\ndictionary = dict(names)\nprint(dictionary)\n\n{'a': '1', 'b': '2', 'c': '3'}",
    "crumbs": [
      "Home",
      "Python",
      "Containers"
    ]
  },
  {
    "objectID": "Python/08_modules.html",
    "href": "Python/08_modules.html",
    "title": "Modules",
    "section": "",
    "text": "A module in Python is a python file that can contain variables, functions and classes",
    "crumbs": [
      "Home",
      "Python",
      "Modules"
    ]
  },
  {
    "objectID": "Python/08_modules.html#math-module",
    "href": "Python/08_modules.html#math-module",
    "title": "Modules",
    "section": "Math Module",
    "text": "Math Module\nThe math module gives us access to mathematical functions\nTo use the math module, import it first, then we can start using it.\nWe can use the math module to find the square root of a number using the math.sqrt() method\n\nimport math\n\nnumber = 16\nnumber_sqrt = math.sqrt(number)\n\nprint('Number:', number)\nprint('Square root of number:', number_sqrt)\n\nNumber: 16\nSquare root of number: 4.0\n\n\nWe can use the math module to get the factorial of a number by using the math.factorial() method\n\nimport math\n\nnumber = 5\nnumber_factorial = math.factorial(number)\n\nprint('Number:', number)\nprint('Factorial:', number_factorial)\n\nNumber: 5\nFactorial: 120\n\n\nThe math module also contains some constants like pi and e\n\nimport math\n\nprint('e:', math.e)\nprint('pi:', math.pi)\n\ne: 2.718281828459045\npi: 3.141592653589793\n\n\nThe math module can do those and so much more",
    "crumbs": [
      "Home",
      "Python",
      "Modules"
    ]
  },
  {
    "objectID": "Python/08_modules.html#random-module",
    "href": "Python/08_modules.html#random-module",
    "title": "Modules",
    "section": "Random Module",
    "text": "Random Module\nThe random module lets us generate a random number\nAs usual, to use the random module, import it first.\nWe can generate a random number that falls within a specified range by using the random.randint() method\n\nimport random\n\nrandom_integer = random.randint(1,100)\nprint('Random Integer:', random_integer)\n\nRandom Integer: 4\n\n\nWe can generate numbers from a gaussian distribution with mean (mu) as 0 and standard deviation (sigma) as 1\n\nnumbers = []\n\ncounter = 0\nwhile counter &lt; 100:\n    numbers.append(random.gauss(mu=0, sigma=1))\n    counter += 1\n    \nprint(numbers)\n\n[0.8310128735410047, 2.402375340413018, -1.2769617295659348, 0.7569506717477539, 1.6026026122392498, 1.4142936594217554, -0.3169917649104485, -0.07305941097531603, -0.7885301448554015, -0.0674611332298377, 0.28288857512573684, 0.08844216926370602, -1.249987094506388, 0.870793290313952, -0.6607737394803138, 0.3780605189691181, 0.20288623881856632, 0.8439702923769746, 1.6500270929422152, -0.5579247768953991, -0.3076290349937902, 0.8927675985413197, -2.3716599434459114, 0.23253728473684382, 0.01698634011714592, -1.506684284668113, -1.516156046117149, -0.7549199652372819, 0.4855840249497611, -1.9426218553454226, -0.5672748318805165, 1.7849639815888045, -0.4223703532919884, -1.4182523392919628, 0.3817982448773813, -1.2151583559744263, 0.21736913499460964, 0.0743448686041854, -0.6217874541247053, -0.05369712902089164, 0.06560332100098984, 0.5791279113149166, 1.5329264216964942, -1.5523813284095307, 0.256018716284597, 1.498941708596562, 0.6484203278916434, 0.956658998431066, -0.7469607705965761, 0.9093585267915438, -0.3301676177291813, -2.1020486475752564, -0.6324768823835674, -0.2621489739923403, 0.36805271395009337, -0.1987104858441708, -0.20226660046300027, -1.0227302328088852, 0.9440428943259802, 1.3499647213634605, 0.28655811659281705, -0.48212404896946465, 1.5732404576352244, 1.7024230857294205, -0.32802550098029193, 2.0808443667109597, 2.2783854541239874, -0.265626754707208, -0.04641950638081212, 0.7941371582079103, -0.36860553191079254, -0.9098450679735101, 1.234946260813307, -2.835066105841072, 1.3883254119625694, 1.2853299658795028, 1.178005875662903, 0.3186472037221876, -1.0006920744966419, -2.3745959188263885, 1.8440465299894964, -0.35610549619690796, 0.5857012223823791, 0.7400382246661824, 0.07225122970263118, -0.5508995490344698, -0.038356750477046286, -0.040997463659922434, 0.6802546773316889, -1.3861271290488735, 0.7275261286416534, 0.3729374034245036, -0.013616473457934613, -0.7620103036607296, 0.15556952852877587, -1.7898533901375224, -1.137248630020012, -1.71518120153122, -0.5817297506694047, -0.4035542913039588]",
    "crumbs": [
      "Home",
      "Python",
      "Modules"
    ]
  },
  {
    "objectID": "Python/08_modules.html#date-and-time",
    "href": "Python/08_modules.html#date-and-time",
    "title": "Modules",
    "section": "Date and Time",
    "text": "Date and Time\nThe datetime module allows us to work with dates\nAs usual we have to import the datetime module to be able to use it.\n\nCurrent Date and Time\nThe datetime.datetime.now() method returns the current date and time\n\nimport datetime\n\ntime_now = datetime.datetime.now()\nprint(time_now)\n\n2024-05-01 08:18:09.070054\n\n\n\n\nThe date Object\nThe date object represents a date (year, month and day)\nTo create a date object, import it from the datetime module first.\n\nfrom datetime import date\n\ntoday = date.today()\nprint('Current date:', today)\n\nCurrent date: 2024-05-01",
    "crumbs": [
      "Home",
      "Python",
      "Modules"
    ]
  },
  {
    "objectID": "Python/08_modules.html#json",
    "href": "Python/08_modules.html#json",
    "title": "Modules",
    "section": "JSON",
    "text": "JSON\nJSON stands for JavaScript Object Notation.\nJSON contains data that are sent or received to and from a server\nJSON is simply a string, if follows a format similar to a Python dictionary\nExample:\n\ndata = \"{'first_name': 'Juma','last_name': 'Shafara', 'age': 39}\"\n\nprint(data)\n\n{'first_name': 'Juma','last_name': 'Shafara', 'age': 39}\n\n\n\nJSON to Dictionary\nBefore we can individually access the data of a JSON, we need to convert it to a Python dictionary first.\nTo do that, we need to import the json module\n\nimport json \n\ndata = '{\"first_name\": \"Juma\",\"last_name\": \"Shafara\", \"age\": 39}'\n\n# convert to dictionary\ndata_dict = json.loads(data)\n\nprint('Fist name:',data_dict['first_name'])\nprint('Last name:', data_dict['last_name'])\nprint('Age:', data_dict['age'])\n\nFist name: Juma\nLast name: Shafara\nAge: 39\n\n\n\n\nDictionary to JSON\nTo convert a dictionay to JSON, use the json.dumps() method.\n\nimport json\n\ndata_dict = {\n    \"first_name\": \"Juma\",\n    \"last_name\": \"Shafara\", \n    \"age\": 39\n    }\n\ndata_json = json.dumps(data_dict)",
    "crumbs": [
      "Home",
      "Python",
      "Modules"
    ]
  },
  {
    "objectID": "Python/03_numbers.html",
    "href": "Python/03_numbers.html",
    "title": "Numbers",
    "section": "",
    "text": "In python, there are three types of numbers",
    "crumbs": [
      "Home",
      "Python",
      "Numbers"
    ]
  },
  {
    "objectID": "Python/03_numbers.html#types",
    "href": "Python/03_numbers.html#types",
    "title": "Numbers",
    "section": "Types",
    "text": "Types\n\nInteger\nAn integer is a number without decimals\n\n# Python Numbers: intgers\n\na = 3\nb = 4\nnumber = 5\n\nprint('a:', a)\nprint('b:', b)\nprint('number:', number)\n\na: 3\nb: 4\nnumber: 5\n\n\n\n\nFloating Point\nA floating point number of just a float is a number with decimals\n\n# Python Numbers: floating point\na = 3.0\nb = 4.21\nnumber = 5.33\n\nprint('a:', a)\nprint('b:', b)\nprint('number:', number)\n\na: 3.0\nb: 4.21\nnumber: 5.33\n\n\n\n\nComplex\nA comple number is an imaginary number. To yield a complex number, append a j o J to a numeric value\n\n# Python Numbers: complex\n\na = 3j\nb = 5.21j\nnumber = 4 + 5.33j\n\nprint('a:', a)\nprint('b:', b)\nprint('number:', number)\n\na: 3j\nb: 5.21j\nnumber: (4+5.33j)",
    "crumbs": [
      "Home",
      "Python",
      "Numbers"
    ]
  },
  {
    "objectID": "Python/03_numbers.html#number-arthmetics",
    "href": "Python/03_numbers.html#number-arthmetics",
    "title": "Numbers",
    "section": "Number Arthmetics",
    "text": "Number Arthmetics\n\n# By Juma Shafara\n\n# Python Numbers: arthmetics\n\nsummation = 4 + 2\nprint('sum:', summation)\n\ndifference = 4 - 2\nprint('difference:', difference)\n\nproduct = 4 * 2\nprint('product:', product)\n\nquotient = 4 / 2\nprint('quotient:', quotient)\n\nsum: 6\ndifference: 2\nproduct: 8\nquotient: 2.0",
    "crumbs": [
      "Home",
      "Python",
      "Numbers"
    ]
  },
  {
    "objectID": "Python/03_numbers.html#number-methods",
    "href": "Python/03_numbers.html#number-methods",
    "title": "Numbers",
    "section": "Number Methods",
    "text": "Number Methods\nNumber methods are special functions used to work with numbers\n\n# sum() can add many numbers at once\nsummation = sum([1,2,3,4,5,6,7,8,9,10])\nprint(summation)\n\n55\n\n\n\n# round() rounds a number to a specified number of decimal places\npi = 3.14159265358979\nrounded_pi = round(pi, 3)\n\nprint('pi:', pi)\nprint('rounded_pi:', rounded_pi)\n\npi: 3.14159265358979\nrounded_pi: 3.142\n\n\n\n# abs() returns the absolute value of a number\nnumber = -5\nabsolute_value = abs(number)\nprint('absolute value of', number, 'is', absolute_value)\n\nabsolute value of -5 is 5\n\n\n\n# pow() returns the value of x to the power of y)\nfour_power_two = pow(4, 2)\nprint(four_power_two)\n\n16\n\n\n\n# divmod() returns the quotient and remainder of a division\n# division = divmod(10, 3)\nquotient, remainder = divmod(10, 3)\nprint('Quotient:', quotient)\nprint('Remainder:', remainder)\n\nQuotient: 3\nRemainder: 1",
    "crumbs": [
      "Home",
      "Python",
      "Numbers"
    ]
  },
  {
    "objectID": "Python/07_advanced.html",
    "href": "Python/07_advanced.html",
    "title": "Advanced",
    "section": "",
    "text": "In Python, everything is an object.A class helps us create objects.\n\n\nUse the class keyword to create a class\n\nclass Person:\n    first_name = \"Betty\"\n    last_name = \"Kawala\"\n    age = 30\n\n## Instantiating a class\n# Now we can ceate an object from the class by instantiating it.\n# To instantiate a class, add round brackets to the class name.\n\nperson_obj1 = Person()\n\ntype(person_obj1)\n\n__main__.Person\n\n\n\n# print attributes\nprint(person_obj1.first_name)\nprint(person_obj1.last_name)\nprint(person_obj1.age)\n\nBetty\nKawala\n30",
    "crumbs": [
      "Home",
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#classes-and-objects",
    "href": "Python/07_advanced.html#classes-and-objects",
    "title": "Advanced",
    "section": "",
    "text": "In Python, everything is an object.A class helps us create objects.\n\n\nUse the class keyword to create a class\n\nclass Person:\n    first_name = \"Betty\"\n    last_name = \"Kawala\"\n    age = 30\n\n## Instantiating a class\n# Now we can ceate an object from the class by instantiating it.\n# To instantiate a class, add round brackets to the class name.\n\nperson_obj1 = Person()\n\ntype(person_obj1)\n\n__main__.Person\n\n\n\n# print attributes\nprint(person_obj1.first_name)\nprint(person_obj1.last_name)\nprint(person_obj1.age)\n\nBetty\nKawala\n30",
    "crumbs": [
      "Home",
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#class-attributes",
    "href": "Python/07_advanced.html#class-attributes",
    "title": "Advanced",
    "section": "Class Attributes",
    "text": "Class Attributes\nA class can have attributes. Forexample the Person Class can have attributes like the name, height and feet\n\nclass Person:\n    def __init__(self, name, height, feet):\n        self.name = name\n        self.height = height\n        self.feet = feet\n\n\nperson_obj1 = Person(name='Betty Kawala', height=1.57, feet=4)\n\nprint('Name:', person_obj1.name)\nprint('Height:', person_obj1.height)\nprint('Feet:', person_obj1.feet)\n\nName: Betty Kawala\nHeight: 1.57\nFeet: 4",
    "crumbs": [
      "Home",
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#methods",
    "href": "Python/07_advanced.html#methods",
    "title": "Advanced",
    "section": "Methods",
    "text": "Methods\nMethods are functions that can access the class attributes. These methods should be defined (created) inside the class\n\nclass Person:\n    def __init__(self, name, height, feet):\n        self.name = name\n        self.height = height\n        self.feet = feet\n        \n    def jump(self):\n        return \"I'm jumping \" + str(self.feet) + \" Feet\"\n\n\nperson_obj1 = Person(name='Juma', height=1.59, feet=5)\n\nprint(person_obj1.jump())\n\nI'm jumping 5 Feet\n\n\nAs you may notice, we used self parameter to access the feet attribute.",
    "crumbs": [
      "Home",
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#python-inheritance",
    "href": "Python/07_advanced.html#python-inheritance",
    "title": "Advanced",
    "section": "Python Inheritance",
    "text": "Python Inheritance\nInheritance is a feature that allows us to create a class that inherits the attributes or properties and methods of another class\n\nExample\nThe Animal class below can be used to tell that an animal can eat\n\nclass Animal:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def eat(self):\n        print(f\"{self.name} is eating.\")\n\nLet’s say we need to create another class called Dog.\nSince a dog is also an animal, it’s more efficient to have access to all the properties and methods of the Animal class than to create another\nThis example creates a class named Dog and inherits from the Animal class\n\nclass Dog(Animal):\n    def __init__(self, name, age, color):\n        super().__init__(name, age)\n        self.color = color\n\n    def sound(self):\n        print(self.name, \"barks\")\n\nNow we can use the properties and methods of both the Animal and the Dog classes using just one instance\n\ndog1 = Dog(name='Brian', age=8, color='White')\ndog1.eat()\ndog1.sound()\n\nBrian is eating.\nBrian barks\n\n\nThe super() and __init__ functions found in the Dog class allow us to inherit the properties and methods of the Animal class.\n\n\nParent and Child Class\nThe parent class is the class from whick the other class inherits from.\nThe child class is the the class that inherits from another class\nIn our example above, the Animal is the parent class while the Dog class is the child class",
    "crumbs": [
      "Home",
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#more-class-and-inheritance-examples",
    "href": "Python/07_advanced.html#more-class-and-inheritance-examples",
    "title": "Advanced",
    "section": "More Class and Inheritance Examples",
    "text": "More Class and Inheritance Examples\n\nAnimal\n\nclass Animal:\n    def __init__(self, name, sound):\n        self.name = name \n        self.sound = sound\n\n    def speak(self):\n        print(self.name + ' is ' + self.sound + 'ing')\n\nclass Dog(Animal):\n    def __init__(self, name, sound):\n        super().__init__(name, sound)\n\n    def walk(self):\n        print( self.name + ' is ' + 'walking...')\n        # return\n    \n\nclass Snake(Animal):\n    def __init__(self, name, sound):\n        super().__init__(name, sound)\n\n    def crawl(self):\n        print(self.name + ' is ' + 'crawling..')\n\nsnake1 = Snake(name='Sensei', sound='Hiss')\n\nsnake1.speak()\n\nSensei is Hissing\n\n\n\n\nLibrary\n\nclass Library:\n    def __init__(self, name):\n        self.name = name\n        self.books = []    # list of books\n        self.lent_books = [] # list of lent books\n\n    # add book to the library\n    def addBook(self, book):\n        self.books.append(book) \n\n    # add many books to the library\n    def addManyBooks(self, books):\n        self.books.extend(books)\n\n    # display books in the library\n    def displayBooks(self):\n        for book in self.books:\n            print(book)\n\n    # lend book to a person\n    def lendBook(self, title, person):\n        for book in self.books:\n            if book.title == title:\n                if book.title not in self.lent_books:\n                    person.borrowed_books.append(book)\n                    self.lent_books.append(book.title)\n                    return\n                else:\n                    print(title + ' is not availabe')\n                    return\n        print('There no such book in the library') \n\n    def __str__(self):\n        return str(self.name)\n\n\nclass Book:\n    def __init__(self, title, author):\n        self.title = title \n        self.author = author\n        self.available = True\n\n    def __str__(self):\n        return str(self.title)\n\n\nclass Person:\n    def __init__(self, name):\n        self.name = name\n        self.borrowed_books = [] # list of borrowed books\n\n    # display all borrowed books\n    def displayBooks(self):\n        for book in self.borrowed_books:\n            print(book)\n\n    def __str__(self):\n        return str(self.name)\n\n\n# create our library\nstat_library = Library(name='Stat-Lib')\n\n\n# create our people\nviola = Person(name='Viola')\nshafara = Person(name='Shafara')\nbetty = Person(name='Betty')\n\n\n# create our books\nour_books = [\n    Book(title='Song of Lawino', author=\"Okot p'Bitek\"),\n    Book(title='Da Vinci Code', author='Dan Brown'),\n    Book(title='Harry Potter', author='JK Rowling')\n] \n\n# add books to the library\nstat_library.addManyBooks(books=our_books)\n\n\n# display available books in the library\nstat_library.displayBooks()\n\nSong of Lawino\nDa Vinci Code\nHarry Potter\n\n\n\n# lend out book \nstat_library.lendBook(title='Harry Potter', person=betty)\n\n\n# lend out book\nstat_library.lendBook(title='Song of Lawino', person=betty)\n\n\n# display books borrowed by Betty\nbetty.displayBooks()\n\nHarry Potter\nSong of Lawino\n\n\n\n# display all lent out books\nstat_library.lent_books\n\n['Harry Potter', 'Song of Lawino']\n\n\n\n# try lending out an already lent book\nstat_library.lendBook(title='Song of Lawino', person=viola)\n\nSong of Lawino is not availabe\n\n\n\n# lend out book\nstat_library.lendBook(title='Da Vinci Code', person=viola)\n\n\n# try lending out non existent book\nstat_library.lendBook(title='Angels and Demons', person=shafara)\n\nThere no such book in the library",
    "crumbs": [
      "Home",
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#formatted-strings",
    "href": "Python/07_advanced.html#formatted-strings",
    "title": "Advanced",
    "section": "Formatted Strings",
    "text": "Formatted Strings\n\nstatement = '{} loves to code in {}'\n\nformatted = statement.format('Juma', 'JavaScript')\n\nprint(formatted)\n\nJuma loves to code in JavaScript\n\n\n\nname = 'Juma'; language = 'JavaScript'\n\nstatement = f'{name} loves to code in {language}'\n\nprint(statement)\n\nJuma loves to code in JavaScript\n\n\n\nanswer = f'The summation of 5 and 7 is {5 + 7}'\n\nprint(answer)\n\nThe summation of 5 and 7 is 12\n\n\n\n# Using indexes\n\nname = 'Juma'\nlanguage = 'javascript'\n\nstatement = f'{name} loves to code in {language}'\n\nmodified = statement.format(language='JavaScript', name='Juma')\n\nprint(modified)\n\nJuma loves to code in javascript\n\n\n\nname = 'Viola'\nfruit = 'orange'\nexpression = 'so much'\n\n# positional formating\nstatement = '{} loves my {}'.format(name, fruit)\n\nprint(statement)\n\nViola loves my orange\n\n\n\n# indexing\nstatement = '{0} loves my {1}'.format(name, fruit)\n\nprint(statement)\n\nViola loves my orange",
    "crumbs": [
      "Home",
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#tryexcept",
    "href": "Python/07_advanced.html#tryexcept",
    "title": "Advanced",
    "section": "try…except",
    "text": "try…except\n\n# try:\n#     statements\n# except:\n#     statements\ndef dataideaArithmetic(x, y, operation):\n    if operation == '+':\n        return x + y\n    elif operation == '-':\n        return x - y\n    elif operation == '/':\n        return x / y \n    else:\n        return x * y\n\n\nprint('''\n    DATAIDEA Arithmects:\n      \nInstructions\n-------------------------\nEnter only two numbers and the operation as +, -, /, x\n''')\n\n\n# number1 = float(input('Enter first number: '))\n# number2 = float(input('Enter second number: '))\n# operator = input('Enter the operator: ')\n\n\ntry:\n    answer = dataideaArithmetic(number1, number2, operator)\n    print(f'{number1}{operator}{number2} = {answer}')\nexcept:\n    print('A problem occured while running the operation')\nelse:\n    print('Your code has run successfully!')\nfinally:\n    print('Code execution complete.')\n\ntry:\n    # age = input('Enter your age: ')\n    age = '32'\n    age_integer = int(age)\n\n    if age_integer &gt;= 18:\n        print('Your vote has been cast')\n    else:\n        print('You are not eligible to vote')\nexcept ValueError:\n    print('A problem occured while picking your age \\n'\n          'You did not enter a number')\nelse:\n    print('Thanks for participating!')\n\n\n    DATAIDEA Arithmects:\n      \nInstructions\n-------------------------\nEnter only two numbers and the operation as +, -, /, x\n\n2.022.0 = 4.0\nYour code has run successfully!\nCode execution complete.\nYou are not eligible to vote\nThanks for participating!\n\n\n\n# Creating your own errors\n\ntry: \n    # age = int(input('Enter your age: '))\n    age = ''\n\n    if age &lt; 18:\n        raise Exception('Not an adult')\nexcept Exception as error:\n    print('A problem occurred \\n'\n          f'Error: {error}')\n\nA problem occurred \nError: '&lt;' not supported between instances of 'str' and 'int'",
    "crumbs": [
      "Home",
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/07_advanced.html#variable-scope",
    "href": "Python/07_advanced.html#variable-scope",
    "title": "Advanced",
    "section": "Variable Scope",
    "text": "Variable Scope\n\n# local scope\n# a variable that is created/defined \n# inside a function has a local scope\n\n# create a fucntion that greats people\n\n# - All variable declared outside a function global\n\nname = 'Voila'\n\ndef my_func():\n    global my_fruit\n    my_fruit = 'orange'\n    # print(name + ' loves my ' + my_fruit)\n\nmy_func()\n\nprint(my_fruit)\n\norange\n\n\n\nnumber = 45 # defined outside a function\n\n# can be accessed here\nprint(number)\n\ndef getSquare():\n    # can also be accessed here\n    print(number ** 2)\n\ngetSquare()\n\n45\n2025\n\n\n\n#Local Scope\n\ndef add():\n    number1 = 5\n    number2 = 7\n    summ = number1 + number2\n    return summ\n\n# print(add())\ntry: \n    print(summ)\nexcept:\n    print(\"summ is not defined\")\n\nsumm is not defined\n\n\n\n# Global Keyword\n\ndef add():\n    global summ\n    number1 = 5\n    number2 = 7\n    summ = number1 + number2\n    return summ\n\nadd()\n\nprint(summ)\n\n12",
    "crumbs": [
      "Home",
      "Python",
      "Advanced"
    ]
  },
  {
    "objectID": "Python/02_variables.html",
    "href": "Python/02_variables.html",
    "title": "Variables",
    "section": "",
    "text": "Varibles are used to store values.\nTo create a variable, use the equal sign (=).\nIn the examples below, we create varibales name fruit and name and we assign them values 'mango' and 'viola' respectively\nfruit = 'mango'\nname = 'voila'\n\n# printing out\nprint(name, ' likes ', fruit )\n\nvoila  likes  mango",
    "crumbs": [
      "Home",
      "Python",
      "Variables"
    ]
  },
  {
    "objectID": "Python/02_variables.html#data-types",
    "href": "Python/02_variables.html#data-types",
    "title": "Variables",
    "section": "Data Types",
    "text": "Data Types\nIn this section of the tutorial, you will learn the most basic data types in Python\n\nNumbers\nThese are two basic types of numbers and they are called: - integer(numbers without decimal places) - floating point numbers(numbers with decimal places)\n\n# python numbers\n# Integers\nage = 45\npopulation = 45000000\n\n\n# Floating point numbers\nheight = 1.7\nweight = 147.45",
    "crumbs": [
      "Home",
      "Python",
      "Variables"
    ]
  },
  {
    "objectID": "Python/02_variables.html#strings",
    "href": "Python/02_variables.html#strings",
    "title": "Variables",
    "section": "Strings",
    "text": "Strings\nStrings are simply text. A string must be surrounded by single or double quotes\n\n# Strings\nname = 'Juma'\nother_name = \"Masaba Calvin\"\nstatement = 'I love coding'\n\n\nSingle or double quotes?\nUse single quotes when your string contains double quotes, or the vice versa.\n\n# when to use which quotes\nreport = 'He said, \"I will not go home\"'\n\n\n\nString Methods\n\ntext = 'shafara VEe'\ncapitalized_text = text.capitalize()\nprint('Capitalized:', capitalized_text)\n\nupper_text = text.upper()\nprint('Upper text', upper_text)\n\nlower_text = text.lower()\nprint('Lower case text:', lower_text)\n\ncorrected_text = text.replace('VEe', 'Viola')\nprint(corrected_text)\n\nprint('shafara' not in text)\n\n\n\nBooleans\nBoolean data type can onlyhave on fo these values: True or False\n\n# Boolean\nmarried = True\nprint(married)\n\nTrue\n\n\n\n\nLists\n\nA list is an ordered collection of data\nIt can contain strings, numbers or even other lists\nLists are written with square brackets ([])\nThe values in lists (also called elements) are separated by commas (,)\n\n\n# Lists\nnames = ['juma', 'john', 'calvin']\nother_stuff = ['mango', True, 38]\n\nprint(names)\nprint(other_stuff)\n\n['juma', 'john', 'calvin']\n['mango', True, 38]\n\n\n\n\nChecking data types\nTo check the data type of an object in python, use type(object), for example, below we get the data type of the object stored in names\n\n# Which data type\ntype(names)\n\nlist",
    "crumbs": [
      "Home",
      "Python",
      "Variables"
    ]
  },
  {
    "objectID": "Python/02_variables.html#converting-types",
    "href": "Python/02_variables.html#converting-types",
    "title": "Variables",
    "section": "Converting Types",
    "text": "Converting Types\n\n# convert an integer to a string\nage = 45\nmessage = 'Peter Griffin is '+ str(age) + ' years old'\n\n# Convert floating point to integer\npi = 3.14159\nprint(int(pi))\n\n3",
    "crumbs": [
      "Home",
      "Python",
      "Variables"
    ]
  },
  {
    "objectID": "Python/00_outline.html",
    "href": "Python/00_outline.html",
    "title": "Python Course Outline",
    "section": "",
    "text": "Python Overview\nIntroduction\nInstalling\nWriting Code\nDisplaying Output\nStatements\nSyntax\nComments\nExercise",
    "crumbs": [
      "Home",
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#basics",
    "href": "Python/00_outline.html#basics",
    "title": "Python Course Outline",
    "section": "",
    "text": "Python Overview\nIntroduction\nInstalling\nWriting Code\nDisplaying Output\nStatements\nSyntax\nComments\nExercise",
    "crumbs": [
      "Home",
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#variables",
    "href": "Python/00_outline.html#variables",
    "title": "Python Course Outline",
    "section": "Variables",
    "text": "Variables\n\nVariables\nData Types\nNumbers\nNumber Methods\nStrings\nType Conversion\nPython Booleans\nExercise",
    "crumbs": [
      "Home",
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#operations",
    "href": "Python/00_outline.html#operations",
    "title": "Python Course Outline",
    "section": "Operations",
    "text": "Operations\n\nOperators Intro\nArithmetics\nAssignment\nComparison\nLogical\nIdentity\nMembership\nExercise",
    "crumbs": [
      "Home",
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#collections",
    "href": "Python/00_outline.html#collections",
    "title": "Python Course Outline",
    "section": "Collections",
    "text": "Collections\n\nContainers\nList\nTuple\nSet\nDictionary\nExercise",
    "crumbs": [
      "Home",
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#flow-control",
    "href": "Python/00_outline.html#flow-control",
    "title": "Python Course Outline",
    "section": "Flow Control",
    "text": "Flow Control\n\nFunctions\nLambda functions\nIf else\nIf else shorthand\nFor Loop\nWhile Loop\nBreak and Continue\nPass\nExercise",
    "crumbs": [
      "Home",
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#advanced",
    "href": "Python/00_outline.html#advanced",
    "title": "Python Course Outline",
    "section": "Advanced",
    "text": "Advanced\n\nClasses and Objects\nInheritance\nVariable Scope\nFormatting Strings\nTry … Except\nIterators\nUser Input\nExercise",
    "crumbs": [
      "Home",
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#modules",
    "href": "Python/00_outline.html#modules",
    "title": "Python Course Outline",
    "section": "Modules",
    "text": "Modules\n\nIntro\nMath\nRandom\nDate and Time\nJSON\nRegular Expressions\nExercise",
    "crumbs": [
      "Home",
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/00_outline.html#working-with-files",
    "href": "Python/00_outline.html#working-with-files",
    "title": "Python Course Outline",
    "section": "Working With Files",
    "text": "Working With Files\n\nFile Handling\nFile Reading\nFile Writing/Creating/Appending\nFile Deleting\nExercise",
    "crumbs": [
      "Home",
      "Python",
      "Python Course Outline"
    ]
  },
  {
    "objectID": "Python/01_basics.html",
    "href": "Python/01_basics.html",
    "title": "Basics",
    "section": "",
    "text": "This course will teach you the basics and advanced concepts of Python Programming",
    "crumbs": [
      "Home",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#what-is-python",
    "href": "Python/01_basics.html#what-is-python",
    "title": "Basics",
    "section": "What is Python",
    "text": "What is Python\n\nPython is a programming language.\nPython is one of the most popular programming languages\n\n\nWho created Python?\nPython was created by Guido van Rossum and it was first implemented in 1989\n\n\nWhat is Python used for?\nPython is used for: 1. Web Development 2. Machine Learning 3. Data Science 4. Scripting 5. And many more\n\n\nWhat is the latest version of Python?\n\nPython 3 is the latest version of Python\nThis tutorial is based on the standards of Python 3",
    "crumbs": [
      "Home",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#installing-python",
    "href": "Python/01_basics.html#installing-python",
    "title": "Basics",
    "section": "Installing Python",
    "text": "Installing Python\nBefore you can run Python on your PC, you need to install it first.\nTo install Python in a PC, go to https://www.python.org/downloads/ then download the latest version.\nAfter that, install it just like how you install other apps.\nMake sure that you check “Add Python to PATH” for easier installation.",
    "crumbs": [
      "Home",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#writing-python-code",
    "href": "Python/01_basics.html#writing-python-code",
    "title": "Basics",
    "section": "Writing Python Code",
    "text": "Writing Python Code\nIn order to learn Python, you need to be able to write and execute code.\n\nPython Console (Shell)\nPython console also known as shell allows you to execute Python code line by line\nAssuming that you have already installed Python on your PC, you can access the Python console by opening the command prompt and typing python\nLet’s start using the console\nType the following and hit enter\nname = 'Juma Shafara'\nAgain, type the following and hit enter\nprint(name)\nAfter that, you should see this\nJuma Shafara\n\n\nPython Files\nPython files are saved with .py file extension\nYou can use any text editor (even notepad) to create Python files\nJust make sure that you save them with the .py extension, forexample hello.py.\nCopy this code and save it as hello.py:\nprint('Hello World!')\nTo run this Python file on a PC, navigate to the folder where is is located using the command prompt.\nThen type the following and hit enter\npython hello.py\nThe console should then output:\nHello World!\n\n\n\nPython Console",
    "crumbs": [
      "Home",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#python-displaying-output",
    "href": "Python/01_basics.html#python-displaying-output",
    "title": "Basics",
    "section": "Python Displaying output",
    "text": "Python Displaying output\nTo display an output in Python, use the print() function.\n\nprint('Hello world!')\n\nHello world!\n\n\n\nprint(27)\n\n27\n\n\n\nprint(3 + 27)\n\n30",
    "crumbs": [
      "Home",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#printing-two-objects",
    "href": "Python/01_basics.html#printing-two-objects",
    "title": "Basics",
    "section": "Printing two objects",
    "text": "Printing two objects\nThe print() function can be used to print two objects. Eg.\n\nprint('Hello', 'Juma')\n\nHello Juma\n\n\n\nx = 3\ny = 7\nsumm = x + y\nprint('the sum is ', summ)\n\nthe sum is  10\n\n\n\nx = 4; y = 3; print(x + y)\n\n7",
    "crumbs": [
      "Home",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#python-statements",
    "href": "Python/01_basics.html#python-statements",
    "title": "Basics",
    "section": "Python Statements",
    "text": "Python Statements\nA python statement is used to write a value, compute a value, assign a value to a variable, call a functino and many more. Eg.\n\nx = 5\ny = 3\nsumm = x + y\nprint(summ)\n\n8\n\n\nIn the example above, we have 4 lines of code. In python, each line typically contains one statement\n\nMultiple statements in one line\nYou can also write multiple statements in a single of code. Simply separate the statements with semicolons ;\n\na = 4; b = 3; sum = a + b; print(sum)\n\n7",
    "crumbs": [
      "Home",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#python-syntax",
    "href": "Python/01_basics.html#python-syntax",
    "title": "Basics",
    "section": "Python Syntax",
    "text": "Python Syntax\nWhen coding in Python, we follow a syntax. Syntax is the set of rules followed when writing programs\n\nPython indentation\n\nIn python, indentation indicates a block(group) of statements\nTabs or leading whitespaces are used to compute the indentation level of a line\nIt depends on you whether you want to use tabs or whitespaces, in the example below, we use 2 whitespaces\n\n\nnumber1 = 4\nnumber2 = 3\n\nif number1 &gt; number2:\n  x = 'Hello, world'\n  print(x)\n\nHello, world",
    "crumbs": [
      "Home",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#python-comments",
    "href": "Python/01_basics.html#python-comments",
    "title": "Basics",
    "section": "Python Comments",
    "text": "Python Comments\n\nComments in Python are used to clarify or explain codes\nComments are not interpreted by Python, meaning comments will not be executed\n\n\n# this is a comment\nx = 4 \ny = 3\n\n# some comment\n# second comment\n# third comment\n\nprint(x + y) # prints out the sum\n\n7",
    "crumbs": [
      "Home",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/01_basics.html#end-of-first-module",
    "href": "Python/01_basics.html#end-of-first-module",
    "title": "Basics",
    "section": "End of first module",
    "text": "End of first module\nThe nice introduction ends here, in the next section, we will look at variables in Python",
    "crumbs": [
      "Home",
      "Python",
      "Basics"
    ]
  },
  {
    "objectID": "Python/04_operators.html",
    "href": "Python/04_operators.html",
    "title": "Operators",
    "section": "",
    "text": "Operators are symbols that perform operations on operands. Operands can be variables, strings, numbers, booleans etc",
    "crumbs": [
      "Home",
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#arithmetic",
    "href": "Python/04_operators.html#arithmetic",
    "title": "Operators",
    "section": "Arithmetic",
    "text": "Arithmetic\nArithemators are symbols that perform mathematical operations on operands\n\n\n\nArithmetic Operator\nDescription\n\n\n\n\n+\nAddition\n\n\n-\nSubraction\n\n\n/\nDivision\n\n\n*\nMultiplication\n\n\n**\nExponentiates\n\n\n%\nRemainder\n\n\n\n\n# Let \nx = 10 \ny = 5\n\n\n# Addition\nsummation = x + y\nprint(summation)\n\n15\n\n\n\n# Subraction\ndifference = x - y\nprint(difference)\n\n5\n\n\n\n# Division\nquotient = x / y\nprint(quotient)\n\n2.0\n\n\n\n# Multiplication\nproduct = x * y\nprint(product)\n\n50\n\n\n\n# Exponentiation \nexponent = x ** y\nprint(exponent)\n\n100000\n\n\n\n# Remainder\nremainder = x % y\nprint(remainder)\n\n0\n\n\n\n# Floor Division\nfloor = 10 / 4\nprint(floor)\n\n2.5\n\n\n\n# Perform in a sequence\nans = 10 * 3 / 2 + 1\nprint(ans)\n\n16.0",
    "crumbs": [
      "Home",
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#assignment",
    "href": "Python/04_operators.html#assignment",
    "title": "Operators",
    "section": "Assignment",
    "text": "Assignment\nAssignment operators are used to assign values to variables.\n\n\n\nName\nOperation\nSame As\n\n\n\n\nAssignment\nx = y\nx = y\n\n\nAddition Ass\nx += y\nx = x + y\n\n\nSubtraction Ass\nx -= y\nx = x - y\n\n\nMult Ass\nx *= y\nx = x * y\n\n\nDivision Ass\nx /= y\nx = x / y\n\n\nExpo Ass\nx **= y\nx = x ** y\n\n\nRemainder Ass\nx %= y\nx = x % y\n\n\nFloor Div Ass\nx //= y\nx = x // y\n\n\n\n\n# Examples\n# Assignment\nx = 10\n# Addition Ass\nx += 5 # x = x + 5 =&gt; x = 10 + 5 =&gt; x = 15\nprint(x)\n\n15\n\n\n\n# Subraction Ass\nx = 10\nx -= 5 # x = x - 5 =&gt; x = 10 - 5 =&gt; x = 5\nprint(x)\n\n5",
    "crumbs": [
      "Home",
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#comparison",
    "href": "Python/04_operators.html#comparison",
    "title": "Operators",
    "section": "Comparison",
    "text": "Comparison\nA comparison operator compares its operands and returns a Boolean value based on whether the comparison is True of False\n\n\n\nName\nOperation\n\n\n\n\nEquality\n==\n\n\nInequality\n!=\n\n\nGreater than\n&gt;\n\n\nLess than\n&lt;\n\n\nGreater or equal\n&gt;=\n\n\nLess or equal\n&lt;=\n\n\n\n\n# Examples\n# Equality \n'Voila' == 'Viola'\n\nFalse\n\n\n\n# Inequality \n'Voila' != 'Viola'\n\nTrue\n\n\n\n# Greater or Equal\n34 &gt;= 43\n\nFalse\n\n\n\n# Tip\nprint('Voila' == 'Viola' == 'Voila')\n#       False == True =&gt; False\n\nFalse\n\n\n\n# weight = int(input(\"Enter your weight: \"))\n# height = int(input('Enter your height: '))\n\nweight = 56\nheight = 1.5\n\nbmi = weight/(height**2)\n\nif bmi &gt; 28:\n    print('You are over weight')\nelif bmi &gt; 18:\n    print('You are normal weight')\nelse:\n    print('You are under weight')\n\nYou are normal weight",
    "crumbs": [
      "Home",
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#identity",
    "href": "Python/04_operators.html#identity",
    "title": "Operators",
    "section": "Identity",
    "text": "Identity\nIdentity operators are used to compare two values to determine if they point to the same object\n\n\n\nOperator\nName\n\n\n\n\nis\nThe is operator\n\n\nis not\nThe is not operator\n\n\n\n\n# Example\n# is\nx = 5\ny = 4\nz = x # x = 5 =&gt; z = 5\n\nprint(x is not z)\n\nFalse",
    "crumbs": [
      "Home",
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#logical",
    "href": "Python/04_operators.html#logical",
    "title": "Operators",
    "section": "Logical",
    "text": "Logical\nLogical operators are commonly used with Booleans. In Python, there are 3 logical operators\n\n\n\nOperator\nDescription\n\n\n\n\nand\nLogical and operator\n\n\nor\nLogical or\n\n\nnot\nLogical not\n\n\n\n\nLogical and\nThe logical and operator returns True if both operands are True\n\n# Example\n# Logical and\nx = 4\nprint(x &gt; 3 and 8 &lt; x)\n#               True and False =&gt; False\n\nFalse\n\n\n\n\nLogical or\nThe logical or operator returns True if one of the operands is True\n\n# Logical or\ny = 7\nexpression_2 = 10 &gt; y or 4 &gt; y\n#                   True or False =&gt; True\nprint(expression_2)\n\nTrue\n\n\n\n\nLogical not\nThe logical not operator returns True if the operand is False, otherwise returns False if the operand is True\n\n# Logical not\nz = 8\nexpression_3 = not(10 == z)\n#               not False =&gt; True\nprint(expression_3)\n\nTrue",
    "crumbs": [
      "Home",
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/04_operators.html#membership",
    "href": "Python/04_operators.html#membership",
    "title": "Operators",
    "section": "Membership",
    "text": "Membership\nMembership operators are used to check if a sequence is present in an object like a string, list etc\n\n\n\nOperator\nName\n\n\n\n\nin\nThe in operator\n\n\nnot in\nThe not in operator\n\n\n\n\n# Example\nname = 'Tinye Robert'\nprint('Robert' not in name)\n\nFalse",
    "crumbs": [
      "Home",
      "Python",
      "Operators"
    ]
  },
  {
    "objectID": "Python/06_flow_control.html",
    "href": "Python/06_flow_control.html",
    "title": "Flow Control",
    "section": "",
    "text": "A function in python is a group statements that perform a particular task\n\n# This function calculates Body Mass Index\ndef calculateBodyMassIndex(weight_kg, height_m):\n\n    body_mass_index = weight_kg / pow(height_m, 2)\n    rounded_bmi = round(body_mass_index, 2)\n\n    return rounded_bmi\n\n\n# lets try\ncalculateBodyMassIndex(67, 1.6)\n\n26.17\n\n\n\n\nTo create a function, we need the following: - The def keyword - A function name - Round brackets () and a colon : - A function body- a group of statements\n\ndef greeter():\n    message = 'Hello'\n    print(message)\n\n\nTo execute a function, it needs to be called\nTo call a function, use its function name with parentheses ()\n\n\ngreeter()\n\nHello\n\n\n\n\n\n\nWhen calling a function, we can pass data using parameters/arguments\nA parameter is a variable declared in the function. In the example below, number1 and number2 are parameter\nThe argument is the value passed to the function when its called. In the example below 3 and 27 are the arguments\n\n\n# define the function\ndef addNumbers(number1, number2):\n    sum = number1 + number2\n    print(sum)\n\n# Call the function\naddNumbers(3, 27)\n\n30\n\n\n\n# setting a default argument\ndef greet(name='you'):\n    message = 'Hello ' + name\n    print(message)\n\ngreet('Tinye')\ngreet()\n\nHello Tinye\nHello you\n\n\n\n\n\nThe return statement is used to return a value to a function caller\n\ndef addNumbers(number1, number2):\n    sum = number1 + number2\n    return sum\n\nsummation = addNumbers(56, 4)\nprint(summation)\n\n60\n\n\n### lambda functions - Lambda functions are functions that donot have names - The body of a lambda function can only have one expression, but can have multiple arguments - The result of the expression is automatically returned\nSyntax: python   lambda parameters: expression\n\n# Example of lambda function\ncalculateBMI = lambda weight_kg, height_m: round((weight_kg/(height_m ** 2)), 2)\n# Calling a labda function\ncalculateBMI(67, 1.7)\n\n23.18\n\n\n\n\n\n\n\n\n# Assume 4 course units\n# 1. Math - A\n# 2. Science - B\n# 3. SST - B\n# 4. English - C\n\n\ndef calculate_CGPA(GPs_list, CUs_list):\n    length = len(GPs_list)\n    product_sum = 0\n\n    for item in range(length):\n        product_sum += GPs_list[item] * CUs_list[item]\n\n    CUs_sum = sum(CUs_list)\n\n    CGPA = product_sum / CUs_sum\n\n    return CGPA\n\n# calculate_CGPA(4, 5)\n\n\n\n\n\ndef getAge(month, year):\n    month_diff = 12 - month\n    year_diff = 2023 - year\n\n    return str(year_diff) + ' years ' + str(month_diff) + ' months'  \n    \nage = getAge(year=2000, month=10) # keyword argument\nage2 = getAge(10, 2000) # positional argument\n\nprint(age)\n\n23 years 2 months\n\n\n\n\n\n\n\nLoops are used to repetitively execute a group of statements\nwe have 2 types, for and while loop\n\n\n\nA for loop is used to loop through or iterate over a sequence or iterable objects\nSyntax:\nfor variable in sequence:\n    statements\n\npets = ['cat', 'dog', 'rabbit']\n# iterate through pets\nfor pet in pets:\n    print(pet)\n\ncat\ndog\nrabbit\n\n\n\n# convert all weights in list from kg to pounds\nweights_kg = [145, 100, 76, 80]\nweights_pds = []\n\nfor weight in weights_kg:\n    pounds = weight * 2.2\n    rounded_pds = round(pounds, 2)\n    weights_pds.append(rounded_pds)\n\nprint(weights_pds)\n\n[319.0, 220.0, 167.2, 176.0]\n\n\n\n# Display all letters in a name\nname = 'Shafara'\n\nfor letter in name:\n    print(letter)\n\nS\nh\na\nf\na\nr\na\n\n\n\n# print 'Hello you' 5 times\nfor step in range(0, 5):\n    print('Hello you')\n\nHello you\nHello you\nHello you\nHello you\nHello you\n\n\n\n\n\n\n\nThe while loop executes a given group of statements as long as the given expression is True\n\nSyntax:\nwhile expression:\n    statements\n\ncounter = 0\n\nwhile counter &lt; 5:\n    print('Hello you')\n    counter += 1\n\nHello you\nHello you\nHello you\nHello you\nHello you\n\n\n\n# Convert the weights in the list from kgs to pounds\nweights_kg = [145, 100, 76, 80]\nweights_pds = []\n\ncounter = 0\nend = len(weights_kg)\n\nwhile counter &lt; end:\n\n    pound = weights_kg[counter] * 2.2\n    rounded_pds = round(pound, 3)\n    weights_pds.append(rounded_pds)\n\n    counter += 1\n\nprint(weights_pds)\n\n[319.0, 220.0, 167.2, 176.0]",
    "crumbs": [
      "Home",
      "Python",
      "Flow Control"
    ]
  },
  {
    "objectID": "Python/06_flow_control.html#functions",
    "href": "Python/06_flow_control.html#functions",
    "title": "Flow Control",
    "section": "",
    "text": "A function in python is a group statements that perform a particular task\n\n# This function calculates Body Mass Index\ndef calculateBodyMassIndex(weight_kg, height_m):\n\n    body_mass_index = weight_kg / pow(height_m, 2)\n    rounded_bmi = round(body_mass_index, 2)\n\n    return rounded_bmi\n\n\n# lets try\ncalculateBodyMassIndex(67, 1.6)\n\n26.17\n\n\n\n\nTo create a function, we need the following: - The def keyword - A function name - Round brackets () and a colon : - A function body- a group of statements\n\ndef greeter():\n    message = 'Hello'\n    print(message)\n\n\nTo execute a function, it needs to be called\nTo call a function, use its function name with parentheses ()\n\n\ngreeter()\n\nHello\n\n\n\n\n\n\nWhen calling a function, we can pass data using parameters/arguments\nA parameter is a variable declared in the function. In the example below, number1 and number2 are parameter\nThe argument is the value passed to the function when its called. In the example below 3 and 27 are the arguments\n\n\n# define the function\ndef addNumbers(number1, number2):\n    sum = number1 + number2\n    print(sum)\n\n# Call the function\naddNumbers(3, 27)\n\n30\n\n\n\n# setting a default argument\ndef greet(name='you'):\n    message = 'Hello ' + name\n    print(message)\n\ngreet('Tinye')\ngreet()\n\nHello Tinye\nHello you\n\n\n\n\n\nThe return statement is used to return a value to a function caller\n\ndef addNumbers(number1, number2):\n    sum = number1 + number2\n    return sum\n\nsummation = addNumbers(56, 4)\nprint(summation)\n\n60\n\n\n### lambda functions - Lambda functions are functions that donot have names - The body of a lambda function can only have one expression, but can have multiple arguments - The result of the expression is automatically returned\nSyntax: python   lambda parameters: expression\n\n# Example of lambda function\ncalculateBMI = lambda weight_kg, height_m: round((weight_kg/(height_m ** 2)), 2)\n# Calling a labda function\ncalculateBMI(67, 1.7)\n\n23.18\n\n\n\n\n\n\n\n\n# Assume 4 course units\n# 1. Math - A\n# 2. Science - B\n# 3. SST - B\n# 4. English - C\n\n\ndef calculate_CGPA(GPs_list, CUs_list):\n    length = len(GPs_list)\n    product_sum = 0\n\n    for item in range(length):\n        product_sum += GPs_list[item] * CUs_list[item]\n\n    CUs_sum = sum(CUs_list)\n\n    CGPA = product_sum / CUs_sum\n\n    return CGPA\n\n# calculate_CGPA(4, 5)\n\n\n\n\n\ndef getAge(month, year):\n    month_diff = 12 - month\n    year_diff = 2023 - year\n\n    return str(year_diff) + ' years ' + str(month_diff) + ' months'  \n    \nage = getAge(year=2000, month=10) # keyword argument\nage2 = getAge(10, 2000) # positional argument\n\nprint(age)\n\n23 years 2 months\n\n\n\n\n\n\n\nLoops are used to repetitively execute a group of statements\nwe have 2 types, for and while loop\n\n\n\nA for loop is used to loop through or iterate over a sequence or iterable objects\nSyntax:\nfor variable in sequence:\n    statements\n\npets = ['cat', 'dog', 'rabbit']\n# iterate through pets\nfor pet in pets:\n    print(pet)\n\ncat\ndog\nrabbit\n\n\n\n# convert all weights in list from kg to pounds\nweights_kg = [145, 100, 76, 80]\nweights_pds = []\n\nfor weight in weights_kg:\n    pounds = weight * 2.2\n    rounded_pds = round(pounds, 2)\n    weights_pds.append(rounded_pds)\n\nprint(weights_pds)\n\n[319.0, 220.0, 167.2, 176.0]\n\n\n\n# Display all letters in a name\nname = 'Shafara'\n\nfor letter in name:\n    print(letter)\n\nS\nh\na\nf\na\nr\na\n\n\n\n# print 'Hello you' 5 times\nfor step in range(0, 5):\n    print('Hello you')\n\nHello you\nHello you\nHello you\nHello you\nHello you\n\n\n\n\n\n\n\nThe while loop executes a given group of statements as long as the given expression is True\n\nSyntax:\nwhile expression:\n    statements\n\ncounter = 0\n\nwhile counter &lt; 5:\n    print('Hello you')\n    counter += 1\n\nHello you\nHello you\nHello you\nHello you\nHello you\n\n\n\n# Convert the weights in the list from kgs to pounds\nweights_kg = [145, 100, 76, 80]\nweights_pds = []\n\ncounter = 0\nend = len(weights_kg)\n\nwhile counter &lt; end:\n\n    pound = weights_kg[counter] * 2.2\n    rounded_pds = round(pound, 3)\n    weights_pds.append(rounded_pds)\n\n    counter += 1\n\nprint(weights_pds)\n\n[319.0, 220.0, 167.2, 176.0]",
    "crumbs": [
      "Home",
      "Python",
      "Flow Control"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "core"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week3/32_data_exploration_and_cleaning_exercise.html",
    "href": "Python Data Analysis/Week3/32_data_exploration_and_cleaning_exercise.html",
    "title": "Data Exploration and Cleaning Exercise",
    "section": "",
    "text": "Load demo.xlsx dataset\nRename the columns as suggested below\n\n\n\nOld name\nNew name\n\n\n\n\nAge\nage\n\n\nGender\ngender\n\n\nMarital Status\nmarital_status\n\n\nAddress\naddress\n\n\nIncome\nincome\n\n\nIncome Category\nincome_category\n\n\nJob Category\njob_category\n\n\n\nDisplay all the columns in the dataset\nDisplay some basic statistics about the numeric variables in the dataset\nDisplay some basic statistics about the categorical variables in the dataset\nWhat are the unique observations under gender?\nCan you fix any problems observed under the gender, give brief explanations why and how\nHow many observations have ‘no answer’ for marital status?\nWrite some piece of code to return only numeric variables from the dataset\nAre there any missing values in the dataset?\nAre there any outliers in the income variable?\nInvestigate the relationship between age and income\nHow many people earn more than 300 units?\nWhat data type is the marital status?\nCreate dummy variables for gender\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week3",
      "Data Exploration and Cleaning Exercise"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week6/62_seaborn_part1.html",
    "href": "Python Data Analysis/Week6/62_seaborn_part1.html",
    "title": "Seaborn Part 1",
    "section": "",
    "text": "Seaborn is a Python data visualization library based on matplotlib. Seaborn makes it easy to create informative and attractive statistical graphics. We’ll cover basic plotting techniques and explore some of the functionalities Seaborn offers.\n\nInstalling Seaborn\nBefore we begin, make sure you have Seaborn installed. You can install it via pip if you haven’t already:\n\n# install seaborn\n!pip install seaborn\n\n\n### Importing Seaborn and Other Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n\nLoading Sample Dataset\nSeaborn comes with some built-in datasets for practice. For this tutorial, we’ll use the “tips” dataset, which contains information about tips given in a restaurant.\n\ntips = sns.load_dataset(\"tips\")\n### Basic Plots\n\n\ntips.head()\n\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\nHere are the columns typically found in the “tips” dataset:\n\ntotal_bill: Total bill amount (including tip).\ntip: Tip amount.\nsex: Gender of the person paying the bill (male or female).\nsmoker: Whether the party included smokers (yes or no).\nday: The day of the week.\ntime: Whether the meal was lunch or dinner.\nsize: The size of the dining party (number of people).\n\n\n1. Scatter Plot\n\nsns.scatterplot(x='total_bill', y='tip', data=tips)\nplt.title('Scatter Plot of Total Bill vs Tip')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2. Line Plot\n\nsns.lineplot(x='size', y='total_bill', data=tips)\nplt.title('Line Plot of Party Size vs Total Bill')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3. Histogram\n\nsns.histplot(tips['total_bill'], bins=10, kde=True)\nplt.title('Histogram of Total Bill')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4. Bar Plot\n\nsns.barplot(x='size', y='total_bill', data=tips)\nplt.title('Average Total Bill by Day')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCustomizing Plots\n\n1. Changing Color Palette\n\nsns.set_palette(\"dark\")\nsns.barplot(x='day', y='total_bill', data=tips)\nplt.title('Average Total Bill by Day')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2. Adding Labels and Legends\n\nsns.scatterplot(x='total_bill', y='tip', hue='sex', data=tips)\nplt.title('Scatter Plot of Total Bill vs Tip')\nplt.xlabel('Total Bill ($)')\nplt.ylabel('Tip ($)')\nplt.legend(title='Gender')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3. Adding Annotations\n\nsns.scatterplot(x='total_bill', y='tip', data=tips)\nplt.title('Scatter Plot of Total Bill vs Tip')\nplt.annotate('This point seems interesting', xy=(20, 2), xytext=(25, 4),\n             arrowprops=dict(facecolor='black', shrink=0.05))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Visualizations\n\n1. Pair Plot\n\nsns.pairplot(tips, hue='sex')\nplt.savefig('plot.png')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2. Heatmap\n\n# Exclude non-numeric columns from correlation calculation\nnumeric_cols = tips.select_dtypes(include=['float64', 'int64'])\ncorrelation_matrix = numeric_cols.corr()\n\n# Plotting the heatmap\nsns.heatmap(correlation_matrix, annot=True)\nplt.title('Correlation Heatmap')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n3. Violin Plot\n\nsns.violinplot(x='day', y='total_bill', data=tips, split=True)\nplt.title('Violin Plot of Total Bill by Day and Gender')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nSeaborn provides a high-level interface for drawing attractive and informative statistical graphics. This tutorial covered basic plotting techniques and some advanced visualizations. Experiment with different plot types and customization options to create visualizations tailored to your specific needs.\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week6",
      "Seaborn Part 1"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week6/61_matplotlib.html",
    "href": "Python Data Analysis/Week6/61_matplotlib.html",
    "title": "Matplotlib",
    "section": "",
    "text": "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom dataidea.datasets import loadDataset\n\n\n# Load the Titanic dataset\ndemo_df = loadDataset('../assets/demo_cleaned.csv', inbuilt=False, file_type='csv')\n\n\nCreate a plot\n\nfig = plt.figure()\n# All plotting is done with respect to an Axes. \nfig.add_axes([0.1, 0.1, 0.5, 0.5])\n\n\n\n\n\n\n\n\nIn most cases, a subplot will fit your needs. A subplot is an axes on a grid system.\n\nfig1, ax = plt.subplots()\nax.hist(demo_df.income)\nplt.show()\n\n\n\n\n\n\n\n\n\nfig2, ax = plt.subplots()\nax.hist(demo_df.income)\n# plt.grid(True)\nax2 = fig2.add_subplot(222) # row-col-num\nax2.hist(demo_df['age'])\nplt.show()\n\n\n\n\n\n\n\n\n\nfig3, axes = plt.subplots(nrows=2,ncols=2)\n\n\n\n\n\n\n\n\n\nfig4, axes = plt.subplots(nrows=2,ncols=2)\n# add bar graph\ngender_counts = demo_df.gender.value_counts()\naxes[0,0].bar(gender_counts.index, gender_counts.values)\n# add histogram\naxes[0,1].hist(demo_df.age, bins=20, edgecolor='black')\n# add box plot\naxes[1, 0].boxplot(demo_df.income, vert=0)\n# add scatter plot\naxes[1, 1].scatter(demo_df.age, demo_df.income)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2D Data or Images\n\nfrom PIL import Image\n\nimage = Image.open('../assets/dataidea-logo.png')\n\nfig4, ax = plt.subplots()\nax.imshow(image)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSave Figure\n\nfig4, axes = plt.subplots(nrows=2,ncols=2)\n# add bar graph\ngender_counts = demo_df.gender.value_counts()\naxes[0,0].bar(gender_counts.index, gender_counts.values)\n# add histogram\naxes[0,1].hist(demo_df.age, bins=20, edgecolor='black')\n# add box plot\naxes[1, 0].boxplot(demo_df.income, vert=0)\n# add scatter plot\naxes[1, 1].scatter(demo_df.age, demo_df.income)\n\nplt.savefig('figure.pdf')\nplt.show()\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week6",
      "Matplotlib"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week8/80_classification_metrics.html",
    "href": "Python Data Analysis/Week8/80_classification_metrics.html",
    "title": "SciKit-Learn Classification Metrics",
    "section": "",
    "text": "In scikit-learn, classification metrics are essential tools to evaluate the performance of a classification model. They provide insights into how well the model is performing and where it may need improvements. Here are some commonly used classification metrics along with examples of how to implement them using scikit-learn:\n\nimport pandas as pd\n\n\n# True labels\ny_true = [0, 1, 1, 0, 1]\n# Predicted labels\ny_pred = [1, 1, 0, 0, 1]\n\n\nAccuracy:\n\nAccuracy measures the ratio of correctly predicted instances to the total instances.\nFormula:\n\nAccuracy = Number of Correct Predictions / Total Number of Predictions\n\n\n\nphoto by evidentlyai\n\n\n\nfrom sklearn.metrics import accuracy_score\n\n# Calculate accuracy\naccuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\nprint(\"Accuracy:\", accuracy)\n\nAccuracy: 0.6\n\n\n\n\nPrecision:\n\nPrecision measures the ability of the classifier not to label as positive a sample that is negative.\nPrecision is simply the models ability to not make a mistake\nFormula:\n\nPrecision = True Positives / (True Positives + False Positives)\n\n\n\nphoto by evidentlyai\n\n\n\nfrom sklearn.metrics import precision_score\n\n# Calculate precision\nprecision = precision_score(y_true, y_pred)\nprint(\"Precision:\", precision * 100)\n\nPrecision: 66.66666666666666\n\n\n\n\nRecall (also known as Sensitivity or True Positive Rate):\n\nRecall measures the ability of the classifier to find all the positive samples.\nFormula:\n\nRecall = True Positives / (True Positives + False Negatives)\n\n\n\nphoto by evidentlyai\n\n\n\nfrom sklearn.metrics import recall_score\n\n# Calculate recall\nrecall = recall_score(y_true, y_pred)\nprint(\"Recall:\", recall)\n\nRecall: 0.6666666666666666\n\n\n\n\nF1 Score:\n\nF1 Score is the harmonic mean of precision and recall.\nIt provides a balance between precision and recall.\nFormula:\n\nF1 Score = (2 x Precision x Recall) / (Precision + Recall)\n\nfrom sklearn.metrics import f1_score\n\n# Calculate F1 Score\nf1 = f1_score(y_true, y_pred)\nprint(\"F1 Score:\", f1)\n\nF1 Score: 0.6666666666666666\n\n\nIn classification tasks, metrics like precision, recall, and F1-score are commonly used to evaluate the performance of a model. When dealing with multi-class classification, you often need a way to aggregate these metrics across all classes. Three common methods for doing this are micro-average, macro-average, and weighted average.\n\n\nMicro-average:\n\nCalculate metrics globally by counting the total true positives, false negatives, and false positives.\nThis method gives equal weight to each individual prediction, regardless of class imbalance.\n\n\n\n\nphoto by evidentlyai\n\n\n\n# Calculate micro-average\nmicro_precision = precision_score(y_true, y_pred, average='micro')\nmicro_recall = recall_score(y_true, y_pred, average='micro')\nmicro_f1 = f1_score(y_true, y_pred, average='micro')\n\nprint('Micro Precision:', micro_precision)\nprint('Micro Recall:', micro_recall)\nprint('Micro F1:', micro_f1)\n\nMicro Precision: 0.6\nMicro Recall: 0.6\nMicro F1: 0.6\n\n\n\n\nMacro-average:\n\nCalculate metrics for each class individually and then average them.\nThis method treats all classes equally, giving each class the same weight.\nTo obtain macro-averaged precision, recall, and F1-score:\n\nCalculate precision, recall, and F1-score for each class.\nAverage the precision, recall, and F1-score across all classes.\n\n\n\n\n\nphoto by evidentlyai\n\n\n\n# Calculate macro-average\nmacro_precision = precision_score(y_true, y_pred, average='macro')\nmacro_recall = recall_score(y_true, y_pred, average='macro')\nmacro_f1 = f1_score(y_true, y_pred, average='macro')\n\nprint('Macro Precision:', macro_precision)\nprint('Macro Recall:', macro_recall)\nprint('Macro F1:', macro_f1)\n\nMacro Precision: 0.5833333333333333\nMacro Recall: 0.5833333333333333\nMacro F1: 0.5833333333333333\n\n\n\n\nWeighted average:\n\nCalculate metrics for each class individually and then average them, weighted by the number of true instances for each class.\nThis method considers class imbalance by giving more weight to classes with more instances.\nTo obtain weighted-averaged precision, recall, and F1-score:\n\nCalculate precision, recall, and F1-score for each class.\nWeighted average is calculated as the sum of (metric * class_weight) / total_number_of_samples, where class_weight is the ratio of the number of true instances in the given class to the total number of true instances.\n\n\n\n# Calculate weighted-average\nweighted_precision = precision_score(y_true, y_pred, average='weighted')\nweighted_recall = recall_score(y_true, y_pred, average='weighted')\nweighted_f1 = f1_score(y_true, y_pred, average='weighted')\n\nprint('Weighted Precision:', weighted_precision)\nprint('Weighted Recall:', weighted_recall)\nprint('Weighted F1:', weighted_f1)\n\nWeighted Precision: 0.6\nWeighted Recall: 0.6\nWeighted F1: 0.6\n\n\nWeighted_precision:\n(Precision_A * N_A + Precision_B * N_B, ... , Precision_n * N_n) / (N_A + N_B + ... + N_n)\n\nMicro-average is useful when overall performance across all classes is important\nMacro-average is helpful when you want to evaluate the model’s performance on smaller classes equally.\nWeighted average is suitable when you want to account for class imbalance.\n\n\n\nThe Classification Report\nThe classification report in scikit-learn provides a comprehensive summary of different classification metrics for each class in the dataset. It includes precision, recall, F1-score, and support (the number of true instances for each label). Here’s how you can generate a classification report:\n\nfrom sklearn.metrics import classification_report\n\n# Generate classification report\nclass_report_dict = classification_report(y_true, y_pred, output_dict=True)\nclass_report_df = pd.DataFrame(class_report_dict).transpose()\n\nprint(\"Classification Report:\\n\")\nclass_report_df\n\nClassification Report:\n\n\n\n\n\n\n\n\n\n\n\nprecision\nrecall\nf1-score\nsupport\n\n\n\n\n0\n0.500000\n0.500000\n0.500000\n2.0\n\n\n1\n0.666667\n0.666667\n0.666667\n3.0\n\n\naccuracy\n0.600000\n0.600000\n0.600000\n0.6\n\n\nmacro avg\n0.583333\n0.583333\n0.583333\n5.0\n\n\nweighted avg\n0.600000\n0.600000\n0.600000\n5.0\n\n\n\n\n\n\n\n\n\n(0.666667 + 0.5)/2\n\n0.5833335\n\n\n\n\nConfusion Matrix:\n\nA confusion matrix is a table that is often used to describe the performance of a classification model.\nIt presents a summary of the model’s predictions on the classification problem, showing correct predictions as well as types of errors made.\n\n\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\n\n# Calculate confusion matrix\nconf_matrix = confusion_matrix(y_true, y_pred)\n\nconf_matrix = pd.DataFrame(conf_matrix, index=[0, 1], columns=[0, 1])\n# print(\"Confusion Matrix:\\n\", conf_matrix)\nconf_matrix\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\n1\n1\n\n\n1\n1\n2\n\n\n\n\n\n\n\n\nThese are just a few of the many classification metrics available in scikit-learn. Depending on your specific problem and requirements, you may want to explore other metrics as well.\nUnderstanding these metrics and how they are computed can provide valuable insights into the performance of a classification model and help in making informed decisions about its improvement.\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week8",
      "SciKit-Learn Classification Metrics"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/72_why_scaling.html",
    "href": "Python Data Analysis/Week7/72_why_scaling.html",
    "title": "Why re-scaling (Iris)?",
    "section": "",
    "text": "Let’s use the Iris dataset, a popular dataset in machine learning. The Iris dataset consists of 150 samples of iris flowers, with each sample containing four features: sepal length, sepal width, petal length, and petal width. We’ll demonstrate the effect of not scaling the features on K-means clustering.\nFirst, let’s import the necessary libraries and load the Iris dataset:\n\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\nNext, let’s perform K-means clustering on the original dataset without scaling the features:\n\n# Apply K-means clustering without scaling\nkmeans_unscaled = KMeans(n_clusters=2, random_state=42)\nkmeans_unscaled.fit(X)\n\n# Get the cluster centers and labels\ncentroids_unscaled = kmeans_unscaled.cluster_centers_\nlabels_unscaled = kmeans_unscaled.labels_\n\n\nfrom sklearn.metrics import silhouette_score\n\n\nsilhouette_avg = silhouette_score(X, kmeans_unscaled.labels_)\nprint('Silhouette Average (Unscaled): ', silhouette_avg)\n\nSilhouette Average (Unscaled):  0.6810461692117462\n\n\nThe interpretation of the silhouette score is relatively straightforward:\n\nClose to +1: A silhouette score near +1 indicates that the sample is far away from the neighboring clusters. This is a good indication of a well-clustered data point.\nClose to 0: A silhouette score near 0 indicates that the sample is on or very close to the decision boundary between two neighboring clusters. It could imply that the sample could belong to either one of the clusters.\nClose to -1: A silhouette score near -1 indicates that the samples might have been assigned to the wrong clusters. This could happen if the clusters overlap significantly or if the wrong number of clusters was chosen.\n\nNow, let’s visualize the clusters without scaling:\n\n# Visualize clusters without scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(X[:, 0], X[:, 1], c=labels_unscaled, cmap='viridis', s=50)\nplt.scatter(centroids_unscaled[:, 0], centroids_unscaled[:, 1], marker='x', s=200, c='black')\n\nplt.xlabel('Sepal Length (cm)')\nplt.ylabel('Sepal Width (cm)')\nplt.title('K-means Clustering Without Scaling')\n\nplt.show()\n\n\n\n\n\n\n\n\nYou’ll notice that the clusters may not seem well-separated or meaningful. This is because the features of the Iris dataset have different scales, with sepal length ranging from approximately 4 to 8 cm, while sepal width ranges from approximately 2 to 4.5 cm.\nNow, let’s repeat the process after scaling the features using StandardScaler:\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Scale the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply K-means clustering on scaled features\nkmeans_scaled = KMeans(n_clusters=2, random_state=42)\nkmeans_scaled.fit(X_scaled)\n\n# Get the cluster centers and labels\ncentroids_scaled = kmeans_scaled.cluster_centers_\nlabels_scaled = kmeans_scaled.labels_\n\n\nsilhouette_avg = silhouette_score(X, kmeans_scaled.labels_)\nprint('Silhouette Average (Scaled): ', silhouette_avg)\n\nSilhouette Average (Scaled):  0.6867350732769777\n\n\nVisualize the clusters after scaling:\n\n# Visualize clusters with scaling\nplt.figure(figsize=(10, 6))\n\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_scaled, cmap='viridis', s=50)\nplt.scatter(centroids_scaled[:, 0], centroids_scaled[:, 1], marker='x', s=200, c='black')\n\nplt.xlabel('Sepal Length (scaled)')\nplt.ylabel('Sepal Width (scaled)')\nplt.title('K-means Clustering With Scaling')\n\nplt.show()\n\n\n\n\n\n\n\n\nYou should see clearer and more meaningful clusters after scaling the features, demonstrating the importance of feature scaling for K-means clustering, especially when dealing with datasets with features of different scales.\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Why re-scaling (Iris)?"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/71_feature_selection.html",
    "href": "Python Data Analysis/Week7/71_feature_selection.html",
    "title": "Feature Selection",
    "section": "",
    "text": "Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.\nHaving irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.\nThree benefits of performing feature selection before modeling your data are:\nYou can learn more about feature selection with scikit-learn in the article Feature selection.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom dataidea.datasets import loadDataset\ndata = loadDataset('../assets/demo_cleaned.csv', \n                    inbuilt=False, file_type='csv')\ndata = pd.get_dummies(data, columns=['gender'], \n                      dtype='int', drop_first=True)\ndata.head(n=5)\n\n\n\n\n\n\n\n\n\nage\nmarital_status\naddress\nincome\nincome_category\njob_category\ngender_m\n\n\n\n\n0\n55\n1\n12\n72.0\n3.0\n3\n0\n\n\n1\n56\n0\n29\n153.0\n4.0\n3\n1\n\n\n2\n24\n1\n4\n26.0\n2.0\n1\n1\n\n\n3\n45\n0\n9\n76.0\n4.0\n2\n1\n\n\n4\n44\n1\n17\n144.0\n4.0\n3\n1",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Feature Selection"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/71_feature_selection.html#univariate-selection",
    "href": "Python Data Analysis/Week7/71_feature_selection.html#univariate-selection",
    "title": "Feature Selection",
    "section": "Univariate Selection",
    "text": "Univariate Selection\nStatistical tests can be used to select those features that have the strongest relationship with the output variable.\nThe scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\nMany different statistical test scan be used with this selection method. For example the ANOVA F-value method is appropriate for numerical inputs and categorical data. This can be used via the f_classif() function. We will select the 4 best features using this method in the example below.\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import f_regression\nfrom sklearn.feature_selection import chi2\n\n\nX = data.drop('marital_status', axis=1)\ny = data.marital_status\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\n\nX_train_numeric = X_train[['age', 'income', 'address']].copy()\n\n\ntest = SelectKBest(score_func=f_classif, k=2)\nfit = test.fit(X_train_numeric, y_train)\nscores = fit.scores_\nfeatures = fit.transform(X_train_numeric)\nselected_indices = test.get_support(indices=True)\n\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\n\nFeature Scores:  [3.73495613 0.40565654 0.50368697]\nSelected Features Indices:  [0 2]\n\n\n\nX = data[['age', 'address']].copy()\ny = data.income\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\n\ntest = SelectKBest(score_func=f_regression, k=1) # Select top 5 features, adjust k as needed\n\n# Fit the selector to the data\nfit = test.fit(X_train_numeric, y_train)\n\n# get scores\ntest_scores = fit.scores_\n\n# summarize selected features\nfeatures = fit.transform(X_train_numeric)\n\n# Get the selected feature indices\nselected_indices = fit.get_support(indices=True)\n\nprint('Feature Scores: ', test_scores)\nprint('Selected Features Indices: ', selected_indices)\n\nFeature Scores:  [0.00660376 0.0464015  2.0207761 ]\nSelected Features Indices:  [2]\n\n\n\nX = data[['gender_m', 'income_category', 'job_category']].copy()\ny = data.marital_status\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n\n\ntest = SelectKBest(score_func = chi2, k=2)\nfit = test.fit(X_train, y_train)\nscores = fit.scores_\nfeatures = fit.transform(X_train)\nselected_indices = fit.get_support(indices=True)\n\nprint('Feature Scores: ', test_scores)\nprint('Selected Features Indices: ', selected_indices)\n\nFeature Scores:  [0.00660376 0.0464015  2.0207761 ]\nSelected Features Indices:  [0 1]",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Feature Selection"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/71_feature_selection.html#recursive-feature-elimination",
    "href": "Python Data Analysis/Week7/71_feature_selection.html#recursive-feature-elimination",
    "title": "Feature Selection",
    "section": "Recursive Feature Elimination",
    "text": "Recursive Feature Elimination\nThe Recursive Feature Elimination (or RFE) works by recursively removing attributes and building a model on those attributes that remain.\nIt uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\nYou can learn more about the RFE class in the scikit-learn documentation.\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n\n# feature extraction\nmodel = LogisticRegression()\nrfe = RFE(model)\nfit = rfe.fit(X, y)\nprint(\"Num Features: %d\" % fit.n_features_)\nprint(\"Selected Features: %s\" % fit.support_)\nprint(\"Feature Ranking: %s\" % fit.ranking_)\n\nNum Features: 1\nSelected Features: [False  True False]\nFeature Ranking: [3 1 2]",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Feature Selection"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/71_feature_selection.html#feature-importance",
    "href": "Python Data Analysis/Week7/71_feature_selection.html#feature-importance",
    "title": "Feature Selection",
    "section": "Feature Importance",
    "text": "Feature Importance\nBagged decision trees like Random Forest and Extra Trees can be used to estimate the importance of features.\nIn the example below we construct a ExtraTreesClassifier classifier for the Pima Indians onset of diabetes dataset. You can learn more about the ExtraTreesClassifier class in the scikit-learn API.\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# feature extraction\nmodel = ExtraTreesClassifier(n_estimators=100)\nmodel.fit(X, y)\nprint(model.feature_importances_)\n\n[0.10237303 0.52467525 0.37295172]\n\n\n\nrfc = RandomForestClassifier()\n\n\nrfc.fit(X_train, y_train)\nrfc.score(X_test, y_test)\n\n0.46938775510204084\n\n\n\nX.head(n=3)\n\n\n\n\n\n\n\n\n\ngender_m\nincome_category\njob_category\n\n\n\n\n0\n0\n3.0\n3\n\n\n1\n1\n4.0\n3\n\n\n2\n1\n2.0\n1\n\n\n\n\n\n\n\n\n\nrfc.fit(X_train[['income_category', 'job_category', 'gender_m']], y_train)\nrfc.score(X_test[['income_category',    'job_category', 'gender_m']], y_test)\n\n0.4489795918367347\n\n\n\nf_classif is most applicable where the input features are continuous and the outcome is categorical.\nf_regression is most applicable where the input features are continuous and the outcome is continuous.\nchi2 is best for when the both the input and outcome are categorical.",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "Feature Selection"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week7/75_bonus.html",
    "href": "Python Data Analysis/Week7/75_bonus.html",
    "title": "ANOVA for Feature Selection",
    "section": "",
    "text": "# !pip install dataidea==0.2.5\n\n\nimport scipy as sp\nfrom sklearn.feature_selection import SelectKBest\nfrom dataidea.datasets import loadDataset\n\n\nfpl = loadDataset('fpl') # load fpl inbuilt\n\n\nfpl.head(n=5) # select top 5\n\n\n\n\n\n\n\n\n\nFirst_Name\nSecond_Name\nClub\nGoals_Scored\nAssists\nTotal_Points\nMinutes\nSaves\nGoals_Conceded\nCreativity\nInfluence\nThreat\nBonus\nBPS\nICT_Index\nClean_Sheets\nRed_Cards\nYellow_Cards\nPosition\n\n\n\n\n0\nBruno\nFernandes\nMUN\n18\n14\n244\n3101\n0\n36\n1414.9\n1292.6\n1253\n36\n870\n396.2\n13\n0\n6\nMID\n\n\n1\nHarry\nKane\nTOT\n23\n14\n242\n3083\n0\n39\n659.1\n1318.2\n1585\n40\n880\n355.9\n12\n0\n1\nFWD\n\n\n2\nMohamed\nSalah\nLIV\n22\n6\n231\n3077\n0\n41\n825.7\n1056.0\n1980\n21\n657\n385.8\n11\n0\n0\nMID\n\n\n3\nHeung-Min\nSon\nTOT\n17\n11\n228\n3119\n0\n36\n1049.9\n1052.2\n1046\n26\n777\n315.2\n13\n0\n0\nMID\n\n\n4\nPatrick\nBamford\nLEE\n17\n11\n194\n3052\n0\n50\n371.0\n867.2\n1512\n26\n631\n274.6\n10\n0\n3\nFWD\n\n\n\n\n\n\n\n\n\n# Create groups of goals scored for each player position\n\nforwards_goals = fpl[fpl.Position == 'FWD']['Goals_Scored']\nmidfielders_goals = fpl[fpl.Position == 'MID']['Goals_Scored']\ndefenders_goals = fpl[fpl.Position == 'DEF']['Goals_Scored']\ngoalkeepers_goals = fpl[fpl.Position == 'GK']['Goals_Scored']\n\n\n# Perform the ANOVA test for the groups\n\nf_statistic, p_value = sp.stats.f_oneway(forwards_goals, midfielders_goals,\n                                         defenders_goals, goalkeepers_goals\n                                        )\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\n\nF-statistic: 33.281034594400445\np-value: 3.9257634156019246e-20\n\n\n\n# Create groups of assists for each player position\n\nforwards_assists = fpl[fpl.Position == 'FWD']['Assists']\nmidfielders_assists = fpl[fpl.Position == 'MID']['Assists']\ndefenders_assists = fpl[fpl.Position == 'DEF']['Assists']\ngoalkeepers_assists = fpl[fpl.Position == 'GK']['Assists']\n\n\n# Perform the ANOVA test for the groups\n\nf_statistic, p_value = sp.stats.f_oneway(forwards_assists, midfielders_assists,\n                                         defenders_assists, goalkeepers_assists\n                                        )\nprint(\"F-statistic:\", f_statistic)\nprint(\"p-value:\", p_value)\n\nF-statistic: 19.263717036430815\np-value: 5.124889288362087e-12\n\n\n\n# Use scikit-learn's SelectKBest (with f_classif)\n\ntest = SelectKBest(k=1)\nfit = test.fit(fpl[['Goals_Scored', 'Assists']], fpl.Position)\nscores = fit.scores_\nfeatures = fit.transform(fpl[['Goals_Scored', 'Assists']])\nselected_indices = test.get_support(indices=True)\n\nprint('Feature Scores: ', scores)\nprint('Selected Features Indices: ', selected_indices)\n\nFeature Scores:  [33.28103459 19.26371704]\nSelected Features Indices:  [0]\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week7",
      "ANOVA for Feature Selection"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week4/44_data_exploration_and_cleaning_exercise.html",
    "href": "Python Data Analysis/Week4/44_data_exploration_and_cleaning_exercise.html",
    "title": "Data Exploration and Cleaning Exercise",
    "section": "",
    "text": "Load demo.xlsx dataset\nRename the columns as suggested below\n\n\n\nOld name\nNew name\n\n\n\n\nAge\nage\n\n\nGender\ngender\n\n\nMarital Status\nmarital_status\n\n\nAddress\naddress\n\n\nIncome\nincome\n\n\nIncome Category\nincome_category\n\n\nJob Category\njob_category\n\n\n\nDisplay all the columns in the dataset\nDisplay some basic statistics about the numeric variables in the dataset\nDisplay some basic statistics about the categorical variables in the dataset\nWhat are the unique observations under gender?\nCan you fix any problems observed under the gender, give brief explanations why and how\nHow many observations have ‘no answer’ for marital status?\nWrite some piece of code to return only numeric variables from the dataset\nAre there any missing values in the dataset?\nAre there any outliers in the income variable?\nInvestigate the relationship between age and income\nHow many people earn more than 300 units?\nWhat data type is the marital status?\nCreate dummy variables for gender\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week4",
      "Data Exploration and Cleaning Exercise"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week4/42_training_models_meaning.html",
    "href": "Python Data Analysis/Week4/42_training_models_meaning.html",
    "title": "Training a model: Meaning",
    "section": "",
    "text": "For one independent variable (feature)\n\nUnderstanding Linear Regression:\n\nLinear regression is a statistical method used to model the relationship between a dependent variable (often denoted as ( y )) and one or more independent variables (often denoted as ( x )).\nThe relationship is modeled as a straight line equation: ( y = mx + b ), where ( m ) is the slope of the line and ( b ) is the y-intercept.\n\nFitting a Model:\n\nWhen we say we’re “fitting a model” in the context of linear regression, it means we’re determining the best-fitting line (or plane in higher dimensions) that represents the relationship between the independent variable(s) and the dependent variable.\nThis process involves finding the values of ( m ) and ( b ) that minimize the difference between the actual observed values of the dependent variable and the values predicted by the model.\nIn simpler terms, fitting a model means finding the line that best describes the relationship between our input data and the output we want to predict.\n\nPython Code Example:\n\nHere’s a simple example of how you might fit a linear regression model using Python, particularly with the scikit-learn library:\n\n\n\n# Importing necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\n\n\n# Creating sample data\nX = np.array([[1], [2], [3], [4], [5]])  # Independent variable (feature)\ny = np.array([2, 4, 5, 4, 5])             # Dependent variable (target)\n\n\n# Creating a linear regression model\nlinear_regression_model = LinearRegression()\n\n# Fitting the linear_regression_model to our data\nlinear_regression_model.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n# Printing the slope (coefficient) and intercept of the best-fitting line\nprint(\"Slope (m):\", linear_regression_model.coef_[0])\nprint(\"Intercept (b):\", linear_regression_model.intercept_)\n\nSlope (m): 0.6\nIntercept (b): 2.2\n\n\n\n# Predictions\ny_predicted = linear_regression_model.predict(X)\n\n\ny_predicted\n\narray([2.8, 3.4, 4. , 4.6, 5.2])\n\n\n\nIn this code:\n\nX represents the independent variable (feature), which is a column vector in this case.\ny represents the dependent variable (target).\nWe create a LinearRegression model object.\nWe fit the model to our data using the .fit() method.\nFinally, we print out the slope (coefficient) and intercept of the best-fitting line.\n\n\n\n# Plotting the linear regression line\nplt.scatter(X, y, color='blue', label='Data Points')\nplt.plot(X, y_predicted, color='red', label='Linear Regression Line')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.title('Linear Regression')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSo, in summary, “fitting a model” means finding the best parameters (like slope and intercept in the case of linear regression) that describe the relationship between our input data and the output we want to predict.\n\n\nFor Multiple Independent Variables\nIf we have multiple independent variables (features) in ( X ), the process is still the same, but the equation becomes more complex. This is known as multiple linear regression.\nHere’s how it works:\n\nUnderstanding Multiple Linear Regression:\n\nInstead of a single independent variable ( x ), we have multiple independent variables represented as a matrix ( X ).\nThe relationship between the dependent variable ( y ) and the independent variables ( X ) is modeled as: [ y = b_0 + b_1x_1 + b_2x_2 + … + b_nx_n ] where ( b_0 ) is the intercept, ( b_1, b_2, …, b_n ) are the coefficients corresponding to each independent variable ( x_1, x_2, …, x_n ).\n\nFitting a Model with Many Variables:\n\nFitting the model involves finding the values of the coefficients ( b_0, b_1, …, b_n ) that minimize the difference between the actual observed values of the dependent variable and the values predicted by the model.\nThe process is essentially the same as in simple linear regression, but with more coefficients to estimate.\n\nPython Code Example:\n\nHere’s how you might fit a multiple linear regression model using Python:\n\n\n\n# Creating sample data with multiple variables\nX = np.array([[1, 2], [2, 4], [3, 6], [4, 8], [5, 10]])  # Independent variables (features)\ny = np.array([2, 4, 5, 4, 5])                            # Dependent variable (target)\n\n\n# Creating a multiple linear regression model\nmultiple_linear_regression_model = LinearRegression()\n\n# Fitting the multiple_linear_regression_model to our data\nmultiple_linear_regression_model.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \n\n\n\n# Printing the coefficients (intercept and slopes) of the best-fitting line\nprint(\"Intercept (b0):\", multiple_linear_regression_model.intercept_)\nprint(\"Coefficients (b1, b2):\", multiple_linear_regression_model.coef_)\n\nIntercept (b0): 2.200000000000001\nCoefficients (b1, b2): [0.12 0.24]\n\n\n\nIn this code:\n\nX represents the independent variables (features), where each row is a data point and each column represents a different feature.\ny represents the dependent variable (target).\nWe create a LinearRegression model object.\nWe fit the model to our data using the .fit() method.\nFinally, we print out the intercept and coefficients of the best-fitting line.\n\n\nSo, fitting a model with many variables involves finding the best parameters (coefficients) that describe the relationship between our input data (multiple independent variables) and the output we want to predict.\n\n\nWhat about Logistic Regression?\nLogistic regression is a type of regression analysis used for predicting the probability of a binary outcome based on one or more predictor variables. Here’s how the fitting process works with logistic regression:\n\nUnderstanding Logistic Regression:\n\nLogistic regression models the probability that a given input belongs to a particular category (binary classification problem).\nInstead of fitting a straight line or plane like in linear regression, logistic regression uses the logistic function (also known as the sigmoid function) to model the relationship between the independent variables and the probability of the binary outcome.\nThe logistic function is defined as:\n\n$ P(y=1 ,|, X) = $ where $ P(y=1 ,|, X) $ is the probability of the positive outcome given the input $ X $, $ b_0 $ is the intercept, $ b_1, b_2, …, b_n $ are the coefficients, and $ x_1, x_2, …, x_n $ are the independent variables.\n\n\n# z = b_0 + b_1x_1 + ... + b_nx_n\nsigmoid = lambda z: 1 / (1 + np.exp(-z))\n\n\nFitting a Logistic Regression Model:\n\nFitting the logistic regression model involves finding the values of the coefficients $ b_0, b_1, …, b_n $ that maximize the likelihood of observing the given data under the assumed logistic regression model.\nThis is typically done using optimization techniques such as gradient descent or other optimization algorithms.\nThe goal is to find the set of coefficients that best separates the two classes or minimizes the error between the predicted probabilities and the actual binary outcomes in the training data.\n\nPython Code Example:\n\nHere’s how you might fit a logistic regression model using Python with the scikit-learn library:\n\n\n\n# import the logistic regression model\nfrom sklearn.linear_model import LogisticRegression\n\n\n# Creating sample data\nX = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9]])  # Independent variable\ny = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1])             # Binary outcome (0 or 1)\n\n\n# Creating a logistic regression model\nlogistic_regression_model = LogisticRegression()\n\n# Fitting the model to our data\nlogistic_regression_model.fit(X, y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \n\n\n\n# Printing the intercept and coefficient(s) of the best-fitting logistic curve\nprint(\"Intercept (b0):\", logistic_regression_model.intercept_)\nprint(\"Coefficient (b1):\", logistic_regression_model.coef_)\n\nIntercept (b0): [-5.29559243]\nCoefficient (b1): [[1.17808562]]\n\n\n\nIn this code:\n\nX represents the independent variable.\ny represents the binary outcome.\nWe create a LogisticRegression model object.\nWe fit the model to our data using the .fit() method.\nFinally, we print out the intercept and coefficient(s) of the best-fitting logistic curve.\n\n\n\n# Predicted probabilities\nprobabilities = logistic_regression_model.predict_proba(X)\n\narray([0.01602411, 0.05023888, 0.14662315, 0.35818512, 0.6444739 ,\n       0.85482058, 0.95031106, 0.98415754, 0.99506855])\n\n\n\nlogistic_regression_model.predict_proba(X)[:, 1]\n\narray([0.01602411, 0.05023888, 0.14662315, 0.35818512, 0.6444739 ,\n       0.85482058, 0.95031106, 0.98415754, 0.99506855])\n\n\n\n# Plotting the logistic regression curve\nplt.scatter(X, y, color='blue', label='Data Points')\nplt.plot(X, probabilities, color='red', label='Logistic Regression Curve')\nplt.xlabel('X')\nplt.ylabel('Probability')\nplt.title('Logistic Regression')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nSo, fitting a logistic regression model involves finding the best parameters (coefficients) that describe the relationship between our input data and the probability of the binary outcome.\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week4",
      "Training a model: Meaning"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/11_python_tutorial.html",
    "href": "Python Data Analysis/Week1/11_python_tutorial.html",
    "title": "Python Quick Review",
    "section": "",
    "text": "Python is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing.\nI expect that many of you will have some experience with Python; for the rest of you, this section will serve as a quick crash course both on the Python programming language and on the use of Python for scientific computing.\nIn this tutorial, we will cover:\n\nBasic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Python Quick Review"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/11_python_tutorial.html#introduction",
    "href": "Python Data Analysis/Week1/11_python_tutorial.html#introduction",
    "title": "Python Quick Review",
    "section": "",
    "text": "Python is a great general-purpose programming language on its own, but with the help of a few popular libraries (numpy, scipy, matplotlib) it becomes a powerful environment for scientific computing.\nI expect that many of you will have some experience with Python; for the rest of you, this section will serve as a quick crash course both on the Python programming language and on the use of Python for scientific computing.\nIn this tutorial, we will cover:\n\nBasic Python: Basic data types (Containers, Lists, Dictionaries, Sets, Tuples), Functions, Classes",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Python Quick Review"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/11_python_tutorial.html#a-brief-note-on-python-versions",
    "href": "Python Data Analysis/Week1/11_python_tutorial.html#a-brief-note-on-python-versions",
    "title": "Python Quick Review",
    "section": "A Brief Note on Python Versions",
    "text": "A Brief Note on Python Versions\nAs of Janurary 1, 2024, Python has officially dropped support for python2. We’ll be using Python 3.10 for this iteration of the course. You can check your Python version at the command line by running python --version.\n\n# checking python version\n!python --version\n\nPython 3.10.12",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Python Quick Review"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/11_python_tutorial.html#basics-of-python",
    "href": "Python Data Analysis/Week1/11_python_tutorial.html#basics-of-python",
    "title": "Python Quick Review",
    "section": "Basics of Python",
    "text": "Basics of Python\nPython is a high-level, dynamically typed multiparadigm programming language. Python code is often said to be almost like pseudocode, since it allows you to express very powerful ideas in very few lines of code while being very readable. As an example, here is an implementation of the classic quicksort algorithm in Python:\n\ndef quicksort(arr):\n    if len(arr) &lt;= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x &lt; pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x &gt; pivot]\n    return quicksort(left) + middle + quicksort(right)\n\nquicksort([3,6,8,10,1,2,1])\n\n[1, 1, 2, 3, 6, 8, 10]\n\n\n\nVariables\nVariables are stores of value\n\nname = 'Juma'\nage = 19\nid_number = 190045\n\n\nRules to consider\n\nVariable names should be meaningful eg number instead of x\nVariable names should contain only alpha-numberic characters, and maybe under_scores\nVariable names can only start with letters or an underscore\nVariable name cannot contain special characters\nVariables names are case sensitive\n\n\nname = 'Eva'\nlist_of_names = ['Eva', 'Shafara', 'Bob'] # snake case\n\n\ndef calculateBMI(weight_kg, height_m): # camel case\n    bmi = weight_kg / height_m ** 2\n    rounded_bmi = round(bmi, 3)\n    return rounded_bmi\n\n\ncalculateBMI(59, 1.63)\n\n22.206\n\n\n\nclass MathFunction:    # Pascal Case\n    def __init__(self, number):\n        self.number = number\n\n    def square(self):\n        return self.number ** 2\n\n    def cube(self):\n        return self.number ** 3\n\n\nmath = MathFunction(number=7)\n\nmath.square()\n\n49\n\n\n\n\n\nBasic data types\n\nNumbers\nIntegers and floats work as you would expect from other languages:\n\nnumber = 3\nprint('Number: ', number)\nprint('Type: ', type(number))\n\nNumber:  3\nType:  &lt;class 'int'&gt;\n\n\n\n# Quick number arithmetics\n\nprint(number + 1)   # Addition\nprint(number - 1)   # Subtraction\nprint(number * 2)   # Multiplication\nprint(number ** 2)  # Enumberponentiation\n\n4\n2\n6\n9\n\n\n\n# Some compound assingment operators\n\nnumber += 1 # number = number + 1\nprint(number)\nnumber *= 2\nprint(number)\nnumber /= 1 # number = number / 1\nprint(number)\nnumber -= 2\nprint(number)\n\n4\n8\n8.0\n6.0\n\n\n\nnumber = 2.5\nprint(type(number))\nprint(number, number + 1, number * 2, number ** 2)\n\n&lt;class 'float'&gt;\n2.5 3.5 5.0 6.25\n\n\n\n# complex numbers\nvector = 2 + 6j\ntype(vector)\n\ncomplex\n\n\n\n\nBooleans\nPython implements all of the usual operators for Boolean logic, but uses English words rather than symbols:\n\nt, f = True, False\ntype(t)\n\nbool\n\n\nNow we let’s look at the operations:\n\n# Logical Operators\n\nprint(t and f) # Logical AND;\nprint(t or f)  # Logical OR;\nprint(not t)   # Logical NOT;\nprint(t != f)  # Logical XOR;\n\nFalse\nTrue\nFalse\nTrue\n\n\n\n\nStrings\nA string is a sequence of characters under some quotes. Eg.\n\nhello = 'hello'   # String literals can use single quotes\nworld = \"world\"   # or double quotes; it does not matter\nprint(hello, len(hello))\n\nhello 5\n\n\n\n# We can string in python\nfull = hello + ' ' + world  # String concatenation\nprint(full)\n\nhello world\n\n\n\nhw12 = '{} {} {}'.format(hello, world, 12)  # string formatting\nprint(hw12)\n\nhello world 12\n\n\n\nstatement = 'I love to code in {}'\nmodified = statement.format('JavaScript')\nprint(modified)\n\nI love to code in JavaScript\n\n\n\n# formatting by indexing\nstatement = '{0} loves to code in {2} and {1}'\nstatement.format('Juma', 'Python', 'JavaScript')\n\n'Juma loves to code in JavaScript and Python'\n\n\n\n# formatting by name\nstatement = '{name} loves to code in {language1} and {language2}'\nstatement.format(language2='Python', name='Juma', language1='JavaScript')\n\n'Juma loves to code in JavaScript and Python'\n\n\n\n# String Literal Interpolation\nname = 'Juma'\nlanguage1 = 'JavaScript'\nlanguage2 = 'Python'\n\nstatement = f'{name} loves to code in {language1} and {language2}'\n\nprint(statement)\n\nJuma loves to code in JavaScript and Python\n\n\nString objects have a bunch of useful methods; for example:\n\nstring_ = \"hello\"\nprint(string_.capitalize())  # Capitalize a string\nprint(string_.upper())       # Convert a string to uppercase; prints \"HELLO\"\nprint(string_.rjust(7))      # Right-justify a string, padding with spaces\nprint(string_.center(7))     # Center a string, padding with spaces\nprint(string_.replace('l', '(ell)'))  # Replace all instances of one substring with another\nprint('  world '.strip())  # Strip leading and trailing whitespace\n\nHello\nHELLO\n  hello\n hello \nhe(ell)(ell)o\nworld\n\n\n\nstatement = 'i love to code in Python '\n\ncapitalized = statement.capitalize()\nupped = statement.upper()\nreplaced = statement.replace('Python', 'javascript')\nstatement.strip()\n\n'i love to code in Python'\n\n\nYou can find a list of all string methods in the documentation.\n\n\n\nContainers\n\nPython containers (collections) are objects that we use to group other objects\nPython includes several built-in container types: lists, dictionaries, sets, and tuples.\n\n\nLists\nA list is an ordered collection of python objects or elements. A list can contain objects of different data types\n\nlist_of_numbers = [3, 1, 2]   # Create a list\nprint(list_of_numbers)\nprint(list_of_numbers[2])\nprint(list_of_numbers[-1])     # Negative indices count from the end of the list; prints \"2\"\n\n[3, 1, 2]\n2\n2\n\n\n\nlist_of_numbers[2] = 'foo'    # replacing a specific value in a list\nprint(list_of_numbers)\n\n[3, 1, 'foo']\n\n\n\nlist_of_numbers.append('bar') # Add a new element to the end of the list\nprint(list_of_numbers)\n\n[3, 1, 'foo', 'bar']\n\n\n\nlast_item = list_of_numbers.pop()     # Remove and return the last element of the list\nprint(last_item)    # returns the last item \nprint(list_of_numbers) # Modifies the original list\n\nbar\n[3, 1, 'foo']\n\n\nResearch on: - del - remove()\nAs usual, you can find all the gory details about lists in the documentation.\n\n\nSlicing\nIn addition to accessing list elements one at a time, Python provides concise syntax to access a range of values in a list; this is known as slicing:\n\nlist_of_numbers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint(list_of_numbers)\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nprint(list_of_numbers)         # Prints \"[0, 1, 2, 3, 4]\"\nprint(list_of_numbers[2:4])    # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\"\nprint(list_of_numbers[2:])     # Get a slice from index 2 to the end; prints \"[2, 3, 4]\"\nprint(list_of_numbers[:2])     # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\"\nprint(list_of_numbers[:])      # Get a slice of the whole list; prints [\"0, 1, 2, 3, 4]\"\nprint(list_of_numbers[:-1])    # Slice indices can be negative; prints [\"0, 1, 2, 3]\"\nlist_of_numbers[2:4] = [8, 9] # Assign a new sublist to a slice\nprint(list_of_numbers)         # Prints \"[0, 1, 8, 9, 4]\"\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[2, 3]\n[2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1]\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n[0, 1, 2, 3, 4, 5, 6, 7, 8]\n[0, 1, 8, 9, 4, 5, 6, 7, 8, 9]\n\n\n\n\nLoops\nA for loop is used to loop through (or iterate) over a sequence of objects (iterable objects). Iterable objects in python include strings, lists, sets etc\nYou can loop over the elements of a list like this:\n\nlist_of_animals = ['cat', 'dog', 'monkey']\n\nfor animal in list_of_animals:\n    print(animal)\n\ncat\ndog\nmonkey\n\n\n\nlist_of_numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]\nlist_of_squared_numbers = []\n\nfor number in list_of_numbers:\n    list_of_squared_numbers.append(pow(number, 2))\n\nlist_of_squared_numbers\n\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 0]\n\n\nIf you want access to the index of each element within the body of a loop, use the built-in enumerate function:\n\nanimals = ['cat', 'dog', 'monkey']\n\nfor index, animal in enumerate(animals):\n    print(f'{index}: {animal}')\n\n0: cat\n1: dog\n2: monkey\n\n\n\n\nList comprehensions:\n\nnumbers = [0, 1, 2, 3, 4]\nsquares = []\n\nfor number in numbers:\n    squares.append(pow(number, 2))\n\nprint(squares)\n\n[0, 1, 4, 9, 16]\n\n\nYou can make this code simpler using a list comprehension:\n\nlist_of_numbers = [0, 1, 2, 3, 4]\n\nsquares = [pow(number, 2) for number in list_of_numbers]\n\nprint(squares)\n\n[0, 1, 4, 9, 16]\n\n\nList comprehensions can also contain conditions:\n\nnumbers = [0, 1, 2, 3, 4]\n\neven_squares = [pow(number, 2) for number in numbers if number % 2 == 0]\n\nprint(even_squares)\n\n[0, 4, 16]\n\n\nResearch: - How to combine lists\n\n\nDictionaries\n\nA dictionary is an unordered and mutable collection of items\nA dictionary is created using curly brackets\nEach item in a dictionary contains a key/value pair\n\n\n# creating a dictionary\nperson = {\n    'first_name': 'Juma',\n    'last_name': 'Shafara',\n    'age': 51,\n    'married': True\n}\nperson\n\n{'first_name': 'Juma', 'last_name': 'Shafara', 'age': 51, 'married': True}\n\n\n\n# accessing items in a dictionary\nfirst_name = person['first_name']\nlast_name = person['last_name']\nfull_name = first_name + ' ' + last_name\n\n# display\nfull_name\n\n'Juma Shafara'\n\n\n\n# add items to a dictionary\nperson['hobby'] = 'Coding'\nperson\n\n{'first_name': 'Juma',\n 'last_name': 'Shafara',\n 'age': 51,\n 'married': True,\n 'hobby': 'Coding'}\n\n\n\nemail = person.get('email', 'email not available')\nprint(email)\n\nemail not available\n\n\n\n# modifying a value in a dictionay\nperson['married'] = False\nperson\n\n{'first_name': 'Juma',\n 'last_name': 'Shafara',\n 'age': 51,\n 'married': False,\n 'hobby': 'Coding'}\n\n\n\n# remove an item from a dictionary\nperson.pop('age')\nperson\n\n{'first_name': 'Juma',\n 'last_name': 'Shafara',\n 'married': False,\n 'hobby': 'Coding'}\n\n\nResearch: - How to remove an item using the del method - How to iterate over objects in a dictionary - Imitate list comprehension with dictionaries\nYou can find all you need to know about dictionaries in the documentation.\n\n\nSets\n\nA set is an unordered, immutable collection of distinct elements.\nA set is created using curly braces\nThe objects are placed inside the brackets and are separated by commas\nAs a simple example, consider the following:\n\n\nanimals = {'cat', 'dog'}\n\nprint('cat' in animals)   # Check if an element is in a set; prints \"True\"\nprint('fish' not in animals)  # prints \"True\"\n\nTrue\nTrue\n\n\n\nanimals.add('fish')      # Add an element to a set\n\nprint('fish' in animals) # Returns \"True\"\n\nprint(len(animals))       # Number of elements in a set;\n\nTrue\n3\n\n\n\nanimals.add('cat')       # Adding an element that is already in the set does nothing\nprint(len(animals)) \n      \nanimals.remove('cat')    # Remove an element from a set\nprint(len(animals))\n\n3\n2\n\n\nResearch: - How to remove with discard() - How to remove with pop() - How to combine sets - How to get the difference between 2 sets - What happens when we have repeated elements in a set\nLoops: Iterating over a set has the same syntax as iterating over a list; however since sets are unordered, you cannot make assumptions about the order in which you visit the elements of the set:\n\nanimals = {'cat', 'dog', 'fish'}\n\nfor index, animal in enumerate(animals):\n    print(f'{index}: {animal}')\n\n0: fish\n1: cat\n2: dog\n\n\nSet comprehensions: Like lists and dictionaries, we can easily construct sets using set comprehensions:\n\nfrom math import sqrt\n\nprint({int(sqrt(x)) for x in range(30)})\n\n{0, 1, 2, 3, 4, 5}\n\n\n\n\nTuples\n\nA tuple is an (immutable) ordered list of values.\nA tuple is in many ways similar to a list; one of the most important differences is that tuples can be used as keys in dictionaries and as elements of sets, while lists cannot. Here is a trivial example:\n\n\nd = {(x, x + 1): x for x in range(10)}  # Create a dictionary with tuple keys\nt = (5, 6)       # Create a tuple\nprint(type(t))\nprint(d[t])       \nprint(d[(1, 2)])\n\n&lt;class 'tuple'&gt;\n5\n1\n\n\n\n# t[0] = 1\n\nResearch: - Creating a tuple - Access items in a tuple - Negative indexing tuples - Using range of indexes - Getting the length of items in a tuple - Looping through a tuple - Checking if an item exists in a tuple - How to combine tuples - Prove that tuples are immutable\n\n\n\nFunctions\n\nA function is a group of statements that performs a particular task\nPython functions are defined using the def keyword. For example:\n\n\ndef overWeightOrUnderweightOrNormal(weight_kg:float, height_m:float) -&gt; str:\n    '''\n    Tells whether someone is overweight or underweight or normal\n    '''\n    height_m2 = pow(height_m, 2)\n    bmi = weight_kg / height_m2\n    rounded_bmi = round(bmi, 3)\n    if bmi &gt; 24:\n        return 'Overweight'\n    elif bmi &gt; 18:\n        return 'Normal'\n    else:\n        return 'Underweight'\n\noverWeightOrUnderweightOrNormal(67, 1.7)\n\n'Normal'\n\n\nWe will often use functions with optional keyword arguments, like this:\n\nbmi = calculateBMI(height_m=1.7, weight_kg=67)\n\nprint(bmi)\n\n23.183\n\n\n\ndef greet(name:str='You')-&gt;str:\n    \"\"\"\n    This function greets people by name\n    Example1:\n    &gt;&gt;&gt; greet(name='John Doe')\n    &gt;&gt;&gt; 'Hello John Doe'\n    Example2:\n    &gt;&gt;&gt; greet()\n    &gt;&gt;&gt; 'Hello You'\n    \"\"\"\n    return f'Hello {name}'\n\n# greet('Eva')\n?greet\n\n\nSignature: greet(name: str = 'You') -&gt; str\nDocstring:\nThis function greets people by name\nExample1:\n&gt;&gt;&gt; greet(name='John Doe')\n&gt;&gt;&gt; 'Hello John Doe'\nExample2:\n&gt;&gt;&gt; greet()\n&gt;&gt;&gt; 'Hello You'\nFile:      /tmp/ipykernel_66386/2049930273.py\nType:      function\n\n\n\n\n\nClasses\n\nIn python, everything is an object\nWe use classes to help us create new object\nThe syntax for defining classes in Python is straightforward:\n\n\nclass Person:\n    first_name = 'John'\n    last_name = 'Tong'\n    age = 20\n\n\n# Instantiating a class\nobject1 = Person()\n\nprint(object1.first_name)\nprint(object1.last_name)\nprint(object1.age)\n\nprint(f'object1 type: {type(object1)}')\n\nJohn\nTong\n20\nobject1 type: &lt;class '__main__.Person'&gt;\n\n\n\n# Instantiating a class\nobject2 = Person()\n\nprint(object2.first_name)\nprint(object2.last_name)\nprint(object2.age)\n\nJohn\nTong\n20\n\n\n\nclass Person:\n    def __init__(self, first_name, last_name, age):\n        self.first_name = first_name\n        self.last_name = last_name\n        self.age = age\n\n    def greet(self, name):\n        return f'Hello {name}'\n\n\nobject1 = Person('Juma', 'Shafara', 24)\nprint(object1.first_name)\nprint(object1.last_name)\nprint(object1.age)\nprint(type(object1))\n\nJuma\nShafara\n24\n&lt;class '__main__.Person'&gt;\n\n\n\nobject2 = Person('Eva', 'Ssozi', 24)\nprint(object2.first_name)\nprint(object2.last_name)\nprint(object2.age)\nprint(object2.greet('Shafara'))\nprint(type(object2))\n\nEva\nSsozi\n24\nHello Shafara\n&lt;class '__main__.Person'&gt;\n\n\n\nclass Student(Person):\n    def __init__(self, first_name, last_name, age, id_number, subjects=[]):\n        super().__init__(first_name, last_name, age)\n        self.id_number = id_number\n        self.subjects = subjects\n\n    def addSubject(self, subject):\n        self.subjects.append(subject)\n\n\nstudent1 = Student('Calvin', 'Masaba', 34, '200045', ['math', 'science'])\n\n\nstudent1.addSubject('english')\n\n\nstudent1.subjects\n\n['math', 'science', 'english']\n\n\nResearch: - Inheritance: This allows to create classes that inherit the attributes and methods of another class",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Python Quick Review"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/12_numpy.html",
    "href": "Python Data Analysis/Week1/12_numpy.html",
    "title": "Numpy",
    "section": "",
    "text": "Numpy is a python package used for scientific computing\nNumpy provides arrays which are greater and faster alternatives to traditional python lists. An array is a group of elements of the same data type\nA standard numpy array is required to have elements of the same data type.\n# Uncomment and run this cell to install numpy\n# !pip install numpy",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Numpy"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/12_numpy.html#array-mathematics",
    "href": "Python Data Analysis/Week1/12_numpy.html#array-mathematics",
    "title": "Numpy",
    "section": "Array mathematics",
    "text": "Array mathematics\n\nArithmetic Operations\n\n# creating arrays\narray1 = np.array([1, 4, 6, 7])\narray2 = np.array([3, 5, 3, 1])\n\n\n# subtract\ndifference1 = array1 - array2\nprint('difference1 =', difference1)\n\n# another way\ndifference2 = np.subtract(array1, array2)\nprint('difference2 =', difference2)\n\ndifference1 = [-2 -1  3  6]\ndifference2 = [-2 -1  3  6]\n\n\n\n# sum\nsummation1 = array1 + array2\nprint('summation1 =', summation1)\n\n# another way\nsummation2 = np.add(array1, array2)\nprint('summation2 =', summation2)\n\nsummation1 = [4 9 9 8]\nsummation2 = [4 9 9 8]\n\n\n\n\nTrigonometric operations\n\n# sin\nprint('sin(array1) =', np.sin(array1))\n# cos\nprint('cos(array1) =', np.cos(array1))\n# log\nprint('log(array1) =', np.log(array1))\n\nsin(array1) = [ 0.84147098 -0.7568025  -0.2794155   0.6569866 ]\ncos(array1) = [ 0.54030231 -0.65364362  0.96017029  0.75390225]\nlog(array1) = [0.         1.38629436 1.79175947 1.94591015]\n\n\n\n# dot product\narray1.dot(array2)\n\n48\n\n\nResearch:\n\nanother way to dot matrices (arrays)\n\n\n\nComparison\n\narray1 == array2\n\narray([False, False, False, False])\n\n\n\narray1 &gt; 3\n\narray([False,  True,  True,  True])\n\n\n\n\nAggregate functions\n\n# average\nmean = array1.mean()\nprint('Mean: ', mean)\n\n# min\nminimum = array1.min()\nprint('Minimum: ', minimum)\n\n# max\nmaximum = array1.max()\nprint('Maximum: ', maximum)\n\n# corrcoef\ncorrelation_coefficient = np.corrcoef(array1, array2)\nprint('Correlation Coefficient: ', correlation_coefficient)\n\nstandard_deviation = np.std(array1)\nprint('Standard Deviation: ', standard_deviation)\n\nMean:  4.5\nMinimum:  1\nMaximum:  7\nCorrelation Coefficient:  [[ 1.         -0.46291005]\n [-0.46291005  1.        ]]\nStandard Deviation:  2.29128784747792\n\n\nResearch:\n\ncopying arrays (you might meet view(), copy())",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Numpy"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/12_numpy.html#subsetting-slicing-and-indexing",
    "href": "Python Data Analysis/Week1/12_numpy.html#subsetting-slicing-and-indexing",
    "title": "Numpy",
    "section": "Subsetting, Slicing and Indexing",
    "text": "Subsetting, Slicing and Indexing\n\nIndexing is the technique we use to access individual elements in an array. 0 represents the first element, 1 the represents second element and so on.\nSlicing is used to access elements of an array using a range of two indexes. The first index is the start of the range while the second index is the end of the range. The indexes are separated by a colon ie [start:end]\n\n\n# Creating numpy arrays of different dimension\n\narr1 = np.array([1, 4, 6, 7]) # 1D array\nprint('Array1 (1D): \\n', arr1)\n\narr2 = np.array([[1.5, 2, 3], [4, 5, 6]]) # 2D array\nprint('Array2 (2D): \\n', arr2)\n\narr3 = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], \n                 [[10, 11, 12], [13, 14, 15], [16, 17, 18]]]) #3D array\nprint('Array3 (3D): \\n', arr3)\n\nArray1 (1D): \n [1 4 6 7]\nArray2 (2D): \n [[1.5 2.  3. ]\n [4.  5.  6. ]]\nArray3 (3D): \n [[[ 1  2  3]\n  [ 4  5  6]\n  [ 7  8  9]]\n\n [[10 11 12]\n  [13 14 15]\n  [16 17 18]]]\n\n\n\n# find the dimensions of an array\narr3.shape\n\n(2, 3, 3)\n\n\n\nIndexing\n\n# accessing items in a 1D array\narr1[2]\n\n6\n\n\n\n# accessing items in 2D array\narr2[0, 2]\n\n3.0\n\n\n\n# accessing in a 3D array\narr3[0, 2, 1]\n\n8\n\n\n\n\nslicing\n\n# slicing 1D array\narr1[0:3]\n\narray([1, 4, 6])\n\n\n\n# slicing a 2D array\narr2[1, 0:2]\n\narray([4., 5.])\n\n\n\n# slicing a 3D array\nfirst = arr3[0, 2]\nsecond = arr3[1, 0]\n\nnp.concatenate((first, second))\n\narray([ 7,  8,  9, 10, 11, 12])\n\n\n\n# boolean indexing\narr1[arr1 &lt; 5]\n\narray([1, 4])\n\n\nResearch:\n\nFancy Indexing",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Numpy"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week1/12_numpy.html#array-manipulation",
    "href": "Python Data Analysis/Week1/12_numpy.html#array-manipulation",
    "title": "Numpy",
    "section": "Array manipulation",
    "text": "Array manipulation\n\nprint(arr2)\n\n[[1.5 2.  3. ]\n [4.  5.  6. ]]\n\n\n\n# transpose\narr2_transpose1 = np.transpose(arr2) \nprint('Transpose1: \\n', arr2_transpose1)\n\n# another way\narr2_transpose2 = arr2.T\nprint('Transpose2: \\n', arr2_transpose2)\n\nTranspose1: \n [[1.5 4. ]\n [2.  5. ]\n [3.  6. ]]\nTranspose2: \n [[1.5 4. ]\n [2.  5. ]\n [3.  6. ]]\n\n\n\n# combining arrays\nfirst = arr3[0, 2]\nsecond = arr3[1, 0]\n\nnp.concatenate((first, second))\n\narray([ 7,  8,  9, 10, 11, 12])\n\n\n\nSome homework\nResearch:\nAdding/Removing Elements - resize() - append() - insert() - delete()\nChanging array shape - ravel() - reshape()\n\n# stacking \n# np.vstack((a,b))\n# np.hstack((a,b))\n# np.column_stack((a,b))\n# np.c_[a, b]\n\n\n# splitting arrays\n# np.hsplit()\n# np.vsplit()",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week1",
      "Numpy"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week5/51_descriptive_statistics.html",
    "href": "Python Data Analysis/Week5/51_descriptive_statistics.html",
    "title": "deepconclusions",
    "section": "",
    "text": "## Uncomment and run this cell to install the packages\n# !pip install pandas numpy statsmodels\nfrom dataidea.packages import *\nThis notebook has been modified to use the Nobel Price Laureates Dataset which you can download from opendatasoft",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week5",
      "Descriptive Statistics and Summary Metrics"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week5/51_descriptive_statistics.html#descriptive-statistics-and-summary-metrics",
    "href": "Python Data Analysis/Week5/51_descriptive_statistics.html#descriptive-statistics-and-summary-metrics",
    "title": "deepconclusions",
    "section": "Descriptive Statistics and Summary Metrics",
    "text": "Descriptive Statistics and Summary Metrics\nDescriptive statistics is a branch of statistics that deals with the presentation and summary of data in a meaningful and informative way. Its primary goal is to describe and summarize the main features of a dataset.\nCommonly used measures in descriptive statistics include:\n\nMeasures of central tendency: These describe the center or average of a dataset and include metrics like mean, median, and mode.\nMeasures of variability: These indicate the spread or dispersion of the data and include metrics like range, variance, and standard deviation.\nMeasures of distribution shape: These describe the distribution of data points and include metrics like skewness and kurtosis.\nMeasures of association: These quantify the relationship between variables and include correlation coefficients.\n\nDescriptive statistics provide simple summaries about the sample and the observations that have been made.\n\n1. Measures of central tendency ie Mean, Median, Mode:\nThe Center of the Data:\nThe center of the data is where most of the values are concentrated.\n\nMean: It is the average value of a dataset calculated by summing all values(numerical) and dividing by the total count.\nMedian: It is the middle value of a dataset when arranged in ascending order. If there is an even number of observations, the median is the average of the two middle values.\nMode: It is the value that appears most frequently in a dataset.\n\n\n# load the dataset (modify the path to point to your copy of the dataset)\ndata = pd.read_csv('../assets/nobel_prize_year.csv')\ndata.sample(n=5)\n\n\n\n\n\n\n\n\n\nYear\nGender\nCategory\nbirth_year\nage\n\n\n\n\n86\n1971\nmale\nMedicine\n1915\n56\n\n\n524\n1978\nmale\nMedicine\n1928\n50\n\n\n840\n1929\nmale\nMedicine\n1858\n71\n\n\n214\n2018\nmale\nChemistry\n1951\n67\n\n\n492\n2012\nmale\nEconomics\n1951\n61\n\n\n\n\n\n\n\n\n\n# get the age column data (optional: convert to numpy array)\nage = np.array(data.age)\n\n\n# Let's get the values that describe the center of the ages data \nmean_value = np.mean(age)\nmedian_value = np.median(age)\nmode_value = sp.stats.mode(age)[0]\n\n# Let's print the values\nprint(\"Mean:\", mean_value)\nprint(\"Median:\", median_value)\nprint(\"Mode:\", mode_value)\n\nMean: 60.1244769874477\nMedian: 60.0\nMode: 56\n\n\nHomework: - Other ways to find mode (ie using pandas and numpy)\n\n\n2. Measures of variability\nThe Variation of the Data:\nThe variation of the data is how spread out the data are around the center.\na) Variance and Standard Deviation: - Variance: It measures the spread of the data points around the mean. - Standard Deviation: It is the square root of the variance, providing a measure of the average distance between each data point and the mean.\n\nIn summary, variance provides a measure of dispersion in squared units, while standard deviation provides a measure of dispersion in the original units of the data\n\n\n# how to implement the variance and standard deviation using numpy\nvariance_value = np.var(age)\nstd_deviation_value = np.std(age)\n\nprint(\"Variance:\", variance_value)\nprint(\"Standard Deviation:\", std_deviation_value)\n\nVariance: 163.1424552703909\nStandard Deviation: 12.772723095346228\n\n\n\nSmaller variances and standard deviation values mean that the data has values similar to each other and closer to the mean and the vice versa is true\n\n\nstd_second = 2 * std_deviation_value  # Multiply by 2 for the second standard deviation\nstd_third = 3 * std_deviation_value   # Multiply by 3 for the third standard deviation\n\nprint(\"First Standard Deviation:\", std_deviation_value)\nprint(\"Second Standard Deviation:\", std_second)\nprint(\"Third Standard Deviation:\", std_third)\n\n# empirical rule, also known as the 68-95-99.7 rule,\n\nFirst Standard Deviation: 12.772723095346228\nSecond Standard Deviation: 25.545446190692456\nThird Standard Deviation: 38.31816928603868\n\n\n\nplt.hist(x=age, bins=20, edgecolor='black')\n# add standard deviation lines\nplt.axvline(mean_value, color='red', linestyle='--', label='Mean')\nplt.axvline(mean_value+std_deviation_value, color='orange', linestyle='--', label='1st std Dev')\nplt.axvline(mean_value-std_deviation_value, color='orange', linestyle='--')\nplt.axvline(mean_value+std_second, color='black', linestyle='--', label='2nd Std Dev')\nplt.axvline(mean_value-std_second, color='black', linestyle='--')\nplt.axvline(mean_value+std_third, color='green', linestyle='--', label='3rd Std Dev')\nplt.axvline(mean_value-std_third, color='green', linestyle='--')\nplt.title('Age of Nobel Prize Winners')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\n# Adjust the position of the legend\nplt.legend(loc='upper left')\n\nplt.show()\n\n\n\n\n\n\n\n\nThe rule to consider:\nThe empirical rule, also known as the 68-95-99.7 rule, describes the distribution of data in a normal distribution. According to this rule:\n\nApproximately 68% of the data falls within one standard deviation of the mean.\nApproximately 95% of the data falls within two standard deviations of the mean.\nApproximately 99.7% of the data falls within three standard deviations of the mean.\n\n\nRange and Interquartile Range (IQR):\n\n\nRange: It is the difference between the maximum and minimum values in a dataset. It is simplest measure of variation\nInterquartile Range (IQR): It is the range between the first quartile (25th percentile) and the third quartile (75th percentile) of the dataset.\n\n\nIn summary, while the range gives an overview of the entire spread of the data from lowest to highest, the interquartile range focuses s`pecifically on the spread of the middle portion of the data, making it more robust against outliers.\n\n\n# One way to obtain range\nmin_age = min(age)\nmax_age = max(age)\nage_range = max_age - min_age\nprint('Range:', age_range)\n\nRange: 93\n\n\n\n# Calculating the range using numpy\nrange_value = np.ptp(age)\n\nprint(\"Range:\", range_value)\n\nRange: 93\n\n\n\nplt.hist(x=age, bins=20, edgecolor='black')\n# add standard deviation lines\nplt.axvline(min_age, color='green', linestyle='--', label=f'Min({min_age})')\nplt.axvline(max_age, color='red', linestyle='--', label=f'Max({max_age})')\nplt.plot([min_age, max_age], [170, 170], label=f'Range({age_range})')\n# labels\nplt.title('Age of Nobel Prize Winners')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\n# Adjust the position of the legend\nplt.legend(loc='upper left')\nplt.show()\n\n\n\n\n\n\n\n\nQuartiles:\nCalculating Quartiles\nThe quartiles (Q0,Q1,Q2,Q3,Q4) are the values that separate each quarter.\nBetween Q0 and Q1 are the 25% lowest values in the data. Between Q1 and Q2 are the next 25%. And so on.\n\nQ0 is the smallest value in the data.\nQ1 is the value separating the first quarter from the second quarter of the data.\nQ2 is the middle value (median), separating the bottom from the top half.\nQ3 is the value separating the third quarter from the fourth quarter\nQ4 is the largest value in the data.\n\n\n# Calculate the quartile\nquartiles = np.quantile(a=age, q=[0, 0.25, 0.5, 0.75, 1])\n\nprint(quartiles)\n\n[ 4. 51. 60. 69. 97.]\n\n\n\nplt.hist(x=age, bins=20, edgecolor='black')\n# add standard deviation lines\nplt.axvline(quartiles[0], color='orange', linestyle='--', label=f'Q0({quartiles[0]})')\nplt.axvline(quartiles[1], color='red', linestyle='--', label=f'Q1({quartiles[1]})')\nplt.axvline(quartiles[2], color='green', linestyle='--', label=f'Q2({quartiles[2]})')\nplt.axvline(quartiles[3], color='blue', linestyle='--', label=f'Q3({quartiles[3]})')\nplt.axvline(quartiles[4], color='black', linestyle='--', label=f'Q4({quartiles[4]})')\nplt.plot([quartiles[0], quartiles[1]], [180, 180], color='red', label=f'25%')\nplt.plot([quartiles[0], quartiles[2]], [175, 175], color='green', label=f'50%')\nplt.plot([quartiles[0], quartiles[3]], [170, 170], color='blue', label=f'75%')\nplt.plot([quartiles[0], quartiles[4]], [165, 165], color='black', label=f'100%')\n# labels\nplt.title('Age of Nobel Prize Winners')\nplt.ylabel('Frequency')\nplt.xlabel('Age')\n# Adjust the position of the legend\nplt.legend(loc='center left')\nplt.show()\n\n\n\n\n\n\n\n\nPercentiles Percentiles are values that separate the data into 100 equal parts.\nFor example, The 95th percentile separates the lowest 95% of the values from the top 5%\n\nThe 25th percentile (P25%) is the same as the first quartile (Q1).\nThe 50th percentile (P50%) is the same as the second quartile (Q2) and the median.\nThe 75th percentile (P75%) is the same as the third quartile (Q3)\n\nCalculating Percentiles with Python\n\nfirst_quartile = np.percentile(age, 25) # 25th percentile\nmiddle_percentile = np.percentile(age, 50)\nthird_quartile = np.percentile(age, 75) # 75th percentile\n\nprint('first_quartile: ', first_quartile)\nprint('middle_percentile: ', middle_percentile)  \nprint('third_quartile', third_quartile)\n\nfirst_quartile:  51.0\nmiddle_percentile:  60.0\nthird_quartile 69.0\n\n\nNote also that we can be able to use the np.quantile() method to calculate the percentiles which makes logical sense as all the values mark a fraction(percentage) of the data\n\npercentiles = np.quantile(a=age, q=[0.25, 0.50, 0.75])\nprint('Percentiles:', percentiles)\n\nPercentiles: [51. 60. 69.]\n\n\nNow we can be able to obtain the interquartile range as the difference between the third and first quartiles as predefined.\n\n# obtain the interquartile\niqr_value = third_quartile - first_quartile\n\nprint('Interquartile range: ', iqr_value)\n\nInterquartile range:  18.0\n\n\nNote: Quartiles and percentiles are both types of quantiles\n\n\n3. Measures of distribution shape ie Skewness and Kurtosis:\nThe shape of the Data:\nThe shape of the data refers to how the data are bounded on either side of the center. - Skewness: It measures the asymmetry of the distribution. - Kurtosis: It measures the peakedness or flatness of the distribution.\n\nIn simple terms, skewness tells you if your data is leaning more to one side or the other, while kurtosis tells you if your data has heavy or light tails and how sharply it peaks.\n\n\n# Plot the histogram\nplt.hist(x=age, bins=20, density=True, edgecolor='black')  # Set density=True for normalized histogram\n\n# Create a normal distribution curve\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = sp.stats.norm.pdf(x, mean_value, std_deviation_value)\nplt.plot(x, p, 'k', linewidth=2)  # 'k' indicates black color, you can change it to any color\n\n# Labels and legend\nplt.xlabel('Age')\nplt.ylabel('Probability Density')\nplt.title('Histogram with Normal Distribution Curve')\nplt.legend(['Normal Distribution', 'Histogram'])\n\nplt.show()\n\n\n\n\n\n\n\n\n\nskewness_value = sp.stats.skew(age)\n\nprint(\"Skewness:\", skewness_value)\n\nSkewness: -0.10796649932566309\n\n\nHow to interpret Skewness:\n\nPositive skewness (&gt; 0) indicates that the tail on the right side of the distribution is longer or fatter than the left side (right skewed).\nNegative skewness (&lt; 0) indicates that the tail on the left side of the distribution is longer or fatter than the right side (left skewed).\n\n\nkurtosis_value = sp.stats.kurtosis(age)\n\nprint(\"Kurtosis:\", kurtosis_value)\n\nKurtosis: -0.09094568849488294\n\n\nHow to interpret Kurtosis:\n\nA kurtosis of 3 indicates the normal distribution (mesokurtic), also known as Gaussian distribution.\nPositive kurtosis (&gt; 3) indicates a distribution with heavier tails and a sharper peak than the normal distribution. This is called leptokurtic.\nNegative kurtosis (&lt; 3) indicates a distribution with lighter tails and a flatter peak than the normal distribution. This is called platykurtic.\n\n\n\n4.Measures of association\n\nCorrelation\n\n\nCorrelation measures the relationship between two numerical variables.\n\nCorrelation Matrix\n\nA correlation matrix is simply a table showing the correlation coefficients between variables\n\nCorrelation Matrix in Python\nWe can use the corrcoef() function in Python to create a correlation matrix.\n\n# Generate example data\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([2, 4, 6, 8, 10])\n\ncorrelation_matrix = np.corrcoef(x, y)\nprint(correlation_matrix)\n\n[[1. 1.]\n [1. 1.]]\n\n\nCorrelation Coefficient: - The correlation coefficient measures the strength and direction of the linear relationship between two continuous variables. - It ranges from -1 to 1, where: - 1 indicates a perfect positive linear relationship, - -1 indicates a perfect negative linear relationship, - 0 indicates no linear relationship.\n\n# Calculate correlation coefficient\ncorrelation = np.corrcoef(x, y)[0, 1]\nprint(\"Correlation Coefficient:\", correlation)\n\nCorrelation Coefficient: 0.9999999999999999\n\n\nCorrelation vs Causality:\nCorrelation measures the numerical relationship between two varaibles\nA high correlation coefficient (close to 1), does not mean that we can for sure conclude an actual relationship between two variables.\nA classic example:\n\nDuring the summer, the sale of ice cream at a beach increases\nSimultaneously, drowning accidents also increase as well\n\nDoes this mean that increase of ice cream sale is a direct cause of increased drowning accidents?\nMeasures of Association for Categorical Variables\n\nContingency Tables and Chi-square Test for Independence:\n\nContingency tables are used to summarize the relationship between two categorical variables by counting the frequency of observations for each combination of categories.\nChi-square test for independence determines whether there is a significant association between the two categorical variables.\n\n\n\n# Pick out on the Gender and Category from the dataset\n# We drop all the missing values just for demonstration purposes\ndemo_data = data[data.Gender != 'org'][['Gender', 'Category']].dropna()\n\n\n# Obtain the cross tabulation of Gender and Category\n# The cross tabulation is also known as the contingency table\ngender_category_tab = pd.crosstab(demo_data.Gender, demo_data.Category)\n\n\n# Let's have a look at the outcome\ngender_category_tab\n\n\n\n\n\n\n\n\nCategory\nChemistry\nEconomics\nLiterature\nMedicine\nPeace\nPhysics\n\n\nGender\n\n\n\n\n\n\n\n\n\n\nfemale\n8\n3\n17\n13\n19\n5\n\n\nmale\n186\n90\n103\n214\n92\n220\n\n\n\n\n\n\n\n\n\nchi2_stat, p_value, dof, expected = sp.stats.chi2_contingency(gender_category_tab)\nprint('Chi-square Statistic:', round(chi2_stat, 3))\nprint('p-value:', round(p_value, 9))\nprint('Degrees of freedom (dof):', dof)\n# print('Expected:', expected)\n\nChi-square Statistic: 41.382\np-value: 7.9e-08\nDegrees of freedom (dof): 5\n\n\nInterpretation of Chi2 Test Results:\n\nThe Chi-square statistic measures the difference between the observed frequencies in the contingency table and the frequencies that would be expected if the variables were independent.\nThe p-value is the probability of obtaining a Chi-square statistic as extreme as, or more extreme than, the one observed in the sample, assuming that the null hypothesis is true (i.e., assuming that there is no association between the variables).\nA low p-value indicates strong evidence against the null hypothesis, suggesting that there is a significant association between the variables.\nA high p-value indicates weak evidence against the null hypothesis, suggesting that there is no significant association between the variables.\n\n\nMeasures of Association for Categorical Variables:\n\nMeasures like Cramer’s V or phi coefficient quantify the strength of association between two categorical variables.\nThese measures are based on chi-square statistics and the dimensions of the contingency table.\n\n\nCramer’s V formula:\n\n\ndef cramersV(contingency_table):\n    \n    chi2_statistic = sp.stats.chi2_contingency(contingency_table)[0]\n    total_observations = gender_category_tab.sum().sum()\n    phi2 = chi2_statistic / total_observations\n    rows, columns = gender_category_tab.shape\n    \n    return np.sqrt(phi2/ min(rows-1, columns-1))\n\n\ndef cramersVCorrected(contingency_table):\n    \n    chi2_statistic = sp.stats.chi2_contingency(contingency_table)[0]\n    total_observations = gender_category_tab.sum().sum()\n    phi2 = chi2_statistic / total_observations\n    rows, columns = gender_category_tab.shape\n    phi2_corrected = max(0, phi2 - ((columns-1)*(rows-1))/(total_observations-1))\n    rows_corrected = rows - ((rows-1)**2)/(total_observations-1)\n    columns_corrected = columns - ((columns-1)**2)/(total_observations-1)\n    \n    return np.sqrt(phi2_corrected / min((columns_corrected-1), (rows_corrected-1)))\n\n\ncramersV(gender_category_tab)\n\n0.2065471240964684\n\n\n\ncramersVCorrected(gender_category_tab)\n\n0.19375370250760823\n\n\nCramer’s V is measure of association between two categorical variables. It ranges from 0 to 1 where:\n\n0 indicates no association between the variables\n1 indicates a perfect association between the variables\n\nHere’s an interpretation of the Cramer’s V:\n\nSmall effect: Around 0.1\nMedium effect: Around 0.3\nLarge effect: Around 0.5 or greater\n\n\n\nFrequency Tables\nFrequency means the number of times a value appears in the data. A table can quickly show us how many times each value appears. If the data has many different values, it is easier to use intervals of values to present them in a table.\nHere’s the age of the 934 Nobel Prize winners up until the year 2020. IN the table, each row is an age interval of 10 years\n\n\n\nAge Interval\nFrequency\n\n\n\n\n10-19\n1\n\n\n20-29\n2\n\n\n30-39\n48\n\n\n40-49\n158\n\n\n50-59\n236\n\n\n60-69\n262\n\n\n70-79\n174\n\n\n80-89\n50\n\n\n90-99\n3\n\n\n\nNote: The intervals for the values are also called bin",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week5",
      "Descriptive Statistics and Summary Metrics"
    ]
  },
  {
    "objectID": "Python Data Analysis/Week2/21_weather_data.html",
    "href": "Python Data Analysis/Week2/21_weather_data.html",
    "title": "Cleaning the weather dataset",
    "section": "",
    "text": "In this notebook, we’ll be using numpy and pandas.\nPandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool.\nNumpy is the fundamental package for scientific computing with Python.\nLet’s install the packages pandas and numpy.\n\n## Uncomment and run this cell to install pandas and numpy\n#!pip install pandas numpy\n\n\n# import the libraries\nimport pandas as pd\nimport numpy as np\nfrom dataidea.datasets import loadDataset\n\nLet’s check the versions of python, numpy and pandas we’ll be using for this notebook\n\n# checking python version\nprint('Python Version: ',)\n!python --version\n\nPython Version: \nPython 3.10.12\n\n\n\n# Checking numpy and pandas versions\nprint('Pandas Version: ', pd.__version__)\nprint('Numpy Version: ', np.__version__)\n\nPandas Version:  2.2.1\nNumpy Version:  1.26.4\n\n\nLet’s load the dataset. We’ll be using a weather dataset that imagined for learning purposes.\n\n# load the dataset\nweather_data = loadDataset('weather')\n\nWe can sample out random rows from the dataset using the sample() method, we can use the n parameter to specify the number of rows to sample\n\n# sample out random values from the dataset\nweather_data.sample(n=5)\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n\n\n\n\n\n\nDisplay some info about the dataset eg number of entries, count of non-null values and variable datatypes using the info() method\n\n# get quick dataframe info\nweather_data.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 9 entries, 0 to 8\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   day          9 non-null      object \n 1   temperature  5 non-null      float64\n 2   windspead    5 non-null      float64\n 3   event        7 non-null      object \ndtypes: float64(2), object(2)\nmemory usage: 416.0+ bytes\n\n\nWe can count all missing values in each column in our dataframe by using dataframe.isna().sum(), eg\n\n# count missing values in each column\nweather_data.isna().sum()\n\nday            0\ntemperature    4\nwindspead      4\nevent          2\ndtype: int64\n\n\nWe can use a boolean-indexing like technique to find all rows in a dataset with missing values in a specific column.\n\n# get rows with missing data in temperature\nweather_data[weather_data.temperature.isna()]\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n# get rows with missing data in event column\nweather_data[weather_data.event.isna()]\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n6\n09/01/2017\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nFor the next part, we would like to demonstrate forward fill (ffill()) and backward fill (bfill), we first create two copies of the dataframe to avoid modifying our original copy in memory. - ffill() fills the missing values with the previous valid value in the column - bfill() fills the missing values with the next valid value in the column\n\n# Create copies of a dataframe\nweather_data1 = weather_data.copy()\nweather_data2 = weather_data.copy()\n\n\n# fill with the previous valid value\nweather_data1['event'] = weather_data1.event.ffill()\nweather_data1\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nSnow\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nSunny\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\n\nweather_data\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nNaN\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nNaN\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\n\n# fill with the next valid value in the column\nweather_data2['event'] = weather_data2.event.bfill()\nweather_data2\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\nNaN\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nRain\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nCloudy\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\nWe can modify (or fill) a specific value in the dataframe by using the loc[] method. This picks the value by its row (index) and column names. Assigning it a new value modifies it in the dataframe as illustrated below\n\n# modify a specific value in the dataframe\nweather_data2.loc[1, 'temperature'] = 29\nweather_data2\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\n29.0\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\nNaN\n7.0\nRain\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\nNaN\nNaN\nSunny\n\n\n6\n09/01/2017\nNaN\nNaN\nCloudy\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\nWe can use the fillna() method to replace all missing values in a column with a specific value as demostrated value\n\n# replace missing values in temperature column with mean\nweather_data2['temperature'] = weather_data2.temperature.fillna(\n    value=weather_data2.temperature.mean()\n)\nweather_data2\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\n29.0\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\nNaN\nSnow\n\n\n3\n06/01/2017\n32.5\n7.0\nRain\n\n\n4\n07/01/2017\n32.0\nNaN\nRain\n\n\n5\n08/01/2017\n32.5\nNaN\nSunny\n\n\n6\n09/01/2017\n32.5\nNaN\nCloudy\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\n\n# create a copy of weather_data2\nweather_data22 = weather_data2.copy()\n\n\n# Replace missing values in windspead column with a specific value\nweather_data22['windspead'] = weather_data2.windspead.fillna(value=7.5)\nweather_data22\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\n29.0\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\n7.5\nSnow\n\n\n3\n06/01/2017\n32.5\n7.0\nRain\n\n\n4\n07/01/2017\n32.0\n7.5\nRain\n\n\n5\n08/01/2017\n32.5\n7.5\nSunny\n\n\n6\n09/01/2017\n32.5\n7.5\nCloudy\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\nWe can also use the fillna() method to fill missing values in multiple columns by passing in the dictionary of key/value pairs of column-name and value to replace. Before we demonstrate this, let’s create a copy of the dataframe to avoid modifying the original in memory\n\n# create a copy of the weather_data dataframe\nweather_data3 = weather_data.copy()\n\n\n# Replace missing values in temperature, column and event\nweather_data3.fillna(value={\n    'temperature': weather_data3.temperature.mean(), \n    'windspead': weather_data3.windspead.max(), \n    'event': weather_data3.event.bfill()\n    }, inplace=True)\n\n\nweather_data3\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n1\n04/01/2017\n33.2\n9.0\nSunny\n\n\n2\n05/01/2017\n28.0\n12.0\nSnow\n\n\n3\n06/01/2017\n33.2\n7.0\nRain\n\n\n4\n07/01/2017\n32.0\n12.0\nRain\n\n\n5\n08/01/2017\n33.2\n12.0\nSunny\n\n\n6\n09/01/2017\n33.2\n12.0\nCloudy\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\nWe can optionally drop all rows with missing values using the dropna() method. Before we demonstrate that, let’s first create a dataframe copy to avoid modifying the original in the memory\n\n# create a copy of weather_data dataframe\nweather_data4 = weather_data.copy()\n\n\n# Drop all rows with missing values\nweather_data4.dropna()\n\n\n\n\n\n\n\n\n\nday\ntemperature\nwindspead\nevent\n\n\n\n\n0\n01/01/2017\n32.0\n6.0\nRain\n\n\n7\n10/01/2017\n34.0\n8.0\nCloudy\n\n\n8\n11/01/2017\n40.0\n12.0\nSunny\n\n\n\n\n\n\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "Python Data Analysis",
      "Week2",
      "Cleaning the weather dataset"
    ]
  }
]